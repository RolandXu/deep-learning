{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 4:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "First 20 Labels: [0, 6, 0, 2, 7, 2, 1, 2, 4, 1, 5, 6, 6, 3, 1, 3, 5, 5, 8, 1]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 13 Max Value: 169\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBD0ANCboAaAxQQ8AjQl6AGjs/wC42Lcq2cSEgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a2695a7b8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 4\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "\n",
    "    normalized = np.ndarray(shape=x.shape, dtype=float)\n",
    "\n",
    "    for (index, value) in np.ndenumerate(x):\n",
    "        normalized[index] = value / float(255)\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((len(x), 10), dtype=np.int)\n",
    "    for k, i in enumerate(x):\n",
    "        one_hot[k][i] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, ) + image_shape, name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides,\n",
    "                   pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # print(conv_num_outputs, x_tensor.get_shape(), conv_ksize, conv_strides)\n",
    "    # print(pool_ksize, pool_strides)\n",
    "\n",
    "    # Create weight and bias\n",
    "    color_channels = x_tensor.get_shape().as_list()[-1]\n",
    "    weight_shape = [*conv_ksize, color_channels, conv_num_outputs]\n",
    "    weight = tf.Variable(tf.random_normal(weight_shape))\n",
    "    bias = tf.Variable(tf.random_normal([conv_num_outputs]))\n",
    "\n",
    "    # apply a convolution and add bias\n",
    "    x_tensor = tf.nn.conv2d(\n",
    "        x_tensor, weight, strides=[1, *conv_strides, 1], padding='SAME')\n",
    "\n",
    "    x_tensor = tf.nn.bias_add(x_tensor, bias)\n",
    "\n",
    "    # Add a nonlinear activation\n",
    "    x_tensor = tf.nn.relu(x_tensor)\n",
    "\n",
    "    x_tensor = tf.nn.max_pool(\n",
    "        x_tensor,\n",
    "        ksize=[1, *pool_ksize, 1],\n",
    "        strides=[1, *pool_strides, 1],\n",
    "        padding='SAME')\n",
    "\n",
    "    #print(\"x_tensor\", x_tensor.get_shape())\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    dim = np.prod(x_tensor.get_shape().as_list()[1:])\n",
    "    x_tensor = tf.reshape(x_tensor, [-1, dim])\n",
    "    #print(x_tensor.get_shape())\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # print (x_tensor.get_shape().as_list())\n",
    "    \n",
    "    x_shape = x_tensor.get_shape().as_list()[1:]\n",
    "    weight_shape = [*x_shape, num_outputs]\n",
    "    \n",
    "    weight = tf.Variable(tf.random_normal(weight_shape))\n",
    "    bias = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    x_tensor = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    \n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # print (x_tensor.get_shape().as_list())\n",
    "    x_shape = x_tensor.get_shape().as_list()[1:]\n",
    "    weight_shape = [*x_shape, num_outputs]\n",
    "    \n",
    "    weight = tf.Variable(tf.random_normal(weight_shape))\n",
    "    bias = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    x_tensor = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    \n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    #print (x.get_shape())\n",
    "    x_tensor = x\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 32\n",
    "    conv_ksize = (3, 3)\n",
    "    conv_strides = (1, 1)\n",
    "    pool_ksize = (3, 3)\n",
    "    pool_strides = (2, 2)\n",
    "    x_tensor = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize,\n",
    "                              conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (3, 3)\n",
    "    x_tensor = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize,\n",
    "                              conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    #x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "\n",
    "    conv_num_outputs = 128\n",
    "    conv_ksize = (3, 3)\n",
    "    x_tensor = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize,\n",
    "                              conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x_tensor = flatten(x_tensor)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    num_outputs = 2048\n",
    "    x_tensor = fully_conn(x_tensor, num_outputs)\n",
    "\n",
    "    x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "\n",
    "    num_outputs = 1024\n",
    "    x_tensor = fully_conn(x_tensor, num_outputs)\n",
    "\n",
    "    #x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "    #num_outputs = 512\n",
    "    #x_tensor = fully_conn(x_tensor, num_outputs)\n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    n_classes = 10\n",
    "    x_tensor = output(x_tensor, n_classes)\n",
    "\n",
    "    # TODO: return output\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch,\n",
    "                         label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    #print(keep_probability, feature_batch.shape, label_batch.shape)\n",
    "\n",
    "    session.run(\n",
    "        optimizer,\n",
    "        feed_dict={\n",
    "            x: feature_batch,\n",
    "            y: label_batch,\n",
    "            keep_prob: keep_probability\n",
    "        })\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(\n",
    "        cost, feed_dict={x: feature_batch,\n",
    "                         y: label_batch,\n",
    "                         keep_prob: 1.})\n",
    "\n",
    "    valid_acc = session.run(\n",
    "        accuracy,\n",
    "        feed_dict={x: valid_features,\n",
    "                   y: valid_labels,\n",
    "                   keep_prob: 1.})\n",
    "\n",
    "    print(\n",
    "        datetime.datetime.now().time(),\n",
    "        'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "keep_probability = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  11:14:35.231310 Loss: 24433108.0000 Validation Accuracy: 0.127800\n",
      "Epoch  2, CIFAR-10 Batch 1:  11:14:53.337882 Loss: 17576418.0000 Validation Accuracy: 0.227200\n",
      "Epoch  3, CIFAR-10 Batch 1:  11:15:11.470416 Loss: 12725330.0000 Validation Accuracy: 0.239600\n",
      "Epoch  4, CIFAR-10 Batch 1:  11:15:29.624008 Loss: 13784984.0000 Validation Accuracy: 0.275800\n",
      "Epoch  5, CIFAR-10 Batch 1:  11:15:47.720497 Loss: 10918347.0000 Validation Accuracy: 0.299400\n",
      "Epoch  6, CIFAR-10 Batch 1:  11:16:05.761762 Loss: 10801940.0000 Validation Accuracy: 0.318400\n",
      "Epoch  7, CIFAR-10 Batch 1:  11:16:23.779157 Loss: 9082938.0000 Validation Accuracy: 0.342400\n",
      "Epoch  8, CIFAR-10 Batch 1:  11:16:41.791747 Loss: 8460356.0000 Validation Accuracy: 0.346000\n",
      "Epoch  9, CIFAR-10 Batch 1:  11:16:59.973218 Loss: 7394451.0000 Validation Accuracy: 0.380800\n",
      "Epoch 10, CIFAR-10 Batch 1:  11:17:18.048398 Loss: 6501036.5000 Validation Accuracy: 0.381800\n",
      "Epoch 11, CIFAR-10 Batch 1:  11:17:36.196157 Loss: 5902877.0000 Validation Accuracy: 0.400200\n",
      "Epoch 12, CIFAR-10 Batch 1:  11:17:54.259264 Loss: 5555973.5000 Validation Accuracy: 0.402400\n",
      "Epoch 13, CIFAR-10 Batch 1:  11:18:12.332937 Loss: 5080985.5000 Validation Accuracy: 0.412800\n",
      "Epoch 14, CIFAR-10 Batch 1:  11:18:30.364797 Loss: 4726145.5000 Validation Accuracy: 0.427400\n",
      "Epoch 15, CIFAR-10 Batch 1:  11:18:48.407787 Loss: 4410227.5000 Validation Accuracy: 0.425800\n",
      "Epoch 16, CIFAR-10 Batch 1:  11:19:06.539965 Loss: 4158681.7500 Validation Accuracy: 0.433800\n",
      "Epoch 17, CIFAR-10 Batch 1:  11:19:24.651243 Loss: 3887052.7500 Validation Accuracy: 0.441000\n",
      "Epoch 18, CIFAR-10 Batch 1:  11:19:42.690950 Loss: 3790929.5000 Validation Accuracy: 0.445600\n",
      "Epoch 19, CIFAR-10 Batch 1:  11:20:00.735077 Loss: 3577139.7500 Validation Accuracy: 0.444600\n",
      "Epoch 20, CIFAR-10 Batch 1:  11:20:18.931455 Loss: 3381876.5000 Validation Accuracy: 0.456800\n",
      "Epoch 21, CIFAR-10 Batch 1:  11:20:36.977529 Loss: 3239722.5000 Validation Accuracy: 0.453600\n",
      "Epoch 22, CIFAR-10 Batch 1:  11:20:55.036072 Loss: 3087790.2500 Validation Accuracy: 0.461400\n",
      "Epoch 23, CIFAR-10 Batch 1:  11:21:13.089320 Loss: 2957637.7500 Validation Accuracy: 0.458000\n",
      "Epoch 24, CIFAR-10 Batch 1:  11:21:31.105631 Loss: 2853689.0000 Validation Accuracy: 0.465400\n",
      "Epoch 25, CIFAR-10 Batch 1:  11:21:49.114824 Loss: 2760652.7500 Validation Accuracy: 0.470800\n",
      "Epoch 26, CIFAR-10 Batch 1:  11:22:07.104756 Loss: 2711244.7500 Validation Accuracy: 0.463000\n",
      "Epoch 27, CIFAR-10 Batch 1:  11:22:25.201633 Loss: 2601626.5000 Validation Accuracy: 0.474000\n",
      "Epoch 28, CIFAR-10 Batch 1:  11:22:43.197951 Loss: 2504229.5000 Validation Accuracy: 0.475800\n",
      "Epoch 29, CIFAR-10 Batch 1:  11:23:01.272846 Loss: 2437648.7500 Validation Accuracy: 0.480600\n",
      "Epoch 30, CIFAR-10 Batch 1:  11:23:19.237940 Loss: 2345860.0000 Validation Accuracy: 0.480600\n",
      "Epoch 31, CIFAR-10 Batch 1:  11:23:37.271870 Loss: 2329413.7500 Validation Accuracy: 0.474400\n",
      "Epoch 32, CIFAR-10 Batch 1:  11:23:55.250902 Loss: 2252185.2500 Validation Accuracy: 0.486800\n",
      "Epoch 33, CIFAR-10 Batch 1:  11:24:13.236655 Loss: 2170638.5000 Validation Accuracy: 0.483800\n",
      "Epoch 34, CIFAR-10 Batch 1:  11:24:31.311164 Loss: 2113176.7500 Validation Accuracy: 0.485800\n",
      "Epoch 35, CIFAR-10 Batch 1:  11:24:49.300792 Loss: 2042839.7500 Validation Accuracy: 0.484200\n",
      "Epoch 36, CIFAR-10 Batch 1:  11:25:07.635035 Loss: 1982104.5000 Validation Accuracy: 0.492600\n",
      "Epoch 37, CIFAR-10 Batch 1:  11:25:25.632266 Loss: 1973500.8750 Validation Accuracy: 0.485000\n",
      "Epoch 38, CIFAR-10 Batch 1:  11:25:43.519584 Loss: 1899672.3750 Validation Accuracy: 0.490000\n",
      "Epoch 39, CIFAR-10 Batch 1:  11:26:01.551054 Loss: 1837146.5000 Validation Accuracy: 0.486000\n",
      "Epoch 40, CIFAR-10 Batch 1:  11:26:19.572055 Loss: 1795752.2500 Validation Accuracy: 0.492600\n",
      "Epoch 41, CIFAR-10 Batch 1:  11:26:37.662072 Loss: 1737932.6250 Validation Accuracy: 0.494400\n",
      "Epoch 42, CIFAR-10 Batch 1:  11:26:55.666095 Loss: 1731242.2500 Validation Accuracy: 0.492400\n",
      "Epoch 43, CIFAR-10 Batch 1:  11:27:13.649814 Loss: 1653207.0000 Validation Accuracy: 0.490400\n",
      "Epoch 44, CIFAR-10 Batch 1:  11:27:31.719231 Loss: 1594593.6250 Validation Accuracy: 0.493400\n",
      "Epoch 45, CIFAR-10 Batch 1:  11:27:49.642400 Loss: 1584097.8750 Validation Accuracy: 0.497400\n",
      "Epoch 46, CIFAR-10 Batch 1:  11:28:07.998266 Loss: 1558648.3750 Validation Accuracy: 0.495800\n",
      "Epoch 47, CIFAR-10 Batch 1:  11:28:26.030813 Loss: 1515614.7500 Validation Accuracy: 0.498800\n",
      "Epoch 48, CIFAR-10 Batch 1:  11:28:43.868701 Loss: 1484973.7500 Validation Accuracy: 0.499600\n",
      "Epoch 49, CIFAR-10 Batch 1:  11:29:01.942510 Loss: 1464820.0000 Validation Accuracy: 0.498200\n",
      "Epoch 50, CIFAR-10 Batch 1:  11:29:19.898239 Loss: 1398175.2500 Validation Accuracy: 0.502600\n",
      "Epoch 51, CIFAR-10 Batch 1:  11:29:37.925531 Loss: 1377671.6250 Validation Accuracy: 0.502400\n",
      "Epoch 52, CIFAR-10 Batch 1:  11:29:55.917517 Loss: 1328045.6250 Validation Accuracy: 0.503000\n",
      "Epoch 53, CIFAR-10 Batch 1:  11:30:14.020968 Loss: 1316969.8750 Validation Accuracy: 0.506000\n",
      "Epoch 54, CIFAR-10 Batch 1:  11:30:32.013554 Loss: 1268268.8750 Validation Accuracy: 0.507000\n",
      "Epoch 55, CIFAR-10 Batch 1:  11:30:49.965000 Loss: 1229873.3750 Validation Accuracy: 0.508800\n",
      "Epoch 56, CIFAR-10 Batch 1:  11:31:07.942292 Loss: 1207013.5000 Validation Accuracy: 0.507000\n",
      "Epoch 57, CIFAR-10 Batch 1:  11:31:25.955762 Loss: 1185507.1250 Validation Accuracy: 0.513000\n",
      "Epoch 58, CIFAR-10 Batch 1:  11:31:43.851744 Loss: 1143853.5000 Validation Accuracy: 0.507200\n",
      "Epoch 59, CIFAR-10 Batch 1:  11:32:01.748905 Loss: 1125981.6250 Validation Accuracy: 0.508000\n",
      "Epoch 60, CIFAR-10 Batch 1:  11:32:19.703342 Loss: 1092020.8750 Validation Accuracy: 0.510400\n",
      "Epoch 61, CIFAR-10 Batch 1:  11:32:37.748181 Loss: 1090792.1250 Validation Accuracy: 0.509000\n",
      "Epoch 62, CIFAR-10 Batch 1:  11:32:55.644793 Loss: 1057744.6250 Validation Accuracy: 0.506800\n",
      "Epoch 63, CIFAR-10 Batch 1:  11:33:13.533462 Loss: 1047718.5625 Validation Accuracy: 0.506600\n",
      "Epoch 64, CIFAR-10 Batch 1:  11:33:31.513046 Loss: 1013733.2500 Validation Accuracy: 0.508800\n",
      "Epoch 65, CIFAR-10 Batch 1:  11:33:49.462963 Loss: 1015600.6875 Validation Accuracy: 0.503600\n",
      "Epoch 66, CIFAR-10 Batch 1:  11:34:07.426623 Loss: 954700.9375 Validation Accuracy: 0.515600\n",
      "Epoch 67, CIFAR-10 Batch 1:  11:34:25.340599 Loss: 946943.3125 Validation Accuracy: 0.509200\n",
      "Epoch 68, CIFAR-10 Batch 1:  11:34:43.219904 Loss: 908573.9375 Validation Accuracy: 0.511600\n",
      "Epoch 69, CIFAR-10 Batch 1:  11:35:01.125508 Loss: 928427.8125 Validation Accuracy: 0.511400\n",
      "Epoch 70, CIFAR-10 Batch 1:  11:35:19.054004 Loss: 890643.0625 Validation Accuracy: 0.514200\n",
      "Epoch 71, CIFAR-10 Batch 1:  11:35:37.081068 Loss: 900497.1875 Validation Accuracy: 0.513200\n",
      "Epoch 72, CIFAR-10 Batch 1:  11:35:55.043649 Loss: 851845.5625 Validation Accuracy: 0.511600\n",
      "Epoch 73, CIFAR-10 Batch 1:  11:36:13.009715 Loss: 832169.1875 Validation Accuracy: 0.511600\n",
      "Epoch 74, CIFAR-10 Batch 1:  11:36:31.084918 Loss: 825315.3125 Validation Accuracy: 0.518600\n",
      "Epoch 75, CIFAR-10 Batch 1:  11:36:49.062429 Loss: 794291.2500 Validation Accuracy: 0.514600\n",
      "Epoch 76, CIFAR-10 Batch 1:  11:37:07.060705 Loss: 786729.1875 Validation Accuracy: 0.516400\n",
      "Epoch 77, CIFAR-10 Batch 1:  11:37:25.061509 Loss: 761841.1875 Validation Accuracy: 0.514400\n",
      "Epoch 78, CIFAR-10 Batch 1:  11:37:43.039312 Loss: 743808.8125 Validation Accuracy: 0.517200\n",
      "Epoch 79, CIFAR-10 Batch 1:  11:38:01.074299 Loss: 733985.4375 Validation Accuracy: 0.521000\n",
      "Epoch 80, CIFAR-10 Batch 1:  11:38:19.070375 Loss: 706643.9375 Validation Accuracy: 0.512000\n",
      "Epoch 81, CIFAR-10 Batch 1:  11:38:37.044676 Loss: 704453.0625 Validation Accuracy: 0.520000\n",
      "Epoch 82, CIFAR-10 Batch 1:  11:38:54.887290 Loss: 683243.5000 Validation Accuracy: 0.520000\n",
      "Epoch 83, CIFAR-10 Batch 1:  11:39:12.793768 Loss: 678286.3125 Validation Accuracy: 0.522600\n",
      "Epoch 84, CIFAR-10 Batch 1:  11:39:30.757731 Loss: 667792.6250 Validation Accuracy: 0.518600\n",
      "Epoch 85, CIFAR-10 Batch 1:  11:39:48.675367 Loss: 653131.6250 Validation Accuracy: 0.520000\n",
      "Epoch 86, CIFAR-10 Batch 1:  11:40:06.749706 Loss: 634410.3750 Validation Accuracy: 0.518000\n",
      "Epoch 87, CIFAR-10 Batch 1:  11:40:24.720506 Loss: 620448.1875 Validation Accuracy: 0.519400\n",
      "Epoch 88, CIFAR-10 Batch 1:  11:40:42.724731 Loss: 606500.9375 Validation Accuracy: 0.516600\n",
      "Epoch 89, CIFAR-10 Batch 1:  11:41:00.674001 Loss: 593822.4375 Validation Accuracy: 0.517600\n",
      "Epoch 90, CIFAR-10 Batch 1:  11:41:18.641455 Loss: 589852.9375 Validation Accuracy: 0.516800\n",
      "Epoch 91, CIFAR-10 Batch 1:  11:41:36.630266 Loss: 583154.7500 Validation Accuracy: 0.520800\n",
      "Epoch 92, CIFAR-10 Batch 1:  11:41:54.618465 Loss: 569133.4375 Validation Accuracy: 0.517600\n",
      "Epoch 93, CIFAR-10 Batch 1:  11:42:12.581814 Loss: 565318.5625 Validation Accuracy: 0.522000\n",
      "Epoch 94, CIFAR-10 Batch 1:  11:42:30.519152 Loss: 560478.0625 Validation Accuracy: 0.516600\n",
      "Epoch 95, CIFAR-10 Batch 1:  11:42:48.445681 Loss: 538485.6875 Validation Accuracy: 0.516800\n",
      "Epoch 96, CIFAR-10 Batch 1:  11:43:06.465128 Loss: 536506.3750 Validation Accuracy: 0.516800\n",
      "Epoch 97, CIFAR-10 Batch 1:  11:43:24.454508 Loss: 521380.3438 Validation Accuracy: 0.513800\n",
      "Epoch 98, CIFAR-10 Batch 1:  11:43:42.373381 Loss: 508049.2188 Validation Accuracy: 0.518800\n",
      "Epoch 99, CIFAR-10 Batch 1:  11:44:00.364196 Loss: 494160.4375 Validation Accuracy: 0.521000\n",
      "Epoch 100, CIFAR-10 Batch 1:  11:44:18.283557 Loss: 487298.3438 Validation Accuracy: 0.518200\n",
      "Epoch 101, CIFAR-10 Batch 1:  11:44:36.295718 Loss: 481488.7812 Validation Accuracy: 0.515400\n",
      "Epoch 102, CIFAR-10 Batch 1:  11:44:54.322793 Loss: 474185.1875 Validation Accuracy: 0.516400\n",
      "Epoch 103, CIFAR-10 Batch 1:  11:45:12.284190 Loss: 466660.8438 Validation Accuracy: 0.514000\n",
      "Epoch 104, CIFAR-10 Batch 1:  11:45:30.310300 Loss: 464302.9062 Validation Accuracy: 0.515400\n",
      "Epoch 105, CIFAR-10 Batch 1:  11:45:48.191386 Loss: 444743.3125 Validation Accuracy: 0.513600\n",
      "Epoch 106, CIFAR-10 Batch 1:  11:46:06.128516 Loss: 436478.0938 Validation Accuracy: 0.518400\n",
      "Epoch 107, CIFAR-10 Batch 1:  11:46:24.073561 Loss: 424565.2188 Validation Accuracy: 0.518400\n",
      "Epoch 108, CIFAR-10 Batch 1:  11:46:42.037747 Loss: 419281.3125 Validation Accuracy: 0.516000\n",
      "Epoch 109, CIFAR-10 Batch 1:  11:47:00.044719 Loss: 409422.5625 Validation Accuracy: 0.519000\n",
      "Epoch 110, CIFAR-10 Batch 1:  11:47:17.997856 Loss: 402182.6875 Validation Accuracy: 0.517800\n",
      "Epoch 111, CIFAR-10 Batch 1:  11:47:35.962583 Loss: 392457.5938 Validation Accuracy: 0.517200\n",
      "Epoch 112, CIFAR-10 Batch 1:  11:47:53.954065 Loss: 384352.4062 Validation Accuracy: 0.518400\n",
      "Epoch 113, CIFAR-10 Batch 1:  11:48:11.916767 Loss: 374695.4062 Validation Accuracy: 0.515800\n",
      "Epoch 114, CIFAR-10 Batch 1:  11:48:29.943823 Loss: 373153.5312 Validation Accuracy: 0.515200\n",
      "Epoch 115, CIFAR-10 Batch 1:  11:48:47.971573 Loss: 370273.1562 Validation Accuracy: 0.511400\n",
      "Epoch 116, CIFAR-10 Batch 1:  11:49:05.981028 Loss: 358060.7188 Validation Accuracy: 0.513600\n",
      "Epoch 117, CIFAR-10 Batch 1:  11:49:23.921079 Loss: 346641.0938 Validation Accuracy: 0.506600\n",
      "Epoch 118, CIFAR-10 Batch 1:  11:49:41.829080 Loss: 340155.1562 Validation Accuracy: 0.509800\n",
      "Epoch 119, CIFAR-10 Batch 1:  11:49:59.758225 Loss: 337866.4062 Validation Accuracy: 0.505400\n",
      "Epoch 120, CIFAR-10 Batch 1:  11:50:17.830023 Loss: 334722.3750 Validation Accuracy: 0.507600\n",
      "Epoch 121, CIFAR-10 Batch 1:  11:50:35.857325 Loss: 323533.7500 Validation Accuracy: 0.508000\n",
      "Epoch 122, CIFAR-10 Batch 1:  11:50:53.844231 Loss: 317661.3125 Validation Accuracy: 0.502800\n",
      "Epoch 123, CIFAR-10 Batch 1:  11:51:11.820177 Loss: 310656.9375 Validation Accuracy: 0.495400\n",
      "Epoch 124, CIFAR-10 Batch 1:  11:51:29.841598 Loss: 305075.9375 Validation Accuracy: 0.492200\n",
      "Epoch 125, CIFAR-10 Batch 1:  11:51:47.756737 Loss: 297879.4062 Validation Accuracy: 0.488800\n",
      "Epoch 126, CIFAR-10 Batch 1:  11:52:05.753906 Loss: 293646.2500 Validation Accuracy: 0.489400\n",
      "Epoch 127, CIFAR-10 Batch 1:  11:52:23.763821 Loss: 287041.3125 Validation Accuracy: 0.483200\n",
      "Epoch 128, CIFAR-10 Batch 1:  11:52:41.753646 Loss: 275914.6250 Validation Accuracy: 0.478000\n",
      "Epoch 129, CIFAR-10 Batch 1:  11:52:59.819339 Loss: 264417.7812 Validation Accuracy: 0.469800\n",
      "Epoch 130, CIFAR-10 Batch 1:  11:53:17.742465 Loss: 263833.5312 Validation Accuracy: 0.465400\n",
      "Epoch 131, CIFAR-10 Batch 1:  11:53:35.689985 Loss: 255863.3281 Validation Accuracy: 0.456800\n",
      "Epoch 132, CIFAR-10 Batch 1:  11:53:53.654204 Loss: 248165.1719 Validation Accuracy: 0.450200\n",
      "Epoch 133, CIFAR-10 Batch 1:  11:54:11.616975 Loss: 246464.5469 Validation Accuracy: 0.439400\n",
      "Epoch 134, CIFAR-10 Batch 1:  11:54:29.664503 Loss: 241835.8750 Validation Accuracy: 0.431800\n",
      "Epoch 135, CIFAR-10 Batch 1:  11:54:47.584343 Loss: 235979.9531 Validation Accuracy: 0.422600\n",
      "Epoch 136, CIFAR-10 Batch 1:  11:55:05.771643 Loss: 225467.4062 Validation Accuracy: 0.423200\n",
      "Epoch 137, CIFAR-10 Batch 1:  11:55:23.763038 Loss: 214304.9062 Validation Accuracy: 0.413600\n",
      "Epoch 138, CIFAR-10 Batch 1:  11:55:41.766997 Loss: 204122.2969 Validation Accuracy: 0.404600\n",
      "Epoch 139, CIFAR-10 Batch 1:  11:55:59.765441 Loss: 195578.7344 Validation Accuracy: 0.392800\n",
      "Epoch 140, CIFAR-10 Batch 1:  11:56:17.730779 Loss: 184794.5000 Validation Accuracy: 0.381000\n",
      "Epoch 141, CIFAR-10 Batch 1:  11:56:35.725956 Loss: 175569.5469 Validation Accuracy: 0.373200\n",
      "Epoch 142, CIFAR-10 Batch 1:  11:56:53.620007 Loss: 165974.7656 Validation Accuracy: 0.359400\n",
      "Epoch 143, CIFAR-10 Batch 1:  11:57:11.563181 Loss: 157994.4062 Validation Accuracy: 0.344600\n",
      "Epoch 144, CIFAR-10 Batch 1:  11:57:29.603509 Loss: 148500.7656 Validation Accuracy: 0.329800\n",
      "Epoch 145, CIFAR-10 Batch 1:  11:57:47.574087 Loss: 137862.9688 Validation Accuracy: 0.312200\n",
      "Epoch 146, CIFAR-10 Batch 1:  11:58:05.781746 Loss: 128414.5547 Validation Accuracy: 0.305000\n",
      "Epoch 147, CIFAR-10 Batch 1:  11:58:23.576821 Loss: 120511.3047 Validation Accuracy: 0.285000\n",
      "Epoch 148, CIFAR-10 Batch 1:  11:58:41.567544 Loss: 113030.1094 Validation Accuracy: 0.273200\n",
      "Epoch 149, CIFAR-10 Batch 1:  11:58:59.577785 Loss: 105164.1250 Validation Accuracy: 0.261400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0ebd1bbc35ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-8b5c43f170f3>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         })\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/roland/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  14:14:10.075545 Loss: 27734886.0000 Validation Accuracy: 0.148400\n",
      "Epoch  1, CIFAR-10 Batch 2:  14:14:13.061914 Loss: 13580098.0000 Validation Accuracy: 0.161400\n",
      "Epoch  1, CIFAR-10 Batch 3:  14:14:16.047572 Loss: 9902466.0000 Validation Accuracy: 0.180800\n",
      "Epoch  1, CIFAR-10 Batch 4:  14:14:19.034452 Loss: 7944119.0000 Validation Accuracy: 0.190400\n",
      "Epoch  1, CIFAR-10 Batch 5:  14:14:22.016920 Loss: 6232388.5000 Validation Accuracy: 0.216200\n",
      "Epoch  2, CIFAR-10 Batch 1:  14:14:25.003643 Loss: 5304151.5000 Validation Accuracy: 0.239800\n",
      "Epoch  2, CIFAR-10 Batch 2:  14:14:27.986876 Loss: 4966842.5000 Validation Accuracy: 0.244600\n",
      "Epoch  2, CIFAR-10 Batch 3:  14:14:30.970931 Loss: 4768766.0000 Validation Accuracy: 0.253800\n",
      "Epoch  2, CIFAR-10 Batch 4:  14:14:33.957423 Loss: 4123777.2500 Validation Accuracy: 0.258200\n",
      "Epoch  2, CIFAR-10 Batch 5:  14:14:36.941765 Loss: 4020631.5000 Validation Accuracy: 0.272400\n",
      "Epoch  3, CIFAR-10 Batch 1:  14:14:39.926956 Loss: 3885711.0000 Validation Accuracy: 0.273800\n",
      "Epoch  3, CIFAR-10 Batch 2:  14:14:42.911940 Loss: 3572120.7500 Validation Accuracy: 0.276800\n",
      "Epoch  3, CIFAR-10 Batch 3:  14:14:45.899045 Loss: 3541025.2500 Validation Accuracy: 0.291000\n",
      "Epoch  3, CIFAR-10 Batch 4:  14:14:48.885015 Loss: 3274564.0000 Validation Accuracy: 0.281000\n",
      "Epoch  3, CIFAR-10 Batch 5:  14:14:51.868189 Loss: 3241597.5000 Validation Accuracy: 0.291800\n",
      "Epoch  4, CIFAR-10 Batch 1:  14:14:54.855030 Loss: 3247930.0000 Validation Accuracy: 0.306200\n",
      "Epoch  4, CIFAR-10 Batch 2:  14:14:57.841716 Loss: 2983709.5000 Validation Accuracy: 0.310000\n",
      "Epoch  4, CIFAR-10 Batch 3:  14:15:00.826247 Loss: 2941420.0000 Validation Accuracy: 0.318200\n",
      "Epoch  4, CIFAR-10 Batch 4:  14:15:03.811136 Loss: 2647622.2500 Validation Accuracy: 0.320000\n",
      "Epoch  4, CIFAR-10 Batch 5:  14:15:06.795663 Loss: 2684399.2500 Validation Accuracy: 0.322600\n",
      "Epoch  5, CIFAR-10 Batch 1:  14:15:09.782741 Loss: 2758646.5000 Validation Accuracy: 0.331000\n",
      "Epoch  5, CIFAR-10 Batch 2:  14:15:12.769035 Loss: 2725965.5000 Validation Accuracy: 0.331000\n",
      "Epoch  5, CIFAR-10 Batch 3:  14:15:15.751275 Loss: 2610515.5000 Validation Accuracy: 0.341400\n",
      "Epoch  5, CIFAR-10 Batch 4:  14:15:18.740013 Loss: 2425871.5000 Validation Accuracy: 0.329400\n",
      "Epoch  5, CIFAR-10 Batch 5:  14:15:21.722927 Loss: 2368577.0000 Validation Accuracy: 0.345000\n",
      "Epoch  6, CIFAR-10 Batch 1:  14:15:24.708408 Loss: 2424573.7500 Validation Accuracy: 0.351200\n",
      "Epoch  6, CIFAR-10 Batch 2:  14:15:27.694093 Loss: 2402487.7500 Validation Accuracy: 0.344000\n",
      "Epoch  6, CIFAR-10 Batch 3:  14:15:30.678899 Loss: 2529586.0000 Validation Accuracy: 0.353200\n",
      "Epoch  6, CIFAR-10 Batch 4:  14:15:33.662763 Loss: 2397611.0000 Validation Accuracy: 0.337400\n",
      "Epoch  6, CIFAR-10 Batch 5:  14:15:36.644662 Loss: 2297969.7500 Validation Accuracy: 0.352600\n",
      "Epoch  7, CIFAR-10 Batch 1:  14:15:39.629494 Loss: 2475635.0000 Validation Accuracy: 0.341800\n",
      "Epoch  7, CIFAR-10 Batch 2:  14:15:42.619510 Loss: 2261940.5000 Validation Accuracy: 0.359400\n",
      "Epoch  7, CIFAR-10 Batch 3:  14:15:45.606361 Loss: 2288551.7500 Validation Accuracy: 0.358400\n",
      "Epoch  7, CIFAR-10 Batch 4:  14:15:48.592526 Loss: 2161888.5000 Validation Accuracy: 0.350200\n",
      "Epoch  7, CIFAR-10 Batch 5:  14:15:51.578932 Loss: 2266252.2500 Validation Accuracy: 0.368400\n",
      "Epoch  8, CIFAR-10 Batch 1:  14:15:54.565978 Loss: 2216625.5000 Validation Accuracy: 0.368600\n",
      "Epoch  8, CIFAR-10 Batch 2:  14:15:57.552052 Loss: 1982184.7500 Validation Accuracy: 0.390200\n",
      "Epoch  8, CIFAR-10 Batch 3:  14:16:00.537624 Loss: 2082738.5000 Validation Accuracy: 0.383000\n",
      "Epoch  8, CIFAR-10 Batch 4:  14:16:03.525972 Loss: 2036354.7500 Validation Accuracy: 0.380600\n",
      "Epoch  8, CIFAR-10 Batch 5:  14:16:06.515252 Loss: 2054572.3750 Validation Accuracy: 0.395400\n",
      "Epoch  9, CIFAR-10 Batch 1:  14:16:09.505086 Loss: 2046984.7500 Validation Accuracy: 0.396600\n",
      "Epoch  9, CIFAR-10 Batch 2:  14:16:12.488576 Loss: 1981342.6250 Validation Accuracy: 0.390000\n",
      "Epoch  9, CIFAR-10 Batch 3:  14:16:15.472498 Loss: 2164240.5000 Validation Accuracy: 0.401800\n",
      "Epoch  9, CIFAR-10 Batch 4:  14:16:18.456513 Loss: 1618355.6250 Validation Accuracy: 0.405200\n",
      "Epoch  9, CIFAR-10 Batch 5:  14:16:21.440964 Loss: 1648224.1250 Validation Accuracy: 0.409000\n",
      "Epoch 10, CIFAR-10 Batch 1:  14:16:24.429597 Loss: 1781946.2500 Validation Accuracy: 0.402200\n",
      "Epoch 10, CIFAR-10 Batch 2:  14:16:27.414844 Loss: 1688400.8750 Validation Accuracy: 0.406600\n",
      "Epoch 10, CIFAR-10 Batch 3:  14:16:30.399506 Loss: 1775781.5000 Validation Accuracy: 0.407200\n",
      "Epoch 10, CIFAR-10 Batch 4:  14:16:33.381875 Loss: 1662866.2500 Validation Accuracy: 0.402000\n",
      "Epoch 10, CIFAR-10 Batch 5:  14:16:36.363484 Loss: 1624894.0000 Validation Accuracy: 0.409000\n",
      "Epoch 11, CIFAR-10 Batch 1:  14:16:39.349098 Loss: 1698434.8750 Validation Accuracy: 0.399000\n",
      "Epoch 11, CIFAR-10 Batch 2:  14:16:42.330982 Loss: 1521499.2500 Validation Accuracy: 0.412800\n",
      "Epoch 11, CIFAR-10 Batch 3:  14:16:45.315752 Loss: 1584750.5000 Validation Accuracy: 0.422600\n",
      "Epoch 11, CIFAR-10 Batch 4:  14:16:48.302150 Loss: 1451999.5000 Validation Accuracy: 0.408600\n",
      "Epoch 11, CIFAR-10 Batch 5:  14:16:51.292740 Loss: 1466921.0000 Validation Accuracy: 0.408400\n",
      "Epoch 12, CIFAR-10 Batch 1:  14:16:54.285295 Loss: 1589491.5000 Validation Accuracy: 0.403400\n",
      "Epoch 12, CIFAR-10 Batch 2:  14:16:57.291969 Loss: 1511937.7500 Validation Accuracy: 0.402200\n",
      "Epoch 12, CIFAR-10 Batch 3:  14:17:00.322388 Loss: 1605139.0000 Validation Accuracy: 0.421800\n",
      "Epoch 12, CIFAR-10 Batch 4:  14:17:03.348597 Loss: 1405353.1250 Validation Accuracy: 0.404400\n",
      "Epoch 12, CIFAR-10 Batch 5:  14:17:06.375181 Loss: 1495092.1250 Validation Accuracy: 0.404600\n",
      "Epoch 13, CIFAR-10 Batch 1:  14:17:09.403597 Loss: 1538523.2500 Validation Accuracy: 0.408200\n",
      "Epoch 13, CIFAR-10 Batch 2:  14:17:12.408143 Loss: 1501329.0000 Validation Accuracy: 0.409200\n",
      "Epoch 13, CIFAR-10 Batch 3:  14:17:15.438934 Loss: 1730457.6250 Validation Accuracy: 0.394000\n",
      "Epoch 13, CIFAR-10 Batch 4:  14:17:18.463853 Loss: 1325833.6250 Validation Accuracy: 0.430000\n",
      "Epoch 13, CIFAR-10 Batch 5:  14:17:21.492989 Loss: 1396856.8750 Validation Accuracy: 0.416800\n",
      "Epoch 14, CIFAR-10 Batch 1:  14:17:24.510241 Loss: 1367877.3750 Validation Accuracy: 0.428000\n",
      "Epoch 14, CIFAR-10 Batch 2:  14:17:27.537177 Loss: 1332857.2500 Validation Accuracy: 0.432000\n",
      "Epoch 14, CIFAR-10 Batch 3:  14:17:30.564278 Loss: 1448076.6250 Validation Accuracy: 0.421000\n",
      "Epoch 14, CIFAR-10 Batch 4:  14:17:33.590803 Loss: 1342434.1250 Validation Accuracy: 0.416800\n",
      "Epoch 14, CIFAR-10 Batch 5:  14:17:36.621946 Loss: 1280220.0000 Validation Accuracy: 0.435800\n",
      "Epoch 15, CIFAR-10 Batch 1:  14:17:39.652594 Loss: 1540834.2500 Validation Accuracy: 0.425800\n",
      "Epoch 15, CIFAR-10 Batch 2:  14:17:42.682194 Loss: 1412065.1250 Validation Accuracy: 0.433800\n",
      "Epoch 15, CIFAR-10 Batch 3:  14:17:45.713071 Loss: 2286134.0000 Validation Accuracy: 0.408000\n",
      "Epoch 15, CIFAR-10 Batch 4:  14:17:48.742031 Loss: 1598475.1250 Validation Accuracy: 0.391800\n",
      "Epoch 15, CIFAR-10 Batch 5:  14:17:51.767805 Loss: 1419508.2500 Validation Accuracy: 0.409400\n",
      "Epoch 16, CIFAR-10 Batch 1:  14:17:54.798506 Loss: 1388726.5000 Validation Accuracy: 0.443200\n",
      "Epoch 16, CIFAR-10 Batch 2:  14:17:57.826818 Loss: 1325902.7500 Validation Accuracy: 0.428000\n",
      "Epoch 16, CIFAR-10 Batch 3:  14:18:00.852788 Loss: 1512852.5000 Validation Accuracy: 0.434800\n",
      "Epoch 16, CIFAR-10 Batch 4:  14:18:03.873656 Loss: 1079914.1250 Validation Accuracy: 0.454200\n",
      "Epoch 16, CIFAR-10 Batch 5:  14:18:06.892758 Loss: 1217373.2500 Validation Accuracy: 0.442200\n",
      "Epoch 17, CIFAR-10 Batch 1:  14:18:09.907213 Loss: 1339079.7500 Validation Accuracy: 0.434600\n",
      "Epoch 17, CIFAR-10 Batch 2:  14:18:12.929193 Loss: 1246980.2500 Validation Accuracy: 0.441000\n",
      "Epoch 17, CIFAR-10 Batch 3:  14:18:15.956898 Loss: 1234943.1250 Validation Accuracy: 0.446000\n",
      "Epoch 17, CIFAR-10 Batch 4:  14:18:18.981660 Loss: 1086514.2500 Validation Accuracy: 0.438400\n",
      "Epoch 17, CIFAR-10 Batch 5:  14:18:22.010049 Loss: 1115376.6250 Validation Accuracy: 0.444800\n",
      "Epoch 18, CIFAR-10 Batch 1:  14:18:25.040661 Loss: 1220947.2500 Validation Accuracy: 0.434200\n",
      "Epoch 18, CIFAR-10 Batch 2:  14:18:28.071473 Loss: 1129750.7500 Validation Accuracy: 0.453800\n",
      "Epoch 18, CIFAR-10 Batch 3:  14:18:31.094191 Loss: 1256778.5000 Validation Accuracy: 0.445200\n",
      "Epoch 18, CIFAR-10 Batch 4:  14:18:34.128434 Loss: 1038513.0000 Validation Accuracy: 0.446800\n",
      "Epoch 18, CIFAR-10 Batch 5:  14:18:37.157205 Loss: 1125737.2500 Validation Accuracy: 0.443000\n",
      "Epoch 19, CIFAR-10 Batch 1:  14:18:40.188474 Loss: 1222378.3750 Validation Accuracy: 0.438200\n",
      "Epoch 19, CIFAR-10 Batch 2:  14:18:43.189710 Loss: 1242105.6250 Validation Accuracy: 0.440000\n",
      "Epoch 19, CIFAR-10 Batch 3:  14:18:46.217988 Loss: 1090204.8750 Validation Accuracy: 0.460800\n",
      "Epoch 19, CIFAR-10 Batch 4:  14:18:49.232286 Loss: 929528.7500 Validation Accuracy: 0.462600\n",
      "Epoch 19, CIFAR-10 Batch 5:  14:18:52.259203 Loss: 1059260.2500 Validation Accuracy: 0.445600\n",
      "Epoch 20, CIFAR-10 Batch 1:  14:18:55.286704 Loss: 1070245.7500 Validation Accuracy: 0.461200\n",
      "Epoch 20, CIFAR-10 Batch 2:  14:18:58.317145 Loss: 1160676.2500 Validation Accuracy: 0.445600\n",
      "Epoch 20, CIFAR-10 Batch 3:  14:19:01.343713 Loss: 1048258.8750 Validation Accuracy: 0.448400\n",
      "Epoch 20, CIFAR-10 Batch 4:  14:19:04.353983 Loss: 900079.9375 Validation Accuracy: 0.463800\n",
      "Epoch 20, CIFAR-10 Batch 5:  14:19:07.367796 Loss: 961318.3750 Validation Accuracy: 0.458600\n",
      "Epoch 21, CIFAR-10 Batch 1:  14:19:10.375129 Loss: 1035385.0625 Validation Accuracy: 0.458200\n",
      "Epoch 21, CIFAR-10 Batch 2:  14:19:13.392536 Loss: 1034685.1875 Validation Accuracy: 0.462800\n",
      "Epoch 21, CIFAR-10 Batch 3:  14:19:16.413109 Loss: 1024975.0625 Validation Accuracy: 0.461200\n",
      "Epoch 21, CIFAR-10 Batch 4:  14:19:19.433773 Loss: 863002.8750 Validation Accuracy: 0.461600\n",
      "Epoch 21, CIFAR-10 Batch 5:  14:19:22.462212 Loss: 957880.6875 Validation Accuracy: 0.451600\n",
      "Epoch 22, CIFAR-10 Batch 1:  14:19:25.491704 Loss: 994437.8750 Validation Accuracy: 0.447400\n",
      "Epoch 22, CIFAR-10 Batch 2:  14:19:28.520438 Loss: 1022683.0625 Validation Accuracy: 0.460200\n",
      "Epoch 22, CIFAR-10 Batch 3:  14:19:31.544095 Loss: 969811.8125 Validation Accuracy: 0.461000\n",
      "Epoch 22, CIFAR-10 Batch 4:  14:19:34.566406 Loss: 838937.5000 Validation Accuracy: 0.462400\n",
      "Epoch 22, CIFAR-10 Batch 5:  14:19:37.590298 Loss: 900381.3750 Validation Accuracy: 0.456800\n",
      "Epoch 23, CIFAR-10 Batch 1:  14:19:40.616537 Loss: 929409.1250 Validation Accuracy: 0.459000\n",
      "Epoch 23, CIFAR-10 Batch 2:  14:19:43.631651 Loss: 992407.0000 Validation Accuracy: 0.458400\n",
      "Epoch 23, CIFAR-10 Batch 3:  14:19:46.658376 Loss: 950969.8750 Validation Accuracy: 0.460600\n",
      "Epoch 23, CIFAR-10 Batch 4:  14:19:49.688957 Loss: 801365.3750 Validation Accuracy: 0.467000\n",
      "Epoch 23, CIFAR-10 Batch 5:  14:19:52.719769 Loss: 902910.6875 Validation Accuracy: 0.462000\n",
      "Epoch 24, CIFAR-10 Batch 1:  14:19:55.749502 Loss: 888869.5000 Validation Accuracy: 0.453800\n",
      "Epoch 24, CIFAR-10 Batch 2:  14:19:58.774522 Loss: 914694.3125 Validation Accuracy: 0.462400\n",
      "Epoch 24, CIFAR-10 Batch 3:  14:20:01.802973 Loss: 982568.1875 Validation Accuracy: 0.460400\n",
      "Epoch 24, CIFAR-10 Batch 4:  14:20:04.809085 Loss: 812338.0000 Validation Accuracy: 0.466800\n",
      "Epoch 24, CIFAR-10 Batch 5:  14:20:07.837163 Loss: 979000.9375 Validation Accuracy: 0.442800\n",
      "Epoch 25, CIFAR-10 Batch 1:  14:20:10.858983 Loss: 968415.2500 Validation Accuracy: 0.458400\n",
      "Epoch 25, CIFAR-10 Batch 2:  14:20:13.880461 Loss: 1139933.7500 Validation Accuracy: 0.451200\n",
      "Epoch 25, CIFAR-10 Batch 3:  14:20:16.910003 Loss: 1016708.9375 Validation Accuracy: 0.440200\n",
      "Epoch 25, CIFAR-10 Batch 4:  14:20:19.927596 Loss: 877119.2500 Validation Accuracy: 0.450200\n",
      "Epoch 25, CIFAR-10 Batch 5:  14:20:22.957321 Loss: 838270.1250 Validation Accuracy: 0.462200\n",
      "Epoch 26, CIFAR-10 Batch 1:  14:20:25.981663 Loss: 979982.4375 Validation Accuracy: 0.463400\n",
      "Epoch 26, CIFAR-10 Batch 2:  14:20:29.011118 Loss: 893246.7500 Validation Accuracy: 0.454400\n",
      "Epoch 26, CIFAR-10 Batch 3:  14:20:32.042142 Loss: 934802.5000 Validation Accuracy: 0.430600\n",
      "Epoch 26, CIFAR-10 Batch 4:  14:20:35.068969 Loss: 778550.5000 Validation Accuracy: 0.456600\n",
      "Epoch 26, CIFAR-10 Batch 5:  14:20:38.096257 Loss: 828671.1250 Validation Accuracy: 0.464800\n",
      "Epoch 27, CIFAR-10 Batch 1:  14:20:41.124294 Loss: 934856.0625 Validation Accuracy: 0.459000\n",
      "Epoch 27, CIFAR-10 Batch 2:  14:20:44.140752 Loss: 839626.6875 Validation Accuracy: 0.452600\n",
      "Epoch 27, CIFAR-10 Batch 3:  14:20:47.163730 Loss: 887855.1875 Validation Accuracy: 0.439600\n",
      "Epoch 27, CIFAR-10 Batch 4:  14:20:50.189737 Loss: 724531.0000 Validation Accuracy: 0.473000\n",
      "Epoch 27, CIFAR-10 Batch 5:  14:20:53.215504 Loss: 781890.3750 Validation Accuracy: 0.472600\n",
      "Epoch 28, CIFAR-10 Batch 1:  14:20:56.244121 Loss: 916026.4375 Validation Accuracy: 0.458800\n",
      "Epoch 28, CIFAR-10 Batch 2:  14:20:59.270687 Loss: 782334.6250 Validation Accuracy: 0.461400\n",
      "Epoch 28, CIFAR-10 Batch 3:  14:21:02.288750 Loss: 807431.3750 Validation Accuracy: 0.440600\n",
      "Epoch 28, CIFAR-10 Batch 4:  14:21:05.291729 Loss: 671425.6875 Validation Accuracy: 0.481200\n",
      "Epoch 28, CIFAR-10 Batch 5:  14:21:08.311883 Loss: 741631.5625 Validation Accuracy: 0.476200\n",
      "Epoch 29, CIFAR-10 Batch 1:  14:21:11.336313 Loss: 861486.1250 Validation Accuracy: 0.463400\n",
      "Epoch 29, CIFAR-10 Batch 2:  14:21:14.359736 Loss: 767970.2500 Validation Accuracy: 0.462400\n",
      "Epoch 29, CIFAR-10 Batch 3:  14:21:17.389037 Loss: 772333.7500 Validation Accuracy: 0.444800\n",
      "Epoch 29, CIFAR-10 Batch 4:  14:21:20.414640 Loss: 667130.4375 Validation Accuracy: 0.483200\n",
      "Epoch 29, CIFAR-10 Batch 5:  14:21:23.436600 Loss: 732588.6250 Validation Accuracy: 0.479600\n",
      "Epoch 30, CIFAR-10 Batch 1:  14:21:26.459794 Loss: 885260.5000 Validation Accuracy: 0.448400\n",
      "Epoch 30, CIFAR-10 Batch 2:  14:21:29.487741 Loss: 863753.7500 Validation Accuracy: 0.439800\n",
      "Epoch 30, CIFAR-10 Batch 3:  14:21:32.505381 Loss: 726339.0000 Validation Accuracy: 0.465400\n",
      "Epoch 30, CIFAR-10 Batch 4:  14:21:35.528207 Loss: 699381.2500 Validation Accuracy: 0.484600\n",
      "Epoch 30, CIFAR-10 Batch 5:  14:21:38.556606 Loss: 704237.5000 Validation Accuracy: 0.477200\n",
      "Epoch 31, CIFAR-10 Batch 1:  14:21:41.583931 Loss: 949688.3125 Validation Accuracy: 0.437200\n",
      "Epoch 31, CIFAR-10 Batch 2:  14:21:44.606649 Loss: 787079.0625 Validation Accuracy: 0.457600\n",
      "Epoch 31, CIFAR-10 Batch 3:  14:21:47.632054 Loss: 699255.2500 Validation Accuracy: 0.467400\n",
      "Epoch 31, CIFAR-10 Batch 4:  14:21:50.657321 Loss: 856045.3125 Validation Accuracy: 0.477800\n",
      "Epoch 31, CIFAR-10 Batch 5:  14:21:53.683460 Loss: 770530.2500 Validation Accuracy: 0.461400\n",
      "Epoch 32, CIFAR-10 Batch 1:  14:21:56.712403 Loss: 733716.2500 Validation Accuracy: 0.463800\n",
      "Epoch 32, CIFAR-10 Batch 2:  14:21:59.732973 Loss: 647740.9375 Validation Accuracy: 0.480800\n",
      "Epoch 32, CIFAR-10 Batch 3:  14:22:02.748244 Loss: 699044.7500 Validation Accuracy: 0.464000\n",
      "Epoch 32, CIFAR-10 Batch 4:  14:22:05.750531 Loss: 666082.8125 Validation Accuracy: 0.474000\n",
      "Epoch 32, CIFAR-10 Batch 5:  14:22:08.760768 Loss: 699073.7500 Validation Accuracy: 0.468400\n",
      "Epoch 33, CIFAR-10 Batch 1:  14:22:11.794081 Loss: 758912.6250 Validation Accuracy: 0.460400\n",
      "Epoch 33, CIFAR-10 Batch 2:  14:22:14.825674 Loss: 710562.2500 Validation Accuracy: 0.467200\n",
      "Epoch 33, CIFAR-10 Batch 3:  14:22:17.855838 Loss: 619338.2500 Validation Accuracy: 0.473200\n",
      "Epoch 33, CIFAR-10 Batch 4:  14:22:20.870452 Loss: 651138.6875 Validation Accuracy: 0.481600\n",
      "Epoch 33, CIFAR-10 Batch 5:  14:22:23.899693 Loss: 634346.8125 Validation Accuracy: 0.482800\n",
      "Epoch 34, CIFAR-10 Batch 1:  14:22:26.912974 Loss: 883956.5000 Validation Accuracy: 0.431000\n",
      "Epoch 34, CIFAR-10 Batch 2:  14:22:29.929695 Loss: 716871.7500 Validation Accuracy: 0.457600\n",
      "Epoch 34, CIFAR-10 Batch 3:  14:22:32.955123 Loss: 788363.5000 Validation Accuracy: 0.462400\n",
      "Epoch 34, CIFAR-10 Batch 4:  14:22:35.977789 Loss: 642955.8750 Validation Accuracy: 0.483800\n",
      "Epoch 34, CIFAR-10 Batch 5:  14:22:39.006067 Loss: 568376.2500 Validation Accuracy: 0.499000\n",
      "Epoch 35, CIFAR-10 Batch 1:  14:22:42.036752 Loss: 748100.9375 Validation Accuracy: 0.455000\n",
      "Epoch 35, CIFAR-10 Batch 2:  14:22:45.047091 Loss: 653677.5625 Validation Accuracy: 0.461400\n",
      "Epoch 35, CIFAR-10 Batch 3:  14:22:48.071438 Loss: 727713.3750 Validation Accuracy: 0.479600\n",
      "Epoch 35, CIFAR-10 Batch 4:  14:22:51.095194 Loss: 616820.6875 Validation Accuracy: 0.481600\n",
      "Epoch 35, CIFAR-10 Batch 5:  14:22:54.113699 Loss: 547601.8750 Validation Accuracy: 0.493600\n",
      "Epoch 36, CIFAR-10 Batch 1:  14:22:57.149769 Loss: 715253.7500 Validation Accuracy: 0.458600\n",
      "Epoch 36, CIFAR-10 Batch 2:  14:23:00.184206 Loss: 709773.5625 Validation Accuracy: 0.446600\n",
      "Epoch 36, CIFAR-10 Batch 3:  14:23:03.195760 Loss: 735295.5000 Validation Accuracy: 0.453000\n",
      "Epoch 36, CIFAR-10 Batch 4:  14:23:06.185101 Loss: 696012.7500 Validation Accuracy: 0.470800\n",
      "Epoch 36, CIFAR-10 Batch 5:  14:23:09.189012 Loss: 569279.8750 Validation Accuracy: 0.493600\n",
      "Epoch 37, CIFAR-10 Batch 1:  14:23:12.188616 Loss: 612064.6250 Validation Accuracy: 0.491000\n",
      "Epoch 37, CIFAR-10 Batch 2:  14:23:15.221969 Loss: 628720.3750 Validation Accuracy: 0.476800\n",
      "Epoch 37, CIFAR-10 Batch 3:  14:23:18.246275 Loss: 617881.3750 Validation Accuracy: 0.462200\n",
      "Epoch 37, CIFAR-10 Batch 4:  14:23:21.276854 Loss: 618498.1250 Validation Accuracy: 0.474800\n",
      "Epoch 37, CIFAR-10 Batch 5:  14:23:24.304153 Loss: 562655.0000 Validation Accuracy: 0.493200\n",
      "Epoch 38, CIFAR-10 Batch 1:  14:23:27.331471 Loss: 633687.3750 Validation Accuracy: 0.484000\n",
      "Epoch 38, CIFAR-10 Batch 2:  14:23:30.357281 Loss: 676951.2500 Validation Accuracy: 0.462800\n",
      "Epoch 38, CIFAR-10 Batch 3:  14:23:33.386129 Loss: 658775.5000 Validation Accuracy: 0.462200\n",
      "Epoch 38, CIFAR-10 Batch 4:  14:23:36.412670 Loss: 560334.7500 Validation Accuracy: 0.489800\n",
      "Epoch 38, CIFAR-10 Batch 5:  14:23:39.440876 Loss: 555368.3750 Validation Accuracy: 0.489200\n",
      "Epoch 39, CIFAR-10 Batch 1:  14:23:42.466191 Loss: 624564.8750 Validation Accuracy: 0.475000\n",
      "Epoch 39, CIFAR-10 Batch 2:  14:23:45.492543 Loss: 624140.3125 Validation Accuracy: 0.467200\n",
      "Epoch 39, CIFAR-10 Batch 3:  14:23:48.519339 Loss: 570967.8125 Validation Accuracy: 0.474800\n",
      "Epoch 39, CIFAR-10 Batch 4:  14:23:51.543256 Loss: 515812.2812 Validation Accuracy: 0.497200\n",
      "Epoch 39, CIFAR-10 Batch 5:  14:23:54.564962 Loss: 579113.3125 Validation Accuracy: 0.469800\n",
      "Epoch 40, CIFAR-10 Batch 1:  14:23:57.594445 Loss: 625931.3750 Validation Accuracy: 0.475600\n",
      "Epoch 40, CIFAR-10 Batch 2:  14:24:00.604588 Loss: 541020.6875 Validation Accuracy: 0.482200\n",
      "Epoch 40, CIFAR-10 Batch 3:  14:24:03.626839 Loss: 526726.3125 Validation Accuracy: 0.484000\n",
      "Epoch 40, CIFAR-10 Batch 4:  14:24:06.640136 Loss: 500651.6562 Validation Accuracy: 0.499400\n",
      "Epoch 40, CIFAR-10 Batch 5:  14:24:09.648699 Loss: 573897.1250 Validation Accuracy: 0.475400\n",
      "Epoch 41, CIFAR-10 Batch 1:  14:24:12.677963 Loss: 609950.1875 Validation Accuracy: 0.480600\n",
      "Epoch 41, CIFAR-10 Batch 2:  14:24:15.707445 Loss: 546156.6875 Validation Accuracy: 0.480200\n",
      "Epoch 41, CIFAR-10 Batch 3:  14:24:18.738904 Loss: 533222.0000 Validation Accuracy: 0.485800\n",
      "Epoch 41, CIFAR-10 Batch 4:  14:24:21.767683 Loss: 482287.1562 Validation Accuracy: 0.496600\n",
      "Epoch 41, CIFAR-10 Batch 5:  14:24:24.796728 Loss: 570821.3750 Validation Accuracy: 0.473000\n",
      "Epoch 42, CIFAR-10 Batch 1:  14:24:27.826689 Loss: 601175.8750 Validation Accuracy: 0.482600\n",
      "Epoch 42, CIFAR-10 Batch 2:  14:24:30.854921 Loss: 532334.0625 Validation Accuracy: 0.481200\n",
      "Epoch 42, CIFAR-10 Batch 3:  14:24:33.887216 Loss: 518741.5938 Validation Accuracy: 0.488000\n",
      "Epoch 42, CIFAR-10 Batch 4:  14:24:36.915137 Loss: 465734.5938 Validation Accuracy: 0.496600\n",
      "Epoch 42, CIFAR-10 Batch 5:  14:24:39.940844 Loss: 548980.2500 Validation Accuracy: 0.479400\n",
      "Epoch 43, CIFAR-10 Batch 1:  14:24:42.966477 Loss: 607153.4375 Validation Accuracy: 0.478600\n",
      "Epoch 43, CIFAR-10 Batch 2:  14:24:45.996562 Loss: 509962.2188 Validation Accuracy: 0.491600\n",
      "Epoch 43, CIFAR-10 Batch 3:  14:24:49.022927 Loss: 476327.0000 Validation Accuracy: 0.492200\n",
      "Epoch 43, CIFAR-10 Batch 4:  14:24:52.051477 Loss: 431686.7500 Validation Accuracy: 0.500800\n",
      "Epoch 43, CIFAR-10 Batch 5:  14:24:55.077396 Loss: 510402.4062 Validation Accuracy: 0.483800\n",
      "Epoch 44, CIFAR-10 Batch 1:  14:24:58.104255 Loss: 515592.0312 Validation Accuracy: 0.494000\n",
      "Epoch 44, CIFAR-10 Batch 2:  14:25:01.120936 Loss: 476953.1562 Validation Accuracy: 0.493200\n",
      "Epoch 44, CIFAR-10 Batch 3:  14:25:04.124439 Loss: 484129.4688 Validation Accuracy: 0.476400\n",
      "Epoch 44, CIFAR-10 Batch 4:  14:25:07.132569 Loss: 419995.5625 Validation Accuracy: 0.499600\n",
      "Epoch 44, CIFAR-10 Batch 5:  14:25:10.148762 Loss: 506267.4375 Validation Accuracy: 0.481000\n",
      "Epoch 45, CIFAR-10 Batch 1:  14:25:13.175844 Loss: 499138.0000 Validation Accuracy: 0.488600\n",
      "Epoch 45, CIFAR-10 Batch 2:  14:25:16.200940 Loss: 445253.2500 Validation Accuracy: 0.496600\n",
      "Epoch 45, CIFAR-10 Batch 3:  14:25:19.223469 Loss: 437450.5938 Validation Accuracy: 0.493400\n",
      "Epoch 45, CIFAR-10 Batch 4:  14:25:22.243765 Loss: 408820.8438 Validation Accuracy: 0.501400\n",
      "Epoch 45, CIFAR-10 Batch 5:  14:25:25.262394 Loss: 483143.7500 Validation Accuracy: 0.482000\n",
      "Epoch 46, CIFAR-10 Batch 1:  14:25:28.285255 Loss: 493316.1875 Validation Accuracy: 0.491800\n",
      "Epoch 46, CIFAR-10 Batch 2:  14:25:31.306422 Loss: 447145.2812 Validation Accuracy: 0.490800\n",
      "Epoch 46, CIFAR-10 Batch 3:  14:25:34.337153 Loss: 421169.7812 Validation Accuracy: 0.486800\n",
      "Epoch 46, CIFAR-10 Batch 4:  14:25:37.366956 Loss: 394663.1250 Validation Accuracy: 0.500800\n",
      "Epoch 46, CIFAR-10 Batch 5:  14:25:40.397714 Loss: 475532.8125 Validation Accuracy: 0.485400\n",
      "Epoch 47, CIFAR-10 Batch 1:  14:25:43.425183 Loss: 482508.1562 Validation Accuracy: 0.491000\n",
      "Epoch 47, CIFAR-10 Batch 2:  14:25:46.453040 Loss: 501105.7188 Validation Accuracy: 0.476800\n",
      "Epoch 47, CIFAR-10 Batch 3:  14:25:49.477536 Loss: 469158.8750 Validation Accuracy: 0.472000\n",
      "Epoch 47, CIFAR-10 Batch 4:  14:25:52.503488 Loss: 404050.5000 Validation Accuracy: 0.505000\n",
      "Epoch 47, CIFAR-10 Batch 5:  14:25:55.534604 Loss: 467026.7188 Validation Accuracy: 0.490000\n",
      "Epoch 48, CIFAR-10 Batch 1:  14:25:58.564623 Loss: 489799.1250 Validation Accuracy: 0.484400\n",
      "Epoch 48, CIFAR-10 Batch 2:  14:26:01.594760 Loss: 482499.6562 Validation Accuracy: 0.478200\n",
      "Epoch 48, CIFAR-10 Batch 3:  14:26:04.600545 Loss: 448699.0625 Validation Accuracy: 0.481600\n",
      "Epoch 48, CIFAR-10 Batch 4:  14:26:07.619492 Loss: 406588.7812 Validation Accuracy: 0.496000\n",
      "Epoch 48, CIFAR-10 Batch 5:  14:26:10.648303 Loss: 474657.3750 Validation Accuracy: 0.481600\n",
      "Epoch 49, CIFAR-10 Batch 1:  14:26:13.679537 Loss: 497942.0000 Validation Accuracy: 0.475000\n",
      "Epoch 49, CIFAR-10 Batch 2:  14:26:16.710232 Loss: 477345.1875 Validation Accuracy: 0.494000\n",
      "Epoch 49, CIFAR-10 Batch 3:  14:26:19.736530 Loss: 467835.4062 Validation Accuracy: 0.473000\n",
      "Epoch 49, CIFAR-10 Batch 4:  14:26:22.763644 Loss: 453039.9062 Validation Accuracy: 0.481200\n",
      "Epoch 49, CIFAR-10 Batch 5:  14:26:25.766729 Loss: 422691.0000 Validation Accuracy: 0.495800\n",
      "Epoch 50, CIFAR-10 Batch 1:  14:26:28.772472 Loss: 530564.7500 Validation Accuracy: 0.464800\n",
      "Epoch 50, CIFAR-10 Batch 2:  14:26:31.801942 Loss: 494688.0625 Validation Accuracy: 0.489200\n",
      "Epoch 50, CIFAR-10 Batch 3:  14:26:34.827973 Loss: 411693.8438 Validation Accuracy: 0.492600\n",
      "Epoch 50, CIFAR-10 Batch 4:  14:26:37.858937 Loss: 459103.7812 Validation Accuracy: 0.480400\n",
      "Epoch 50, CIFAR-10 Batch 5:  14:26:40.878581 Loss: 404883.3125 Validation Accuracy: 0.496800\n",
      "Epoch 51, CIFAR-10 Batch 1:  14:26:43.907621 Loss: 497861.3438 Validation Accuracy: 0.467200\n",
      "Epoch 51, CIFAR-10 Batch 2:  14:26:46.939666 Loss: 402587.3125 Validation Accuracy: 0.502200\n",
      "Epoch 51, CIFAR-10 Batch 3:  14:26:49.971144 Loss: 437473.4688 Validation Accuracy: 0.477200\n",
      "Epoch 51, CIFAR-10 Batch 4:  14:26:53.001194 Loss: 391516.0625 Validation Accuracy: 0.493800\n",
      "Epoch 51, CIFAR-10 Batch 5:  14:26:56.027217 Loss: 438394.6250 Validation Accuracy: 0.482200\n",
      "Epoch 52, CIFAR-10 Batch 1:  14:26:59.057750 Loss: 416202.2500 Validation Accuracy: 0.494200\n",
      "Epoch 52, CIFAR-10 Batch 2:  14:27:02.075645 Loss: 404468.9375 Validation Accuracy: 0.501000\n",
      "Epoch 52, CIFAR-10 Batch 3:  14:27:05.084179 Loss: 399022.2188 Validation Accuracy: 0.492200\n",
      "Epoch 52, CIFAR-10 Batch 4:  14:27:08.085015 Loss: 378306.8125 Validation Accuracy: 0.494200\n",
      "Epoch 52, CIFAR-10 Batch 5:  14:27:11.076549 Loss: 379905.1562 Validation Accuracy: 0.499400\n",
      "Epoch 53, CIFAR-10 Batch 1:  14:27:14.096795 Loss: 400805.8750 Validation Accuracy: 0.494000\n",
      "Epoch 53, CIFAR-10 Batch 2:  14:27:17.112496 Loss: 385172.9375 Validation Accuracy: 0.502400\n",
      "Epoch 53, CIFAR-10 Batch 3:  14:27:20.139842 Loss: 380668.4688 Validation Accuracy: 0.491000\n",
      "Epoch 53, CIFAR-10 Batch 4:  14:27:23.168996 Loss: 367925.9062 Validation Accuracy: 0.491800\n",
      "Epoch 53, CIFAR-10 Batch 5:  14:27:26.185581 Loss: 388499.0938 Validation Accuracy: 0.495800\n",
      "Epoch 54, CIFAR-10 Batch 1:  14:27:29.199223 Loss: 403344.6875 Validation Accuracy: 0.493400\n",
      "Epoch 54, CIFAR-10 Batch 2:  14:27:32.225777 Loss: 395840.3750 Validation Accuracy: 0.495400\n",
      "Epoch 54, CIFAR-10 Batch 3:  14:27:35.248988 Loss: 362674.4062 Validation Accuracy: 0.491200\n",
      "Epoch 54, CIFAR-10 Batch 4:  14:27:38.276683 Loss: 334371.1875 Validation Accuracy: 0.505600\n",
      "Epoch 54, CIFAR-10 Batch 5:  14:27:41.302744 Loss: 380049.2500 Validation Accuracy: 0.494200\n",
      "Epoch 55, CIFAR-10 Batch 1:  14:27:44.333050 Loss: 381232.7500 Validation Accuracy: 0.495200\n",
      "Epoch 55, CIFAR-10 Batch 2:  14:27:47.363958 Loss: 385694.7188 Validation Accuracy: 0.494200\n",
      "Epoch 55, CIFAR-10 Batch 3:  14:27:50.385088 Loss: 359579.4375 Validation Accuracy: 0.492600\n",
      "Epoch 55, CIFAR-10 Batch 4:  14:27:53.413576 Loss: 324776.2500 Validation Accuracy: 0.505600\n",
      "Epoch 55, CIFAR-10 Batch 5:  14:27:56.439746 Loss: 380751.6875 Validation Accuracy: 0.486200\n",
      "Epoch 56, CIFAR-10 Batch 1:  14:27:59.468068 Loss: 406219.4062 Validation Accuracy: 0.481400\n",
      "Epoch 56, CIFAR-10 Batch 2:  14:28:02.485783 Loss: 400965.7812 Validation Accuracy: 0.495000\n",
      "Epoch 56, CIFAR-10 Batch 3:  14:28:05.499820 Loss: 404839.3750 Validation Accuracy: 0.485800\n",
      "Epoch 56, CIFAR-10 Batch 4:  14:28:08.522691 Loss: 341580.5938 Validation Accuracy: 0.501200\n",
      "Epoch 56, CIFAR-10 Batch 5:  14:28:11.517234 Loss: 330220.1562 Validation Accuracy: 0.508000\n",
      "Epoch 57, CIFAR-10 Batch 1:  14:28:14.523813 Loss: 363774.3750 Validation Accuracy: 0.498400\n",
      "Epoch 57, CIFAR-10 Batch 2:  14:28:17.522221 Loss: 367788.6875 Validation Accuracy: 0.501000\n",
      "Epoch 57, CIFAR-10 Batch 3:  14:28:20.537066 Loss: 394116.0938 Validation Accuracy: 0.491200\n",
      "Epoch 57, CIFAR-10 Batch 4:  14:28:23.563394 Loss: 333080.0625 Validation Accuracy: 0.501600\n",
      "Epoch 57, CIFAR-10 Batch 5:  14:28:26.584697 Loss: 332221.2188 Validation Accuracy: 0.508800\n",
      "Epoch 58, CIFAR-10 Batch 1:  14:28:29.605362 Loss: 378855.3750 Validation Accuracy: 0.484600\n",
      "Epoch 58, CIFAR-10 Batch 2:  14:28:32.628720 Loss: 365089.4375 Validation Accuracy: 0.507400\n",
      "Epoch 58, CIFAR-10 Batch 3:  14:28:35.638328 Loss: 430432.7500 Validation Accuracy: 0.486800\n",
      "Epoch 58, CIFAR-10 Batch 4:  14:28:38.667305 Loss: 329898.1250 Validation Accuracy: 0.499200\n",
      "Epoch 58, CIFAR-10 Batch 5:  14:28:41.684023 Loss: 375849.6562 Validation Accuracy: 0.498400\n",
      "Epoch 59, CIFAR-10 Batch 1:  14:28:44.697067 Loss: 366233.0625 Validation Accuracy: 0.500000\n",
      "Epoch 59, CIFAR-10 Batch 2:  14:28:47.722015 Loss: 348490.6875 Validation Accuracy: 0.505000\n",
      "Epoch 59, CIFAR-10 Batch 3:  14:28:50.738222 Loss: 352261.6562 Validation Accuracy: 0.487800\n",
      "Epoch 59, CIFAR-10 Batch 4:  14:28:53.763557 Loss: 326354.4688 Validation Accuracy: 0.491600\n",
      "Epoch 59, CIFAR-10 Batch 5:  14:28:56.791675 Loss: 326855.3750 Validation Accuracy: 0.499600\n",
      "Epoch 60, CIFAR-10 Batch 1:  14:28:59.820597 Loss: 366302.9375 Validation Accuracy: 0.501000\n",
      "Epoch 60, CIFAR-10 Batch 2:  14:29:02.846005 Loss: 364970.8125 Validation Accuracy: 0.500800\n",
      "Epoch 60, CIFAR-10 Batch 3:  14:29:05.858833 Loss: 356019.6562 Validation Accuracy: 0.488800\n",
      "Epoch 60, CIFAR-10 Batch 4:  14:29:08.880569 Loss: 337356.2500 Validation Accuracy: 0.493600\n",
      "Epoch 60, CIFAR-10 Batch 5:  14:29:11.900976 Loss: 319321.8750 Validation Accuracy: 0.497200\n",
      "Epoch 61, CIFAR-10 Batch 1:  14:29:14.929425 Loss: 327290.8750 Validation Accuracy: 0.508000\n",
      "Epoch 61, CIFAR-10 Batch 2:  14:29:17.957253 Loss: 342237.3438 Validation Accuracy: 0.506000\n",
      "Epoch 61, CIFAR-10 Batch 3:  14:29:20.978238 Loss: 361913.5938 Validation Accuracy: 0.482000\n",
      "Epoch 61, CIFAR-10 Batch 4:  14:29:23.989536 Loss: 347666.9375 Validation Accuracy: 0.486800\n",
      "Epoch 61, CIFAR-10 Batch 5:  14:29:27.015923 Loss: 308181.2812 Validation Accuracy: 0.500600\n",
      "Epoch 62, CIFAR-10 Batch 1:  14:29:30.045901 Loss: 338869.7500 Validation Accuracy: 0.501400\n",
      "Epoch 62, CIFAR-10 Batch 2:  14:29:33.077424 Loss: 320243.1250 Validation Accuracy: 0.510000\n",
      "Epoch 62, CIFAR-10 Batch 3:  14:29:36.102645 Loss: 308128.0000 Validation Accuracy: 0.503600\n",
      "Epoch 62, CIFAR-10 Batch 4:  14:29:39.130010 Loss: 295766.2812 Validation Accuracy: 0.508000\n",
      "Epoch 62, CIFAR-10 Batch 5:  14:29:42.156868 Loss: 308196.9375 Validation Accuracy: 0.500800\n",
      "Epoch 63, CIFAR-10 Batch 1:  14:29:45.183310 Loss: 367367.7500 Validation Accuracy: 0.491000\n",
      "Epoch 63, CIFAR-10 Batch 2:  14:29:48.206837 Loss: 343721.4375 Validation Accuracy: 0.505800\n",
      "Epoch 63, CIFAR-10 Batch 3:  14:29:51.233349 Loss: 338581.4375 Validation Accuracy: 0.495800\n",
      "Epoch 63, CIFAR-10 Batch 4:  14:29:54.259408 Loss: 340706.5625 Validation Accuracy: 0.490200\n",
      "Epoch 63, CIFAR-10 Batch 5:  14:29:57.284701 Loss: 349619.6250 Validation Accuracy: 0.480600\n",
      "Epoch 64, CIFAR-10 Batch 1:  14:30:00.315149 Loss: 367933.0000 Validation Accuracy: 0.492200\n",
      "Epoch 64, CIFAR-10 Batch 2:  14:30:03.312683 Loss: 363504.2500 Validation Accuracy: 0.498800\n",
      "Epoch 64, CIFAR-10 Batch 3:  14:30:06.300863 Loss: 471901.0625 Validation Accuracy: 0.466600\n",
      "Epoch 64, CIFAR-10 Batch 4:  14:30:09.327200 Loss: 288318.4375 Validation Accuracy: 0.507200\n",
      "Epoch 64, CIFAR-10 Batch 5:  14:30:12.356087 Loss: 317506.1562 Validation Accuracy: 0.491400\n",
      "Epoch 65, CIFAR-10 Batch 1:  14:30:15.382315 Loss: 342122.1875 Validation Accuracy: 0.491400\n",
      "Epoch 65, CIFAR-10 Batch 2:  14:30:18.404578 Loss: 329819.9375 Validation Accuracy: 0.508800\n",
      "Epoch 65, CIFAR-10 Batch 3:  14:30:21.430708 Loss: 405027.3438 Validation Accuracy: 0.469400\n",
      "Epoch 65, CIFAR-10 Batch 4:  14:30:24.460761 Loss: 296259.0312 Validation Accuracy: 0.502200\n",
      "Epoch 65, CIFAR-10 Batch 5:  14:30:27.486248 Loss: 353209.5625 Validation Accuracy: 0.483600\n",
      "Epoch 66, CIFAR-10 Batch 1:  14:30:30.504854 Loss: 352181.1562 Validation Accuracy: 0.486200\n",
      "Epoch 66, CIFAR-10 Batch 2:  14:30:33.532929 Loss: 323443.4375 Validation Accuracy: 0.506600\n",
      "Epoch 66, CIFAR-10 Batch 3:  14:30:36.560131 Loss: 298048.0625 Validation Accuracy: 0.498400\n",
      "Epoch 66, CIFAR-10 Batch 4:  14:30:39.588581 Loss: 353397.7188 Validation Accuracy: 0.466600\n",
      "Epoch 66, CIFAR-10 Batch 5:  14:30:42.613215 Loss: 401141.9062 Validation Accuracy: 0.469400\n",
      "Epoch 67, CIFAR-10 Batch 1:  14:30:45.645038 Loss: 365383.0625 Validation Accuracy: 0.480200\n",
      "Epoch 67, CIFAR-10 Batch 2:  14:30:48.672641 Loss: 314532.1562 Validation Accuracy: 0.508200\n",
      "Epoch 67, CIFAR-10 Batch 3:  14:30:51.693362 Loss: 399065.4062 Validation Accuracy: 0.463400\n",
      "Epoch 67, CIFAR-10 Batch 4:  14:30:54.709540 Loss: 324906.5625 Validation Accuracy: 0.487400\n",
      "Epoch 67, CIFAR-10 Batch 5:  14:30:57.732198 Loss: 348728.4062 Validation Accuracy: 0.487000\n",
      "Epoch 68, CIFAR-10 Batch 1:  14:31:00.759131 Loss: 301757.4375 Validation Accuracy: 0.512000\n",
      "Epoch 68, CIFAR-10 Batch 2:  14:31:03.780292 Loss: 290059.9375 Validation Accuracy: 0.513200\n",
      "Epoch 68, CIFAR-10 Batch 3:  14:31:06.779472 Loss: 380268.3750 Validation Accuracy: 0.479800\n",
      "Epoch 68, CIFAR-10 Batch 4:  14:31:09.805833 Loss: 273899.0938 Validation Accuracy: 0.494200\n",
      "Epoch 68, CIFAR-10 Batch 5:  14:31:12.828581 Loss: 325112.8125 Validation Accuracy: 0.484400\n",
      "Epoch 69, CIFAR-10 Batch 1:  14:31:15.856420 Loss: 278913.1562 Validation Accuracy: 0.521600\n",
      "Epoch 69, CIFAR-10 Batch 2:  14:31:18.871209 Loss: 296085.9688 Validation Accuracy: 0.506200\n",
      "Epoch 69, CIFAR-10 Batch 3:  14:31:21.897164 Loss: 366665.8750 Validation Accuracy: 0.487400\n",
      "Epoch 69, CIFAR-10 Batch 4:  14:31:24.924204 Loss: 260181.0312 Validation Accuracy: 0.501600\n",
      "Epoch 69, CIFAR-10 Batch 5:  14:31:27.949786 Loss: 311443.0000 Validation Accuracy: 0.493800\n",
      "Epoch 70, CIFAR-10 Batch 1:  14:31:30.973900 Loss: 263730.8750 Validation Accuracy: 0.522800\n",
      "Epoch 70, CIFAR-10 Batch 2:  14:31:33.999788 Loss: 303126.5000 Validation Accuracy: 0.496000\n",
      "Epoch 70, CIFAR-10 Batch 3:  14:31:37.029041 Loss: 368275.7812 Validation Accuracy: 0.484400\n",
      "Epoch 70, CIFAR-10 Batch 4:  14:31:40.055822 Loss: 268995.5938 Validation Accuracy: 0.501400\n",
      "Epoch 70, CIFAR-10 Batch 5:  14:31:43.053290 Loss: 322659.2500 Validation Accuracy: 0.489200\n",
      "Epoch 71, CIFAR-10 Batch 1:  14:31:46.088404 Loss: 267712.2500 Validation Accuracy: 0.512800\n",
      "Epoch 71, CIFAR-10 Batch 2:  14:31:49.109038 Loss: 267365.0000 Validation Accuracy: 0.505000\n",
      "Epoch 71, CIFAR-10 Batch 3:  14:31:52.134084 Loss: 303875.6250 Validation Accuracy: 0.500400\n",
      "Epoch 71, CIFAR-10 Batch 4:  14:31:55.159866 Loss: 233685.8281 Validation Accuracy: 0.513600\n",
      "Epoch 71, CIFAR-10 Batch 5:  14:31:58.184489 Loss: 249780.3594 Validation Accuracy: 0.506800\n",
      "Epoch 72, CIFAR-10 Batch 1:  14:32:01.210376 Loss: 242607.7031 Validation Accuracy: 0.516200\n",
      "Epoch 72, CIFAR-10 Batch 2:  14:32:04.228244 Loss: 257870.4844 Validation Accuracy: 0.498000\n",
      "Epoch 72, CIFAR-10 Batch 3:  14:32:07.254302 Loss: 279918.3438 Validation Accuracy: 0.507400\n",
      "Epoch 72, CIFAR-10 Batch 4:  14:32:10.287218 Loss: 220061.9688 Validation Accuracy: 0.510200\n",
      "Epoch 72, CIFAR-10 Batch 5:  14:32:13.302729 Loss: 235919.3438 Validation Accuracy: 0.509200\n",
      "Epoch 73, CIFAR-10 Batch 1:  14:32:16.313821 Loss: 239187.3281 Validation Accuracy: 0.513600\n",
      "Epoch 73, CIFAR-10 Batch 2:  14:32:19.333084 Loss: 242348.8125 Validation Accuracy: 0.508600\n",
      "Epoch 73, CIFAR-10 Batch 3:  14:32:22.359595 Loss: 269878.2812 Validation Accuracy: 0.503800\n",
      "Epoch 73, CIFAR-10 Batch 4:  14:32:25.386455 Loss: 228426.0312 Validation Accuracy: 0.501800\n",
      "Epoch 73, CIFAR-10 Batch 5:  14:32:28.414630 Loss: 226517.8594 Validation Accuracy: 0.506600\n",
      "Epoch 74, CIFAR-10 Batch 1:  14:32:31.438689 Loss: 225690.1406 Validation Accuracy: 0.518000\n",
      "Epoch 74, CIFAR-10 Batch 2:  14:32:34.467219 Loss: 224475.8750 Validation Accuracy: 0.516200\n",
      "Epoch 74, CIFAR-10 Batch 3:  14:32:37.495599 Loss: 262073.9062 Validation Accuracy: 0.505400\n",
      "Epoch 74, CIFAR-10 Batch 4:  14:32:40.516552 Loss: 247238.0469 Validation Accuracy: 0.493000\n",
      "Epoch 74, CIFAR-10 Batch 5:  14:32:43.541114 Loss: 280874.5000 Validation Accuracy: 0.496200\n",
      "Epoch 75, CIFAR-10 Batch 1:  14:32:46.568783 Loss: 240562.0000 Validation Accuracy: 0.511800\n",
      "Epoch 75, CIFAR-10 Batch 2:  14:32:49.593498 Loss: 233935.2188 Validation Accuracy: 0.518800\n",
      "Epoch 75, CIFAR-10 Batch 3:  14:32:52.622460 Loss: 284971.9688 Validation Accuracy: 0.494600\n",
      "Epoch 75, CIFAR-10 Batch 4:  14:32:55.648817 Loss: 246483.7812 Validation Accuracy: 0.493000\n",
      "Epoch 75, CIFAR-10 Batch 5:  14:32:58.673347 Loss: 235724.7344 Validation Accuracy: 0.505400\n",
      "Epoch 76, CIFAR-10 Batch 1:  14:33:01.699945 Loss: 242465.4688 Validation Accuracy: 0.503200\n",
      "Epoch 76, CIFAR-10 Batch 2:  14:33:04.712815 Loss: 221792.9062 Validation Accuracy: 0.506800\n",
      "Epoch 76, CIFAR-10 Batch 3:  14:33:07.729319 Loss: 249742.0625 Validation Accuracy: 0.504400\n",
      "Epoch 76, CIFAR-10 Batch 4:  14:33:10.758915 Loss: 231999.2188 Validation Accuracy: 0.492400\n",
      "Epoch 76, CIFAR-10 Batch 5:  14:33:13.782000 Loss: 239712.0469 Validation Accuracy: 0.503000\n",
      "Epoch 77, CIFAR-10 Batch 1:  14:33:16.808026 Loss: 231850.6875 Validation Accuracy: 0.506600\n",
      "Epoch 77, CIFAR-10 Batch 2:  14:33:19.830678 Loss: 222014.5000 Validation Accuracy: 0.506200\n",
      "Epoch 77, CIFAR-10 Batch 3:  14:33:22.861462 Loss: 265017.6250 Validation Accuracy: 0.499400\n",
      "Epoch 77, CIFAR-10 Batch 4:  14:33:25.844938 Loss: 233597.8281 Validation Accuracy: 0.489800\n",
      "Epoch 77, CIFAR-10 Batch 5:  14:33:28.828512 Loss: 265208.5000 Validation Accuracy: 0.491800\n",
      "Epoch 78, CIFAR-10 Batch 1:  14:33:31.808915 Loss: 274623.3125 Validation Accuracy: 0.488200\n",
      "Epoch 78, CIFAR-10 Batch 2:  14:33:34.790032 Loss: 239941.6719 Validation Accuracy: 0.512000\n",
      "Epoch 78, CIFAR-10 Batch 3:  14:33:37.772983 Loss: 244454.3438 Validation Accuracy: 0.515200\n",
      "Epoch 78, CIFAR-10 Batch 4:  14:33:40.758072 Loss: 261252.2500 Validation Accuracy: 0.485600\n",
      "Epoch 78, CIFAR-10 Batch 5:  14:33:43.744368 Loss: 249111.4062 Validation Accuracy: 0.495800\n",
      "Epoch 79, CIFAR-10 Batch 1:  14:33:46.733005 Loss: 225687.6094 Validation Accuracy: 0.512200\n",
      "Epoch 79, CIFAR-10 Batch 2:  14:33:49.714730 Loss: 239199.6094 Validation Accuracy: 0.498000\n",
      "Epoch 79, CIFAR-10 Batch 3:  14:33:52.699955 Loss: 261586.6875 Validation Accuracy: 0.505000\n",
      "Epoch 79, CIFAR-10 Batch 4:  14:33:55.685161 Loss: 238307.3281 Validation Accuracy: 0.483800\n",
      "Epoch 79, CIFAR-10 Batch 5:  14:33:58.664076 Loss: 274294.8438 Validation Accuracy: 0.490800\n",
      "Epoch 80, CIFAR-10 Batch 1:  14:34:01.649457 Loss: 240584.9219 Validation Accuracy: 0.504400\n",
      "Epoch 80, CIFAR-10 Batch 2:  14:34:04.632889 Loss: 237474.0781 Validation Accuracy: 0.500600\n",
      "Epoch 80, CIFAR-10 Batch 3:  14:34:07.616853 Loss: 276901.5312 Validation Accuracy: 0.497800\n",
      "Epoch 80, CIFAR-10 Batch 4:  14:34:10.601330 Loss: 256138.7500 Validation Accuracy: 0.478800\n",
      "Epoch 80, CIFAR-10 Batch 5:  14:34:13.582748 Loss: 274593.0000 Validation Accuracy: 0.487800\n",
      "Epoch 81, CIFAR-10 Batch 1:  14:34:16.567487 Loss: 274357.1562 Validation Accuracy: 0.489000\n",
      "Epoch 81, CIFAR-10 Batch 2:  14:34:19.554396 Loss: 337548.9688 Validation Accuracy: 0.474000\n",
      "Epoch 81, CIFAR-10 Batch 3:  14:34:22.539922 Loss: 322516.5000 Validation Accuracy: 0.470200\n",
      "Epoch 81, CIFAR-10 Batch 4:  14:34:25.526135 Loss: 240592.3125 Validation Accuracy: 0.495800\n",
      "Epoch 81, CIFAR-10 Batch 5:  14:34:28.509873 Loss: 221541.0781 Validation Accuracy: 0.504800\n",
      "Epoch 82, CIFAR-10 Batch 1:  14:34:31.492939 Loss: 240131.6562 Validation Accuracy: 0.511200\n",
      "Epoch 82, CIFAR-10 Batch 2:  14:34:34.477898 Loss: 249345.0625 Validation Accuracy: 0.487200\n",
      "Epoch 82, CIFAR-10 Batch 3:  14:34:37.458201 Loss: 334495.3750 Validation Accuracy: 0.465400\n",
      "Epoch 82, CIFAR-10 Batch 4:  14:34:40.440115 Loss: 234891.4688 Validation Accuracy: 0.488800\n",
      "Epoch 82, CIFAR-10 Batch 5:  14:34:43.424983 Loss: 240317.2188 Validation Accuracy: 0.496600\n",
      "Epoch 83, CIFAR-10 Batch 1:  14:34:46.409929 Loss: 226641.3281 Validation Accuracy: 0.510200\n",
      "Epoch 83, CIFAR-10 Batch 2:  14:34:49.390824 Loss: 242705.4531 Validation Accuracy: 0.489000\n",
      "Epoch 83, CIFAR-10 Batch 3:  14:34:52.374375 Loss: 305077.5938 Validation Accuracy: 0.471000\n",
      "Epoch 83, CIFAR-10 Batch 4:  14:34:55.357450 Loss: 271207.6562 Validation Accuracy: 0.470200\n",
      "Epoch 83, CIFAR-10 Batch 5:  14:34:58.339487 Loss: 234660.2656 Validation Accuracy: 0.493400\n",
      "Epoch 84, CIFAR-10 Batch 1:  14:35:01.323403 Loss: 248655.2344 Validation Accuracy: 0.496200\n",
      "Epoch 84, CIFAR-10 Batch 2:  14:35:04.312679 Loss: 222624.0000 Validation Accuracy: 0.497600\n",
      "Epoch 84, CIFAR-10 Batch 3:  14:35:07.297250 Loss: 273340.6875 Validation Accuracy: 0.484000\n",
      "Epoch 84, CIFAR-10 Batch 4:  14:35:10.281008 Loss: 242647.6250 Validation Accuracy: 0.474800\n",
      "Epoch 84, CIFAR-10 Batch 5:  14:35:13.268916 Loss: 207985.7188 Validation Accuracy: 0.504200\n",
      "Epoch 85, CIFAR-10 Batch 1:  14:35:16.255514 Loss: 212161.7500 Validation Accuracy: 0.508600\n",
      "Epoch 85, CIFAR-10 Batch 2:  14:35:19.241550 Loss: 199418.3438 Validation Accuracy: 0.506400\n",
      "Epoch 85, CIFAR-10 Batch 3:  14:35:22.226724 Loss: 237786.4531 Validation Accuracy: 0.496200\n",
      "Epoch 85, CIFAR-10 Batch 4:  14:35:25.205638 Loss: 212539.5938 Validation Accuracy: 0.492400\n",
      "Epoch 85, CIFAR-10 Batch 5:  14:35:28.186982 Loss: 182503.7969 Validation Accuracy: 0.506200\n",
      "Epoch 86, CIFAR-10 Batch 1:  14:35:31.171098 Loss: 190498.8906 Validation Accuracy: 0.517800\n",
      "Epoch 86, CIFAR-10 Batch 2:  14:35:34.155932 Loss: 191649.9844 Validation Accuracy: 0.506000\n",
      "Epoch 86, CIFAR-10 Batch 3:  14:35:37.141055 Loss: 221394.6719 Validation Accuracy: 0.509400\n",
      "Epoch 86, CIFAR-10 Batch 4:  14:35:40.127177 Loss: 182562.6406 Validation Accuracy: 0.506200\n",
      "Epoch 86, CIFAR-10 Batch 5:  14:35:43.110353 Loss: 182190.1719 Validation Accuracy: 0.506200\n",
      "Epoch 87, CIFAR-10 Batch 1:  14:35:46.095859 Loss: 183233.0469 Validation Accuracy: 0.517200\n",
      "Epoch 87, CIFAR-10 Batch 2:  14:35:49.079888 Loss: 190902.0156 Validation Accuracy: 0.507800\n",
      "Epoch 87, CIFAR-10 Batch 3:  14:35:52.064540 Loss: 183286.6875 Validation Accuracy: 0.521400\n",
      "Epoch 87, CIFAR-10 Batch 4:  14:35:55.049850 Loss: 153930.7031 Validation Accuracy: 0.522600\n",
      "Epoch 87, CIFAR-10 Batch 5:  14:35:58.030332 Loss: 163726.9219 Validation Accuracy: 0.513800\n",
      "Epoch 88, CIFAR-10 Batch 1:  14:36:01.016311 Loss: 173564.7969 Validation Accuracy: 0.516000\n",
      "Epoch 88, CIFAR-10 Batch 2:  14:36:04.004202 Loss: 195978.0938 Validation Accuracy: 0.498800\n",
      "Epoch 88, CIFAR-10 Batch 3:  14:36:06.992683 Loss: 169597.4531 Validation Accuracy: 0.527600\n",
      "Epoch 88, CIFAR-10 Batch 4:  14:36:09.980248 Loss: 145976.0625 Validation Accuracy: 0.523200\n",
      "Epoch 88, CIFAR-10 Batch 5:  14:36:12.965273 Loss: 162202.5938 Validation Accuracy: 0.516400\n",
      "Epoch 89, CIFAR-10 Batch 1:  14:36:15.951782 Loss: 172113.1875 Validation Accuracy: 0.519000\n",
      "Epoch 89, CIFAR-10 Batch 2:  14:36:18.939271 Loss: 183851.6094 Validation Accuracy: 0.510200\n",
      "Epoch 89, CIFAR-10 Batch 3:  14:36:21.927435 Loss: 163453.5469 Validation Accuracy: 0.528400\n",
      "Epoch 89, CIFAR-10 Batch 4:  14:36:24.914396 Loss: 142395.8594 Validation Accuracy: 0.523000\n",
      "Epoch 89, CIFAR-10 Batch 5:  14:36:27.896479 Loss: 156967.1094 Validation Accuracy: 0.517600\n",
      "Epoch 90, CIFAR-10 Batch 1:  14:36:30.876696 Loss: 167600.3906 Validation Accuracy: 0.518000\n",
      "Epoch 90, CIFAR-10 Batch 2:  14:36:33.860648 Loss: 177726.9062 Validation Accuracy: 0.507800\n",
      "Epoch 90, CIFAR-10 Batch 3:  14:36:36.844597 Loss: 169899.4531 Validation Accuracy: 0.524000\n",
      "Epoch 90, CIFAR-10 Batch 4:  14:36:39.829164 Loss: 149823.8750 Validation Accuracy: 0.519400\n",
      "Epoch 90, CIFAR-10 Batch 5:  14:36:42.814299 Loss: 153447.1094 Validation Accuracy: 0.516200\n",
      "Epoch 91, CIFAR-10 Batch 1:  14:36:45.800311 Loss: 164457.3438 Validation Accuracy: 0.528400\n",
      "Epoch 91, CIFAR-10 Batch 2:  14:36:48.784657 Loss: 176443.9531 Validation Accuracy: 0.511600\n",
      "Epoch 91, CIFAR-10 Batch 3:  14:36:51.770270 Loss: 172074.4688 Validation Accuracy: 0.524000\n",
      "Epoch 91, CIFAR-10 Batch 4:  14:36:54.754961 Loss: 155910.6406 Validation Accuracy: 0.514200\n",
      "Epoch 91, CIFAR-10 Batch 5:  14:36:57.737577 Loss: 155376.4375 Validation Accuracy: 0.518400\n",
      "Epoch 92, CIFAR-10 Batch 1:  14:37:00.725798 Loss: 173377.6875 Validation Accuracy: 0.532400\n",
      "Epoch 92, CIFAR-10 Batch 2:  14:37:03.710053 Loss: 169607.5469 Validation Accuracy: 0.510000\n",
      "Epoch 92, CIFAR-10 Batch 3:  14:37:06.697536 Loss: 193361.6094 Validation Accuracy: 0.518200\n",
      "Epoch 92, CIFAR-10 Batch 4:  14:37:09.681210 Loss: 168595.9219 Validation Accuracy: 0.504600\n",
      "Epoch 92, CIFAR-10 Batch 5:  14:37:12.685083 Loss: 170830.3594 Validation Accuracy: 0.519600\n",
      "Epoch 93, CIFAR-10 Batch 1:  14:37:15.717941 Loss: 185246.7812 Validation Accuracy: 0.528800\n",
      "Epoch 93, CIFAR-10 Batch 2:  14:37:18.746668 Loss: 192674.2031 Validation Accuracy: 0.501000\n",
      "Epoch 93, CIFAR-10 Batch 3:  14:37:21.770321 Loss: 232337.2500 Validation Accuracy: 0.496200\n",
      "Epoch 93, CIFAR-10 Batch 4:  14:37:24.789527 Loss: 180910.8125 Validation Accuracy: 0.499600\n",
      "Epoch 93, CIFAR-10 Batch 5:  14:37:27.806624 Loss: 203710.2188 Validation Accuracy: 0.503800\n",
      "Epoch 94, CIFAR-10 Batch 1:  14:37:30.823483 Loss: 178323.1094 Validation Accuracy: 0.521400\n",
      "Epoch 94, CIFAR-10 Batch 2:  14:37:33.850090 Loss: 171771.7656 Validation Accuracy: 0.516800\n",
      "Epoch 94, CIFAR-10 Batch 3:  14:37:36.877664 Loss: 214241.8438 Validation Accuracy: 0.504800\n",
      "Epoch 94, CIFAR-10 Batch 4:  14:37:39.901234 Loss: 151856.4375 Validation Accuracy: 0.512600\n",
      "Epoch 94, CIFAR-10 Batch 5:  14:37:42.929745 Loss: 191160.3125 Validation Accuracy: 0.509000\n",
      "Epoch 95, CIFAR-10 Batch 1:  14:37:45.946842 Loss: 201519.6250 Validation Accuracy: 0.502800\n",
      "Epoch 95, CIFAR-10 Batch 2:  14:37:48.963400 Loss: 168450.5625 Validation Accuracy: 0.511400\n",
      "Epoch 95, CIFAR-10 Batch 3:  14:37:51.979337 Loss: 207496.1094 Validation Accuracy: 0.505600\n",
      "Epoch 95, CIFAR-10 Batch 4:  14:37:55.003278 Loss: 193851.8906 Validation Accuracy: 0.493200\n",
      "Epoch 95, CIFAR-10 Batch 5:  14:37:58.000662 Loss: 159686.4688 Validation Accuracy: 0.519000\n",
      "Epoch 96, CIFAR-10 Batch 1:  14:38:01.024710 Loss: 215835.2500 Validation Accuracy: 0.508800\n",
      "Epoch 96, CIFAR-10 Batch 2:  14:38:04.045265 Loss: 206982.4531 Validation Accuracy: 0.498600\n",
      "Epoch 96, CIFAR-10 Batch 3:  14:38:07.069210 Loss: 163162.7812 Validation Accuracy: 0.530000\n",
      "Epoch 96, CIFAR-10 Batch 4:  14:38:10.072288 Loss: 141143.2656 Validation Accuracy: 0.517400\n",
      "Epoch 96, CIFAR-10 Batch 5:  14:38:13.077974 Loss: 143514.9375 Validation Accuracy: 0.523800\n",
      "Epoch 97, CIFAR-10 Batch 1:  14:38:16.108781 Loss: 171445.1250 Validation Accuracy: 0.517800\n",
      "Epoch 97, CIFAR-10 Batch 2:  14:38:19.134882 Loss: 192780.2500 Validation Accuracy: 0.499600\n",
      "Epoch 97, CIFAR-10 Batch 3:  14:38:22.159839 Loss: 153439.3594 Validation Accuracy: 0.533200\n",
      "Epoch 97, CIFAR-10 Batch 4:  14:38:25.191725 Loss: 130682.1797 Validation Accuracy: 0.525400\n",
      "Epoch 97, CIFAR-10 Batch 5:  14:38:28.209232 Loss: 144525.1250 Validation Accuracy: 0.518200\n",
      "Epoch 98, CIFAR-10 Batch 1:  14:38:31.236407 Loss: 163359.6250 Validation Accuracy: 0.518800\n",
      "Epoch 98, CIFAR-10 Batch 2:  14:38:34.260234 Loss: 186526.1250 Validation Accuracy: 0.505200\n",
      "Epoch 98, CIFAR-10 Batch 3:  14:38:37.286974 Loss: 161370.6094 Validation Accuracy: 0.531200\n",
      "Epoch 98, CIFAR-10 Batch 4:  14:38:40.313251 Loss: 126805.7891 Validation Accuracy: 0.532600\n",
      "Epoch 98, CIFAR-10 Batch 5:  14:38:43.338584 Loss: 145440.2656 Validation Accuracy: 0.513400\n",
      "Epoch 99, CIFAR-10 Batch 1:  14:38:46.362924 Loss: 157876.7188 Validation Accuracy: 0.521000\n",
      "Epoch 99, CIFAR-10 Batch 2:  14:38:49.390510 Loss: 174601.9375 Validation Accuracy: 0.509000\n",
      "Epoch 99, CIFAR-10 Batch 3:  14:38:52.414202 Loss: 168837.7812 Validation Accuracy: 0.521200\n",
      "Epoch 99, CIFAR-10 Batch 4:  14:38:55.446344 Loss: 142409.2344 Validation Accuracy: 0.530600\n",
      "Epoch 99, CIFAR-10 Batch 5:  14:38:58.465120 Loss: 140166.3125 Validation Accuracy: 0.509200\n",
      "Epoch 100, CIFAR-10 Batch 1:  14:39:01.491180 Loss: 170471.6562 Validation Accuracy: 0.515400\n",
      "Epoch 100, CIFAR-10 Batch 2:  14:39:04.506523 Loss: 184786.8281 Validation Accuracy: 0.492000\n",
      "Epoch 100, CIFAR-10 Batch 3:  14:39:07.521806 Loss: 193005.7500 Validation Accuracy: 0.500800\n",
      "Epoch 100, CIFAR-10 Batch 4:  14:39:10.538264 Loss: 153375.0938 Validation Accuracy: 0.512400\n",
      "Epoch 100, CIFAR-10 Batch 5:  14:39:13.563526 Loss: 114632.5000 Validation Accuracy: 0.538800\n",
      "Epoch 101, CIFAR-10 Batch 1:  14:39:16.588601 Loss: 136548.0000 Validation Accuracy: 0.529400\n",
      "Epoch 101, CIFAR-10 Batch 2:  14:39:19.613276 Loss: 179290.0625 Validation Accuracy: 0.485600\n",
      "Epoch 101, CIFAR-10 Batch 3:  14:39:22.631519 Loss: 195778.8750 Validation Accuracy: 0.501600\n",
      "Epoch 101, CIFAR-10 Batch 4:  14:39:25.657961 Loss: 168371.8750 Validation Accuracy: 0.500200\n",
      "Epoch 101, CIFAR-10 Batch 5:  14:39:28.664116 Loss: 118518.3906 Validation Accuracy: 0.537800\n",
      "Epoch 102, CIFAR-10 Batch 1:  14:39:31.699939 Loss: 151088.9531 Validation Accuracy: 0.518200\n",
      "Epoch 102, CIFAR-10 Batch 2:  14:39:34.728457 Loss: 152791.5938 Validation Accuracy: 0.500200\n",
      "Epoch 102, CIFAR-10 Batch 3:  14:39:37.755828 Loss: 182937.5000 Validation Accuracy: 0.507200\n",
      "Epoch 102, CIFAR-10 Batch 4:  14:39:40.785473 Loss: 146201.4219 Validation Accuracy: 0.510600\n",
      "Epoch 102, CIFAR-10 Batch 5:  14:39:43.812662 Loss: 143858.0156 Validation Accuracy: 0.520200\n",
      "Epoch 103, CIFAR-10 Batch 1:  14:39:46.839521 Loss: 137484.9375 Validation Accuracy: 0.527000\n",
      "Epoch 103, CIFAR-10 Batch 2:  14:39:49.862624 Loss: 180088.6094 Validation Accuracy: 0.485000\n",
      "Epoch 103, CIFAR-10 Batch 3:  14:39:52.887755 Loss: 170173.7344 Validation Accuracy: 0.517000\n",
      "Epoch 103, CIFAR-10 Batch 4:  14:39:55.912947 Loss: 131402.4375 Validation Accuracy: 0.514200\n",
      "Epoch 103, CIFAR-10 Batch 5:  14:39:58.926070 Loss: 159024.9375 Validation Accuracy: 0.503000\n",
      "Epoch 104, CIFAR-10 Batch 1:  14:40:01.942907 Loss: 163340.1250 Validation Accuracy: 0.519400\n",
      "Epoch 104, CIFAR-10 Batch 2:  14:40:04.953071 Loss: 152780.5156 Validation Accuracy: 0.502000\n",
      "Epoch 104, CIFAR-10 Batch 3:  14:40:07.959333 Loss: 135729.3281 Validation Accuracy: 0.537400\n",
      "Epoch 104, CIFAR-10 Batch 4:  14:40:10.987708 Loss: 118064.1250 Validation Accuracy: 0.521800\n",
      "Epoch 104, CIFAR-10 Batch 5:  14:40:14.015528 Loss: 140347.2188 Validation Accuracy: 0.506000\n",
      "Epoch 105, CIFAR-10 Batch 1:  14:40:17.024379 Loss: 152789.9844 Validation Accuracy: 0.515000\n",
      "Epoch 105, CIFAR-10 Batch 2:  14:40:20.040463 Loss: 157803.2969 Validation Accuracy: 0.491600\n",
      "Epoch 105, CIFAR-10 Batch 3:  14:40:23.071278 Loss: 156045.6719 Validation Accuracy: 0.509800\n",
      "Epoch 105, CIFAR-10 Batch 4:  14:40:26.093782 Loss: 108628.6094 Validation Accuracy: 0.539400\n",
      "Epoch 105, CIFAR-10 Batch 5:  14:40:29.119246 Loss: 144625.5781 Validation Accuracy: 0.500600\n",
      "Epoch 106, CIFAR-10 Batch 1:  14:40:32.149842 Loss: 139180.6562 Validation Accuracy: 0.524600\n",
      "Epoch 106, CIFAR-10 Batch 2:  14:40:35.169164 Loss: 150190.4844 Validation Accuracy: 0.495200\n",
      "Epoch 106, CIFAR-10 Batch 3:  14:40:38.189795 Loss: 181286.3281 Validation Accuracy: 0.493000\n",
      "Epoch 106, CIFAR-10 Batch 4:  14:40:41.219448 Loss: 111657.1953 Validation Accuracy: 0.532200\n",
      "Epoch 106, CIFAR-10 Batch 5:  14:40:44.246420 Loss: 128488.6797 Validation Accuracy: 0.505600\n",
      "Epoch 107, CIFAR-10 Batch 1:  14:40:47.277500 Loss: 139813.7656 Validation Accuracy: 0.522400\n",
      "Epoch 107, CIFAR-10 Batch 2:  14:40:50.304546 Loss: 132938.1094 Validation Accuracy: 0.492600\n",
      "Epoch 107, CIFAR-10 Batch 3:  14:40:53.327899 Loss: 147431.5938 Validation Accuracy: 0.511000\n",
      "Epoch 107, CIFAR-10 Batch 4:  14:40:56.357451 Loss: 112048.0391 Validation Accuracy: 0.524400\n",
      "Epoch 107, CIFAR-10 Batch 5:  14:40:59.379818 Loss: 123739.9531 Validation Accuracy: 0.507000\n",
      "Epoch 108, CIFAR-10 Batch 1:  14:41:02.399712 Loss: 137897.7188 Validation Accuracy: 0.520600\n",
      "Epoch 108, CIFAR-10 Batch 2:  14:41:05.406443 Loss: 136286.3125 Validation Accuracy: 0.484400\n",
      "Epoch 108, CIFAR-10 Batch 3:  14:41:08.431672 Loss: 140225.0781 Validation Accuracy: 0.516600\n",
      "Epoch 108, CIFAR-10 Batch 4:  14:41:11.461730 Loss: 101166.8359 Validation Accuracy: 0.526400\n",
      "Epoch 108, CIFAR-10 Batch 5:  14:41:14.490033 Loss: 123174.5703 Validation Accuracy: 0.507600\n",
      "Epoch 109, CIFAR-10 Batch 1:  14:41:17.522182 Loss: 138367.7500 Validation Accuracy: 0.526200\n",
      "Epoch 109, CIFAR-10 Batch 2:  14:41:20.553855 Loss: 138812.8906 Validation Accuracy: 0.488200\n",
      "Epoch 109, CIFAR-10 Batch 3:  14:41:23.577736 Loss: 125854.7344 Validation Accuracy: 0.520000\n",
      "Epoch 109, CIFAR-10 Batch 4:  14:41:26.608476 Loss: 108177.6094 Validation Accuracy: 0.520800\n",
      "Epoch 109, CIFAR-10 Batch 5:  14:41:29.620123 Loss: 106263.1172 Validation Accuracy: 0.511200\n",
      "Epoch 110, CIFAR-10 Batch 1:  14:41:32.620314 Loss: 133884.6250 Validation Accuracy: 0.530600\n",
      "Epoch 110, CIFAR-10 Batch 2:  14:41:35.651227 Loss: 141361.5156 Validation Accuracy: 0.487400\n",
      "Epoch 110, CIFAR-10 Batch 3:  14:41:38.669515 Loss: 118113.1875 Validation Accuracy: 0.527400\n",
      "Epoch 110, CIFAR-10 Batch 4:  14:41:41.696218 Loss: 109269.8750 Validation Accuracy: 0.522400\n",
      "Epoch 110, CIFAR-10 Batch 5:  14:41:44.723033 Loss: 119172.0547 Validation Accuracy: 0.494400\n",
      "Epoch 111, CIFAR-10 Batch 1:  14:41:47.750825 Loss: 148355.2500 Validation Accuracy: 0.514800\n",
      "Epoch 111, CIFAR-10 Batch 2:  14:41:50.784725 Loss: 119993.5547 Validation Accuracy: 0.507800\n",
      "Epoch 111, CIFAR-10 Batch 3:  14:41:53.811207 Loss: 144891.4688 Validation Accuracy: 0.503800\n",
      "Epoch 111, CIFAR-10 Batch 4:  14:41:56.838148 Loss: 126324.4062 Validation Accuracy: 0.525800\n",
      "Epoch 111, CIFAR-10 Batch 5:  14:41:59.858006 Loss: 116943.5625 Validation Accuracy: 0.507000\n",
      "Epoch 112, CIFAR-10 Batch 1:  14:42:02.870115 Loss: 124076.2109 Validation Accuracy: 0.521200\n",
      "Epoch 112, CIFAR-10 Batch 2:  14:42:05.897467 Loss: 102403.5312 Validation Accuracy: 0.529400\n",
      "Epoch 112, CIFAR-10 Batch 3:  14:42:08.916233 Loss: 137744.3750 Validation Accuracy: 0.504000\n",
      "Epoch 112, CIFAR-10 Batch 4:  14:42:11.925536 Loss: 143498.6875 Validation Accuracy: 0.512600\n",
      "Epoch 112, CIFAR-10 Batch 5:  14:42:14.940973 Loss: 112631.7969 Validation Accuracy: 0.513600\n",
      "Epoch 113, CIFAR-10 Batch 1:  14:42:17.971444 Loss: 104852.2969 Validation Accuracy: 0.531200\n",
      "Epoch 113, CIFAR-10 Batch 2:  14:42:20.998845 Loss: 97378.8438 Validation Accuracy: 0.529400\n",
      "Epoch 113, CIFAR-10 Batch 3:  14:42:24.013980 Loss: 127042.5703 Validation Accuracy: 0.509200\n",
      "Epoch 113, CIFAR-10 Batch 4:  14:42:27.031519 Loss: 120701.4141 Validation Accuracy: 0.524800\n",
      "Epoch 113, CIFAR-10 Batch 5:  14:42:30.038419 Loss: 97910.7422 Validation Accuracy: 0.514800\n",
      "Epoch 114, CIFAR-10 Batch 1:  14:42:33.050589 Loss: 108667.3438 Validation Accuracy: 0.532000\n",
      "Epoch 114, CIFAR-10 Batch 2:  14:42:36.053292 Loss: 98090.7500 Validation Accuracy: 0.531000\n",
      "Epoch 114, CIFAR-10 Batch 3:  14:42:39.068275 Loss: 122769.3359 Validation Accuracy: 0.504000\n",
      "Epoch 114, CIFAR-10 Batch 4:  14:42:42.090722 Loss: 114966.7891 Validation Accuracy: 0.521800\n",
      "Epoch 114, CIFAR-10 Batch 5:  14:42:45.095018 Loss: 95743.0703 Validation Accuracy: 0.521800\n",
      "Epoch 115, CIFAR-10 Batch 1:  14:42:48.127749 Loss: 102374.2109 Validation Accuracy: 0.528600\n",
      "Epoch 115, CIFAR-10 Batch 2:  14:42:51.140219 Loss: 98793.5156 Validation Accuracy: 0.526400\n",
      "Epoch 115, CIFAR-10 Batch 3:  14:42:54.153799 Loss: 130302.7969 Validation Accuracy: 0.497000\n",
      "Epoch 115, CIFAR-10 Batch 4:  14:42:57.165701 Loss: 94894.1641 Validation Accuracy: 0.530800\n",
      "Epoch 115, CIFAR-10 Batch 5:  14:43:00.194697 Loss: 97552.9453 Validation Accuracy: 0.523000\n",
      "Epoch 116, CIFAR-10 Batch 1:  14:43:03.196992 Loss: 104105.8281 Validation Accuracy: 0.522200\n",
      "Epoch 116, CIFAR-10 Batch 2:  14:43:06.207811 Loss: 106000.1094 Validation Accuracy: 0.517400\n",
      "Epoch 116, CIFAR-10 Batch 3:  14:43:09.202053 Loss: 120656.1875 Validation Accuracy: 0.507600\n",
      "Epoch 116, CIFAR-10 Batch 4:  14:43:12.196981 Loss: 92093.0781 Validation Accuracy: 0.530800\n",
      "Epoch 116, CIFAR-10 Batch 5:  14:43:15.206157 Loss: 91151.8438 Validation Accuracy: 0.523200\n",
      "Epoch 117, CIFAR-10 Batch 1:  14:43:18.208356 Loss: 98210.9141 Validation Accuracy: 0.525400\n",
      "Epoch 117, CIFAR-10 Batch 2:  14:43:21.221260 Loss: 113858.5000 Validation Accuracy: 0.511200\n",
      "Epoch 117, CIFAR-10 Batch 3:  14:43:24.230180 Loss: 119673.1562 Validation Accuracy: 0.503800\n",
      "Epoch 117, CIFAR-10 Batch 4:  14:43:27.251716 Loss: 97344.8906 Validation Accuracy: 0.524800\n",
      "Epoch 117, CIFAR-10 Batch 5:  14:43:30.275563 Loss: 90992.3125 Validation Accuracy: 0.523400\n",
      "Epoch 118, CIFAR-10 Batch 1:  14:43:33.295062 Loss: 104655.7344 Validation Accuracy: 0.522400\n",
      "Epoch 118, CIFAR-10 Batch 2:  14:43:36.299802 Loss: 95138.8359 Validation Accuracy: 0.524600\n",
      "Epoch 118, CIFAR-10 Batch 3:  14:43:39.307086 Loss: 105858.1562 Validation Accuracy: 0.511600\n",
      "Epoch 118, CIFAR-10 Batch 4:  14:43:42.327120 Loss: 87179.8125 Validation Accuracy: 0.522600\n",
      "Epoch 118, CIFAR-10 Batch 5:  14:43:45.323280 Loss: 100279.8125 Validation Accuracy: 0.509400\n",
      "Epoch 119, CIFAR-10 Batch 1:  14:43:48.319674 Loss: 118434.2656 Validation Accuracy: 0.516800\n",
      "Epoch 119, CIFAR-10 Batch 2:  14:43:51.316746 Loss: 91241.1484 Validation Accuracy: 0.526400\n",
      "Epoch 119, CIFAR-10 Batch 3:  14:43:54.299997 Loss: 101941.6719 Validation Accuracy: 0.514800\n",
      "Epoch 119, CIFAR-10 Batch 4:  14:43:57.294654 Loss: 85538.3672 Validation Accuracy: 0.523000\n",
      "Epoch 119, CIFAR-10 Batch 5:  14:44:00.310480 Loss: 96856.4375 Validation Accuracy: 0.517200\n",
      "Epoch 120, CIFAR-10 Batch 1:  14:44:03.308917 Loss: 114706.5547 Validation Accuracy: 0.515200\n",
      "Epoch 120, CIFAR-10 Batch 2:  14:44:06.325844 Loss: 109522.1797 Validation Accuracy: 0.513800\n",
      "Epoch 120, CIFAR-10 Batch 3:  14:44:09.348574 Loss: 92837.5781 Validation Accuracy: 0.521600\n",
      "Epoch 120, CIFAR-10 Batch 4:  14:44:12.376336 Loss: 97631.4844 Validation Accuracy: 0.510800\n",
      "Epoch 120, CIFAR-10 Batch 5:  14:44:15.398913 Loss: 107200.1016 Validation Accuracy: 0.506200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5167570114135742\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAJ/CAYAAAB4GhsgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3XecZFWZ//HP07knZxjiEAyjGBEQURjEHDGBGXDXFczo\nuuKqK+quaVfxJ6Y1sqYFFcMaQdEBRBEl5zzEYYDJ0zk8vz+eU3Vv36nqrp6unp6u/r5fr3pV1T33\nnnuquqr6qVPPOcfcHRERERGRRtQ01Q0QEREREZksCnZFREREpGEp2BURERGRhqVgV0REREQaloJd\nEREREWlYCnZFREREpGEp2BURERGRhqVgV0REREQaloJdEREREWlYCnZFREREpGEp2BURERGRhqVg\nV0REREQaloJdEREREWlYCnZFREREpGEp2J1iZravmb3czE4xsw+Y2Wlm9g4ze5WZPcXM5kx1G6sx\nsyYze6mZnW1mt5nZFjPz3OVnU91GkV2Nma0ovE9Or8e+uyozW1V4DCdOdZtEZGZpmeoGzERmtgg4\nBXgzsO8Yuw+b2Q3AxcCvgAvcvXeSmzim9Bh+DBw91W2Rnc/MzgJOGGO3QWAT8DBwBfEa/l933zy5\nrRMREcmoZ3cnM7MXATcA/87YgS7E3+ggIjj+JfDKyWvduHyHcQS66t2ZkVqAJcCjgdcCXwHuM7PT\nzUxftKeRwnv3rKluj4jIeOgfzk5kZscBPwCaC0VbgGuBB4A+YCGwD7CSXfALiZk9FXhhbtNdwEeB\nvwNbc9u7d2a7ZFqYDXwEONLMnu/ufVPdIBERaWwKdncSMzuA6A3NB7rXAR8Efu3ugxWOmQMcBbwK\neBkwbyc0tRYvL9x/qbtfPSUtkV3F+4i0lrwWYDfg6cBbiS9wJUcTPb1v2imtExGRGUvB7s7zH0B7\n7v7vgZe4e0+1A9x9G5Gn+yszewfwj0Tv71Q7OHd7jQJdAR529zUVtt8GXGJmXwC+T3xpKznRzL7g\n7lftjAZOR+k5talux0S4+2qm+WMQkeltl/uJvBGZWSfwktymAeCE0QLdInff6u5nuPvv697A8VuW\nu33/lLVCpo30Wn8dcEtuswEnT02LRERkplCwu3M8GejM3f+zu0/nIDE/HdrAlLVCppUU8J5R2HzM\nVLRFRERmDqUx7By7F+7ftzNPbmbzgGcAewKLiUFk64C/uvvdO1JlHZtXF2a2P5FesRfQBqwB/uju\nD45x3F5ETunexONam467dwJt2RN4LLA/sCBt3gDcDfxlhk+9dUHh/gFm1uzuQ+OpxMwOAh4DLCcG\nva1x9x/UcFw78DRiJpRlwBDxXrjG3a8ZTxuq1P8I4FBgD6AXuBe4zN136nu+QrseCTwRWEq8JruJ\n1/p1wA3uPjyFzRuTme0NPJXIAZ9LvJ/uBy529011Ptf+RAfF3sQYi3XAJe5+xwTqfBTx/O9OdBYM\nAtuAe4BbgZvc3SfYdBGpxt11meQL8GrAc5ff7KTzPgX4DdBfOH/+cg0xLZSNUs+qUY6vdlmdjl2z\no8cW2nBWfp/c9qOAPwLDFerpB74MzKlQ32OAX1c5bhg4F9izxue5KbXjK8DtYzy2ISJf++ga6/6f\nwvFfG8ff/5OFY3852t95nK+tswp1n1jjcZ0VnpNlFfbLv25W57afRARoxTo2jXHeg4AfAV2j/G3u\nAd4NtO7A83EE8Ncq9Q4SufcHp31XFMpPH6XemvetcOwC4GPEl6zRXpMPAd8CDhnjb1zTpYbPj5pe\nK+nY44CrRjnfAPA74KnjqHN17vg1ue2HEV/GKn0mOHApcPg4ztMKvJfIWx/redtEfOY8ux7vT110\n0WXkZcobMBMuwDMLH2xbgQWTeD4DPjPKh3aly2pgYZX6iv+saqovHbtmR48ttGHEP9607Z01Psa/\nkQt4idkkums4bg2wTw3P95t24DE68FmgeYy6ZwM3Fo57dQ1tenbhubkXWFzH19hZhTadWONxHRWe\nh6UV9su/blYTgzt/OMpzWTHYJb6I/CfxJaPWv8vV1PhFJ53jX2t8HfYTecsrCttPH6XumvctHPcy\nYOM4X49XjfE3rulSw+fHmK8VYuaZ34/z3J8Hmmqoe3XumDVp2zsYvVMg/zc8roZzLCUWUhnv8/ez\ner1HddFFl+yiNIad43Lin21p2rE5wHfM7LUeMy7U29eBfyhs6yd6Ju4nenyeQkz4X3IUcJGZHenu\nGyehTXWV5iz+f+muE70/txOB/hOBA3K7PwU4EzjJzI4GziFL4bkpXfqJeY0flztuX6JndazFM4q5\n7z3A9cTPxFuI3sx9gMcTKRYl7yF6pk6rVrG7d5nZ8USvYUfa/DUz+7u731bpGDPbHfguWbrJEPBa\nd18/xuPYGfYq3HciKBvL54kp+ErHXEkWEO8P7Fc8wMyaib/1KwpF3cR7ci3xnjwAeALZ8/V44M9m\ndqi7rxutUWb2bmKmlbwh4u91D/GT+5OIdItWIoAsvjfrKrXpc2yfbvQA8UvOw8As4m/xOEbOEjPl\nzGwucCHxPs7bCFyWrpcTaQ35tr+L+Ex7/TjP9zrgC7lN1xG9sX3Ea+NgsueyFTjLzK5091ur1GfA\nT4i/e946Yj71h4kvR/NT/QeilEKRyTXV0fZMuRA/IRe/xd9PTLD/OOr38/IJhXMME4HCgsJ+LcQ/\n3c2F/f+3Qp0dRA9T6XJvbv9LC2Wly+7p2L3S/WIqxz9XOa58bKENZxWOL/Va/Qo4oML+xxFBZ/55\nODw95w78GXhiheNWAesL53rBGM95aUq4T6ZzVOxdIr5kvJ+RP6UPA4fV8Hc9udCmvwNtFfZrIn7W\nze/74Ul4PRf/HifWeNw/FY67rcp+a3L7bM3d/i6wV4X9V1TY9h+Fc60j0iAqPW8HsP179NdjPJbH\nsX1v4A+Kr9/0NzkOeDDts6FwzOmjnGNFrfum/Z/L9r3YFxJ5ytt9xhDB4ouJn9AvL5QtIXtP5uv7\nMdXfu5X+DqvG81oBvl3YfwvwFgrpJUSw+Fm271V/yxj1r87tu43sc+KnwIEV9l9J9Pbnz3HOKPW/\nsLDvrcRAzIqf8cSvNy8FzgZ+VO/3qi666OIKdnfaEx09R72FD8H8ZT0RuH2Y+Al69g6cYw7b/3R5\n6hjHHMb2eYyj5o1RJZ9yjGPG9Q+vwvFnVXjOvs8oP1sSSyxXCpB/D7SPctyLav3HlvbffbT6Kux/\neOG1MGr9uePOKbTr/1XY54OFff4w2nM0gddz8e8x5t+T+NJUTMmomINM5fSXT42jfYcxMui7mQpf\nogrHNLF9jvTzR9n/j4V9vzRG/Y9l+0C3bsEu0Vu7rrD/F2v9+wO7jVKWr/Oscb5Wan7vE4NF8/t2\nA0eMUf/bC8dso0pKVtp/dYW/wRcZfdzCboz8bO2rdg4id7+03wCw3zieq47xPLe66KJLbRdNPbaT\neCy88AYiyKlkEfACYkDJ+cBGM7vYzN6SZlOoxQlko/8Bfuvuxameiu36K/Bvhc3vqvF8U+l+ogdn\ntFHk3yR6rktKo9Df4KMsU+vuvySCo5JVozXE3R8Yrb4K+/8F+FJu07FploCxvJlI1Sh5p5m9tHTH\nzJ5OLNtc8hDwujGeo53CzDqIXtlHF4r+u8YqriIC+VqdRpZeMggc6+6jLsiSnqe3MHK2lHdX2tfM\nHsPI18UtwKlj1H898C+jtnpi3szIObD/CLyj1r+/j5GysZMUP3s+6u6XjHaAu3+R6JUvmc34UkWu\nIzoFfJRzrCOC2JI2Io2ikvxKgVe5+521NsTdq/1/EJEJULC7E7n7j4ifE/9Uw+6tRC/HV4E7zOyt\nKRdsNK8r3P9IjU37AhEYlbzAzBbVeOxU+ZqPke/s7v1A8R/l2e6+tob6/5C7vSzlwdbTz3O329g+\nP3E77r6FSAfpz23+tpntk/5e/0uWF+7AG2t8rPWwxMxWFC4HmtnTzOxfgBuAVxaO+b67X15j/Wd4\njdOTpanf8ou4/MDdb6zl2BRsfC236Wgzm1Vh12Je6GfS620s3yLSgCbDmwv3Rw3gdjVmNhs4Nrdp\nI5GCVYsPFe6PJ2/3DHevZb7wXxfuP6GGY5aOox0iMkkU7O5k7n6luz8DOJLoeRx1HthkMdETeLaZ\ntVXaIfUMPjm36Q53v6zGNg0Q0zKVq6N6r8Wu4vwa97u9cP93NR5XHPw17n9aFuaa2R7FQJDtBw8V\nezwrcve/E3m/JQuJIPd/GDn46z/d/bfjbfME/CdwZ+FyK/Fl49NsP4DsErYPzkbzy7F3KVvFyM+2\nc8dxLMBFudutwCEV9jk8d7s0Vd2YUi/rj8fZnjGZ2VIiTaLkbz79lvE+hJEDtX5a6y8m6bHekNv0\nuDTQrRa1vk9uKtyv9pmQ/1VoXzN7W431i8gk0QjQKeLuFwMXQ/kn0acRswYcQvTyVfoichwxkrfS\nh+dBjByZ/NdxNulS4K25+wezfU/GrqT4j6eaLYX7N1fca+zjxkwlSaP/n0XMGnAIEcBW/HJSwcIa\n98PdP29mq4hBLRCvnbxLGd9P/jtTDzGLxr/V2JsGcLe7bxjHOY4o3N+YvmDUqrlwf39ikFde/ovl\nrT6+hQ3+No59a3VY4f7Fk3COyXZw4f6OfIY9Jt1uIj5Hx3oetnjtq1kWF4Op9plwNiNTWr5oZscS\nA+9+49NgthuRRqNgdxfg7jcQvRLfADCzBcTPeacS0yDlvdXMvlXh599iL0PFaXFGUQwCd/Wf32pd\nhWywTse1jrazmR1O5J8+brT9RlFrXnbJSUQe6z6F7ZuA17h7sf1TYYh4vtcTU4VdTKQUjCdwhZEp\nNrUoTm92UcW9ajcipSf9ipL/exV/PRhLxSnjJqiYZlNT2sYuZio+w2pezdDdBwqZZBU/E9z9MjP7\nMiM7D56VLsNmdi2RynYRMcC3ll/3RGQClMawC3L3Te5+FtEz8bEKu7yjwrYFhfvFnsmxFD/0a+5p\nnAoTGHRV98FaZvY8YjDQjga6MM73Yuod+kSFove6+5oJtGNHneTuVri0uPtid3+kux/v7l/cgUAX\nYnT9eNQ733xO4X7xvTHR91o9LC7cr+sSujvJVHyGTdbgzbcTv650F7Y3Ebm+byNmV1lrZn80s1fW\nMCZDRHaQgt1dmIePEB+Kec+q5fBxnk4ftDsgDQz7HiNTSNYAHweeDzyK+CfekQ8EqbAIwjjPu5iY\npq7o9WY209/Xo/bC74Cx3hu74ntt2gxMG8Wu+LzWJH12f4JIgXk/8Be2/7UI4n/wKmLMxIVmtnyn\nNVJkBlEaw/RwJnB87v6eZtbp7j25bcWenPnjPEfxZ3TlldXmrYzsVTsbOKGGkfm1Dp7ZTuoB+h9g\nzwrFRxMj0yv9IjBT5HuPB4HOOqd1FN8bE32v1UOxx7zYSzodNNxnWJqy7DPAZ8xsDnAo8AzifXoE\nI/8HPwP4bVq5r+apDEVkbDO9B2i6qDSquvgTXTGv8cBxnuORY9Qnlb0wd3sz8I81TkE1kanMTi2c\n9zJGzurxb2b2jAnUP93l54ttYYK96EUpEMn/xH5AtX2rGO97sxbFOYRXTsI5JltDf4a5+zZ3/4O7\nf9TdVxFLHn+IGLRZ8njgTVPRPpFGpmB3eqiUV1bMZ7uOkfOvFkdnj6U41Vit85/WqhF+Vq0k/w/5\nT+7eVeNxOzS1m5k9BfhUbtNGYvaHN5I9x83AD1Kqw0x0aeH+MZNwjitytx+RBpXWqtJUZhN1KSPf\nY9Pxy07xM2cin2HDxADOXZa7P+zu/8H2U/C9eCraI9LIFOxOD48q3N9WXFAh9Tbl/1kcYGbFqXwq\nMrMWImAqV8f4p/0ZS/FnuVqn5NrV5X9qrWlATUpDeM14T5RW0juHkTmpb3L3u939PGKu25K9iKmO\nZqLfF+6fOAnn+EvudhPwiloOSvnUrxpzx3Fy94eA63ObDjWziQyYLMq/fyfrvfs3Rua1vqzavOJF\n6bHm5xm+zt231rNxk+gcRq6suWKK2iHSsBTs7gRmtpuZ7TaBKoo/a62ust8PCveLywBX83ZGLjP6\nG3dfX+OxtSqOlK73imRTJZ9nWPwZtZo3sGM/M3+NGPBScqa7/yx3/4OM7NV8sZlNh6Wf68rdbwMu\nyG06zMyKqwtO1PcL9//FzGoZGPcmKuda18PXCvc/V8cR/vn376S8d9OvIvmVBRdReU7xSj5euP+9\nujRqJ0j55PlZG2pJgxKRcVCwu3OsJJb8/ZSZLRtz7xwzewVwSmFzcXaGkv9h5D+ll5jZW6vsW6r/\nELb/R/GF8bSxRncA+UUEnjkJ55gK1+ZuH2xmR422s5kdSgw4HBcz+ydGDlK8Enhffp/0T/M1jAzA\nP2Nm+QUQZorTC/e/bmbPHk8FZrbczF5Qqczdr2fkQhOPBM4Yo77HEIOVJss3GZmv/Czg87UGvGN8\nIc/PYXtIGmw1GYqfPR9Pn1FVmdkpZAusAHQRz8WUMLNT0op2te7/fEZOl1frwjciUiMFuzvPLGIK\nmnvN7Kdm9orRPhDNbKWZfQ34ISNXdLqC7XtwAUg/272nsPlMM/tPMxsxstnMWszsJGL53Pw/rh+m\nn8TrKqVZ5JcvPsrMvmFmx5jZIwrL6U6nXt/i0q/nmtlLijuZWaeZnUr0OM4jVsKriZkdBHw+t2kb\ncHylEdtpjt18DmAbcM44lk5tCO7+J0bOQ9xJjHT/spk9otpxZrbAzI4zs3OIKeTeOMpp3sHIL3Bv\nM7PvF1+/ZtZkZq8ifpFZyCTNgevu3UR78zn+7wQuSIuebMfM2s3sRWb2Y0ZfMTG/MMcc4Fdm9rL0\nOVVcCnsij+Ei4Lu5TbOB35nZPxR7zs1snpl9BvhioZr37eB8zvXyfuDu9Fo4ttp7L30Gv5FY7jtv\n2vRKi0wXmnps52slVkc7FsDMbgPuJoKfYeKf4WOAvSscey/wqtEWVHD3b5nZkcAJaVMT8M/AO8zs\nL8BaYlqiQ4AlhcNvZPte5Ho6k5FLuf5DuhRdSMw9OR18i5gdoRRALQZ+bmZ3EV9MeomffQ8jvvBA\njL4+hZhbc1RmNovoye/MbT7Z3auuLuXuPzazrwInp00HAl8BXl/jY2oUHyZWmCs97ibieT8l/X1u\nIAb4tRLviUcwjnxJd7/WzN4PfC63+bXA8WZ2KXAPERgeTIy8h8hJPZVJyqd29/PN7J+Bz5LNO3s0\n8GczWwtcQ6xo10nkdT+ebI7oSrO+lHwDeC/Qke4fmS6VTDR14u3Ewgul1SPnp/N/2swuI74s7A4c\nnmtPydnu/pUJnr8eOojXwmsBN7NbgDvJpkNbDjyJ7adX+5m7/2KntVJkhlCwu3NsIILZYnAJEYjU\nMsXO74E317g61knpnO8m+8fTzugB5J+Al05mj4i7n2NmhzFy3fhpzd37Uk/uH8gCGoB906VoGzFA\n6aYaT3Em8eWn5NvuXswXreRU4otFaZDS68zsAnefMYPW0pfCN5jZ1cC/M3Lhj2p/n6JR52p19zPS\nF5KPk73Xmhn5pa5kkPhyN9Hli0eV2nQfESDmexWXM/I1Op4615jZiUSQ3jnG7hPi7ltSOtBPiEC9\nZDGxUEs1XyJ6snc1RgwyLg40LjqHrJNCROpIaQw7gbtfQ/REPJPoBfo7MFTDob3EB/6L3f3ZtS4D\nm1bveQ8xFc/5VF65p+R64gP2yJ3x019q12HEP6a/Eb1M03pAhrvfBDyZ+Pmx2nO9DfgO8Hh3/20t\n9ZrZaxg5OPEmKi8VXalNvUSOb37gy5lm9uhajm8k7v5fxMC+z7P9fLSV3Ex8yTjc3cf8pSNNH3Uk\nI9N08oaJ9+ER7v6dmho9Qe7+Q2J+4f9iZB5vJeuIwW2jBlrufg4x/uCjRErGWkbOEVs37r6JmDLu\ntURvdDVDRGrQEe7+9gksI15PLyWeo0sZ+7NtmGj/C9391VpMQmRymHujTn+6a0u9QY9Ml2VkPTBb\niF7Z64Eb6rHyU8rXPZIYBb6ICLzWAX+tNYCW2qS5bY8kfg7vIJ7n+4CLU06lTLE0UOzxxC8tC4gv\nlZuA24Hr3f3BUQ4fq+5HEF8yl6d67wMuc/d7JtruCbTJiLSAxwJLidSKbalt1wM3+i7+j8DM9iGe\n192Iz8oNwP3E+2rKV0qrxsw6gIOIX+92J577AWIg8W3AFVOcXywyIyjYFREREZGGpTQGEREREWlY\nCnZFREREpGEp2BURERGRhqVgV0REREQaloJdEREREWlYCnZFREREpGEp2BURERGRhqVgV0REREQa\nloJdEREREWlYCnZFREREpGEp2BURERGRhqVgV0REREQaloJdEREREWlYCnZFREREpGEp2BURERGR\nhqVgV0REREQaloJdEREREWlYCnZFREREpGEp2BURERGRhqVgV0REREQaloJdEREREWlYCnZFRERE\npGEp2BURERGRhqVgdxoysxVm5mbmU90WERERkV1Zy1Q3YCqZ2YnACuBn7n7V1LZGREREROptRge7\nwInAUcAaQMGuiIiISINRGoOIiIiINCwFuyIiIiLSsGZksGtmJ6bBXUelTd8uDfhKlzX5/cxsdbr/\nOjO70MzWp+3Hpu1npfunj3LO1WmfE6uUt5rZP5nZBWb2kJn1mdldZnZ+2j57HI/vCWa2Lp3ve2Y2\n09NVREREZIaaqUFQD7AOWAS0AlvStpKHigeY2ReAdwDDwOZ0XRdmtifwS+CJadNwatPewD7As4Fb\ngNU11PU04FfAAuArwNvcXbM2iIiIyIw0I3t23f0cd98d+HPa9C533z13OaRwyMHA24GPAIvdfRGw\nMHf8DjOzduD/iED3YeAEYJ67LwRmA4cAn2dkMF6trucAvyMC3U+7+1sV6IqIiMhMNlN7dsdrDvBJ\nd/9YaYO7byF6XyfqH4AnA33AMe5+Te4cPcDf02VUZvZy4H+BNuBf3f2TdWibiIiIyLSmYLc2Q8Dn\nJqnuN6brb+cD3fEws5OArxM99W9z9y/Xq3EiIiIi09mMTGPYAbe5+8P1rtTMWokUCYBf72Ad7wK+\nCTjwRgW6IiIiIhn17NZmuwFrdbKI7G9w9w7W8fl0/TF3/97EmyQiIiLSONSzW5uhSarX6lDH2en6\nn83s0DrUJyIiItIwFOzWx2C67hhln/kVtq3PHbvvDp77DcC5wDzgPDN78g7WIyIiItJwZnqwW5or\nd6I9rJvS9V6VCtOCECuL2919ALg83X3BjpzY3QeB1wC/IKYcO9/MHr8jdYmIiIg0mpke7JamDlsw\nwXquTdfPMbNKvbunAu1Vjv1Ouj5xR4PUFDS/EvgNsBj4nZltF1yLiIiIzDQzPdi9Pl2/3MwqpRnU\n6hfEog9Lge+Y2TIAM5tvZh8ETidWXavkm8BVRDB8gZm9wcxmpeM7zexQM/u6mR02WgPcvR94OXAB\nsCzV9YgJPCYRERGRaW+mB7vfBfqBpwMPm9l9ZrbGzP40nkrcfQNwWrr7KmCdmW0ENgD/DnyMCGgr\nHdsHvAS4DlhC9PRuMbMNQBfwV+Afgc4a2tGb6roQWA78wcz2H89jEREREWkkMzrYdfebgGcDvyV6\nXncnBopVzL0do64vAMcDlwLdxHN7CfCy/MprVY69B3gK8E7gT8BWYBYxHdl5wJuBy2psRzfwonTu\nvYiAd5/xPh4RERGRRmDuPtVtEBERERGZFDO6Z1dEREREGpuCXRERERFpWAp2RURERKRhKdgVERER\nkYalYFdEREREGpaCXRERERFpWAp2RURERKRhKdgVERERkYalYFdEREREGlbLVDdARKQRmdmdwDxg\nzRQ3RURkuloBbHH3/SZSScMGu1fevtEBmrHytpam5nQdHdpNTVlZU3PcbmY47leo0yz2seZsW1Oq\nq1Q2MDhcLuvrHwRgaGhou7ramuO45qbsTOW60nXPwEC5rLysc7ravHlDuWzunHlRV2tH1NOSf8yl\n23E9lGtf/0C071H7zs0OEJF6mdfZ2blo5cqVi6a6ISIi09GNN95IT0/PhOtp2GC3pRS0WhbHNafb\npfgvH+xaKYpMPHffSgFz6fhcAN2U9vPhuDYf2q5sOG1rygW2zaXgujkfVsf+Q0MpSB7M6irVXwp6\nB/qzQHigrz/t0xrn680C2mEfHnGce1Y2NFy6PReRmcTMVgB3Av/j7idO0mnWrFy5ctHll18+SdWL\niDS2gw8+mCuuuGLNROtRzq6ITAozW2FmbmZnTXVbRERk5mrYnl0Rkal23X2bWXHar6a6GSIiU2LN\np1441U0AGjjYbW+LxNomz6UxpDSC5uYoM/PtjitlPeR+7S9vyzIi8sf5iOt8V3lrKVXBIr3A8jnC\n6XbTiL712FZKOWjN5wanO6WzmQ9m50k5uq2t8ecczOUIDw2nXN2UGpFPz/Dh7XOJRURERBqJ0hhE\npO7M7HQiJxbghJTOULqcaGar0u3TzexQM/uVmW1I21akOtzMVlep/6z8voWyQ83sHDO7z8z6zGyt\nmZ1vZsfV0O4mM/tCqvsnZtaxY8+AiIjsKhq2Z7ejNfWmDue6aMsDy9I2zw1CS728pcFoI3pASxMh\nlOrK9wh704idRpwu1dXSvP3sD6U2DPb3lbe0tLXFjaaoq6UpO09zuZs3zQiRHwjXkk6adhnO9WaX\n2j6QenZ7ujeXy7ZuSbcfuxyROlsNLADeBVwN/CxXdlUqAzgc+ADwJ+BbwBKgf0dPamZvBr4CDAH/\nB9wKLAOeArwV+OEox3YA3wNeAXwJeKfnR3SKiMi01LDBrohMHXdfbWZriGD3Knc/PV9uZqvSzecA\nJ7v7f0/0nGb2GODLwBbgGe5+faF8r1GOXQT8HDgCOM3dPz2O81abbuHRtdYhIiKTp2GD3ZaUYOtN\n208h66lXNTcrWTbHbem+5XtHR/beNttg7rjCtGS5fqDhNF1YS+pxbckljXT1RY9ub24KsXnt7SPa\nMEB2nuFtvQD0d3XH/c1bymWDqUO42WIuusH+rBFd3V0ArO+J/fv7svnq+oa3z1kW2cmuqkegm5xC\nfKZ9vBjoArj7vZUOMrN9gd8CBwBvcPfv16k9IiKyC2jYYFdEpoXL6ljXU9P1b8ZxzKOAvwCzgee7\n+wXjPak+l5D/AAAgAElEQVS7H1xpe+rxffJ46xMRkfrSADURmUoP1LGuUh7wfeM45pHAcuAO4Io6\ntkVERHYRDduzW1qid+RKaKUbpTSGLNa38pxjPvIasnnIyvN+ZXOClaYOs/KgsuwwS2PImtIANc+n\nRgzG/oP51djSVGVD6x4GYP3f/lYu677jdgA2rrsn6t7/EeWyjlmxGmn/po0A9HiWGtGXVmFrSlOx\nNXdkq6X5kt0QmWKj5dI41T+jFlTYtild7wncVOP5fwHcDHwCuMDMnuPuD9d4rIiITAMNG+yKyJQr\nTRnSPOpe1W0E9i5uNLNm4IkV9r+UmHXh+dQe7OLunzSzHuAM4I9m9ix3X7djTR7poD3nc/kuMqm6\niMhM1bDB7mCxNzZ3x9K1e356sdi/pam0EETW4zpcmlYs7TM0uP2gt+bWNHgt17M7XOosTl28zc3Z\n092ZBpUN5Qaa9V97BwB3/fgXAGy8Mhvk3bxkNgAbtmwDYO7c+eWyno3R27u+ZysAt+ZmbtptKKZg\nW5p6e9tyq1jc1Bl18oaXbfd4ROpgI/Gm22cHj78MeF7qbT0/t/1DwL4V9v8KcDLwYTM7z91vyBea\n2V7VBqm5++fNrJeYzeFCM3umu9+/g+0WEZFdSMMGuyIytdx9m5n9FXiGmX0fuIVs/tta/BfwXODn\nZnYOsAF4GrAfMY/vqsL5bjCztwJfBa40s58T8+wuJnp8twJHj9Ler6aA95vARSngvbvGtoqIyC5K\nA9REZDK9AfgV8DzgI8DHqXGGgjQzwrHA9cCrgROANcChwF1Vjvk68HTgl0Qw/D7gJcDDxEIRY53z\nLOD1RM/xRWa2fy1tFRGRXVfD9uz2D5R+ys9SDprLKQoR4w/n5pkdHk4DuZoivbAlN9Ksta20fxpA\nNpRLf0hz9g4PlVIdsjaU5uptH4wUgoGHsnEv6667GoCHb7qtvK1z3YY4322xyuqi2bPLZTYnBpZ1\nD0X+Q79n7etrjTbcuS3m7r0ktyrb3OG4/azm2GfpYJbisGGjxuHI5HL324AXVynePh9o++P/j8o9\nwSemS6Vj/kKsgjZavWuqnd/d/xf437HaJiIi04N6dkVERESkYTVsz25Xd/RoWm6gWVtrPNyOthi0\n1ZxbXc1Tl+xA6oX1oaysNT1Nram3NzfzGINpkbOhoTQgbij7/jDYG4PJrvjTRXG9+qJy2V//ckmc\nb2M2QO25T4wB5nstWxpVMadctqA7Vk7reiCmJR1eeWC5rG+P5bHtvkgvXJbr2R1oi1XZBlPPc3N/\n7vnwNkREREQamXp2RURERKRhNWzP7kBaV8FKKzvklDp022ktbytNOVbqth0YHs7q6o3u21LPcHtr\nbiEIS7m66XtD1+auctnv/u+HAJz3i3MBuOL6m8tl7Yuj93bZsiXlbTdvil7ba3o3A9DZmfXsrjoi\nViS9/+HIs21ryf50C/fYE4DdN/YAsLRrY9b2VMfs5nhcgx1Zb25Tc/b4RURERBqRenZFREREpGEp\n2BURERGRhtWwaQyl6cJaLJsLrC2lH7S0xE/6zbnVxJrSiqalTZ4bhFYavFZaZW14MCsc8qizbzDK\ntnVng8NuuTWtbNYbqRRPOuaZ5bJZ82IFtJ41d5a3Na/fBMDVm+4DYP5uy8tlT53TCUBXZ5xvY3eW\nqvDIltIUZ5Fu0b81S6Vo646pxpraI2VhsCVbea2/tWH//CIiIiKAenZFREREpIE1bNfegrnzAMhP\nrpU6dPHm6KkdGswGr7U1R9zf1BRlTc1Z721++jKAodzKEUMDUdabFmtompUN+jrm5S8FYP5eMRit\nc2424OyW228FYHigN1dv1LU5TXE2uzn78wxsjf2sL87T15v13i5ZGAtOPEg8np6h7HFZW2m6tXg8\n7bkFMdoGskF4IiIiIo1IPbsiIiIi0rAatme3p2t93GidVd7W1NEBQH9vTNHVtW1T7oiYq2xWZ+zf\n1JL10M5P+bUDA5GPO9DXUy5ra4o656TjNw9mi0T0DMXtJQfE1GBzFi4ul911X+Tzzsp1rvanHtr2\n1Fs8mFv04p5NMeXYpt7o4W1uynqe582ONmxsi/3784tetMeiEu3p8bSU5mQDhoez3mERERGRRqSe\nXRERERFpWAp2RURERKRhNWwaw3m/OguAbduyPIG993kkAKVxX830l8u2bo2UgzlzFwLQ0ZNNITZn\ndgws6+3aCsCK3OC1JZ0LgGywm3VlaQy7r1sHQEtPpB5s7L8+q/PWGKC221DWvmub4pzzetM12Xmu\nuuKKaNfaSGfY7cCsfX2pXc2tsX9bezYsr6UpbjcNpwF4HdmffIhssJqIiIhII1LProiMYGarzWzS\nvwmZ2QozczM7a7LPJSIiM1fD9uzedvN1ANx559rytnv2uRuAww8/FICOjvZy2b57HgDA/M7oCe0/\n96flsrX3RQ/tcEv0ws6au6hc1tUTvcPeEk9l2/zZ5bK90wCzvdM+/d3ZwLbH9kfP7LoNWU9w76w4\n92MHIs7oGsoGkK3tiYFlQ/3RS7x5/bpy2SWXXBLnGYp9lszqLJf198c5B7qj53lwuKNc1jI7mwpN\nREREpBE1bLArIjvsjcCsMfcSERGZBhTsisgI7n73VLdBRESkXho22F23LTqmli4/oLxt7tz4eX/T\n+gcA2GLzy2V922Lb44cjTWD27XeWy+a3x8CvPTpi//a5WZpA3/yUFlBa2axrc7lsa3d33Ni2DYCh\n3Kpn92yIgWZ3DmSD5LZ0xVy4R/amuXA9WwltW5on9+rhKLslV9f1d94GwIKtG6J9PdlcukP9cTst\nzsaW9dncwl19C5CZwcxOBF4MPAlYTkwsfS3wFXf/XmHf1cBR7m65bauAPwIfBX4NfAQ4HFgI7Ofu\na8xsTdr9CcB/AC8DFgN3AF8FznT3MXOBzeyRwJuAZwH7AvOAB4DzgI+5+72F/fNt+1k69xHEAop/\nAz7g7n+ucJ4W4J+InuzHEJ+HNwPfBL7s7lpiUESkATRssCsiI3wFuAG4CFhLBKEvAL5rZo9y9w/X\nWM/hwAeAPwHfApZAblqTCDB/DywAzk73XwH8P+BRwNtqOMfLgZOJAPbPqf7HAv8IvNjMnuLu91U4\n7inAvwB/Ab4B7JPOfYGZPdHdby7taGatwC+A5xIB7g+AXuBo4EzgMOANNbQVM7u8StGjazleREQm\nV8MGu4sXx6pley7JemGH+qM3dc1tVwPgLcvKZX3rHgKgY0308LbPyQZyLV68GwC9e0adA51Z2XBz\ndP70b9gYG9ZvzOq8JzqgutfHeYf7u8tlg7NicNi23DRhG7piMNmQRYfa7Fwn2Jy0qtqNw/EnM7Lj\nOpvj9sCmGOy2sTuLPQZS51wr0c4hsl7f3rasDml4B7n77fkNZtYG/AY4zcy+WiWALHoOcLK7/3eV\n8uVET+5B7t6XzvMRoof1rWZ2jrtfNMY5vgucUTo+197npPZ+CDilwnEvBE5y97Nyx7yF6FV+F/DW\n3L4fJALdLwLvdo+fUcysGfga8CYz+7G7/3yMtoqIyC5OU4+JzADFQDdt6we+RHzpPabGqq4aJdAt\n+UA+UHX3DcDH092TamjrfcVAN20/H7ieCFIruSQf6CbfAgaBQ0sbzKwJeDuRGnFqKdBN5xgC3gs4\n8Lqx2pqOObjSBbipluNFRGRyNWzP7utfejQAt96aLeSwdm3ktO67fK+4v623XLatJ3pFW3pigQZy\niy80rYve2tJkX13DWS5tS0/qrd0cx9vmLGd3eFuaVqwvelNzKZDM3xa9rwdZtu0Ai6nQZvkgAEO5\nv85w6pkdSItXNA3l6pobucSWtnUtW1Iu2zxvLgAL7n8wHtbG9VmdQ9njkMZmZvsA7yeC2n2AzsIu\ne9ZY1WVjlA8SqQdFq9P1k8Y6gZkZEWieSOT/LoTcCisj0yby/l7c4O4DZrYu1VHySCKN41bgQ5Z7\nD+b0ACvHaquIiOz6GjbYFZFgZvsTQepC4GLgfGAzMASsAE4A2qsdX/DAGOUP53tKKxw3v0JZ0eeA\ndxO5xecB9xHBJ0QAvG+V4zZV2T7IyGB5cbp+BDHQrhpNRC0i0gAU7Io0vvcQAd5JxZ/5zew1RLBb\nq7FmU1hiZs0VAt7d0/Xm4gGF9iwD3glcBzzN3bdWaO9EldrwU3d/eR3qExGRXVjDBrvdW+Jn+96u\nbIWyWR2xullHUzzsrgfvKJcNdEeKYEd3dA7N6cmm9pqzIDqjFgxEesFAZ/a0zUnbhrsjJWJrV3Zc\n72CkL7QNRwrCYC5Dui8NbOvMDUKb1RI/p/pgbGsezPZ/oDna1zUUx/UOZIUbN8Zj3Mui86p3SZbG\n0L9P/DrdszFihpYNWSMG+6v9GiwN5sB0fW6FsqPqfK4W4GlED3LeqnR95RjH70+MJTi/QqC7Vyqf\nqJuIXuCnmlmruw+MdYCIiExfGqAm0vjWpOtV+Y1m9lxiOq96+6SZldMizGwRMYMCwLfHOHZNun56\nmhmhVMcc4OvU4Qu6uw8S04stB75gZsX8ZcxsuZk9ZqLnEhGRqdewPbtn/+T/ADhw/6wjqLUt/ndu\n64pBZQ9vzHphvdTL2RTTce3eNLtc1tIZC1TMTT28HbkBYB3p19rhvkgpbH8gS2lcf/c9AAylhSry\nw2DaSj/ytmZ/gs6h+O4xnOay72/Jen03EQf0pvM15f5ys5vjcbU0xXHe11MuG0oD5oYGow0DucxF\nH1TP7gzxZWIWhB+Z2blEDuxBwPOAHwLH1/Fca4n83+vM7P+AVuCVRGD55bGmHXP3B8zsbODVwFVm\ndj6R5/tsYh7cq4An1qGdHycGv51MzN37B+J5WUbk8h5BTE92Qx3OJSIiU0g9uyINzt2vIRZL+DOx\nkMQpxKpkLyfmoK2nfmLls/OJgPUtRI7su4jpvmrxD8AniBkj3kZMNfZLIj1i1JzfWqXUhWOJ1dNu\nBl5ETDn2POJz8cPA9+txLhERmVoN27M7nHpor74+m+rymGNiOrKelF87MJibejTl884+cA8Amnuz\naT4X7hnL6nbuFotL3HfH/eWy9WnBiHmd8avt3u3ZErwL5kfK4YPdMeXZhr4sNbAl5fPmc3Y3D0W3\n60BLXA9blpfbPhzLBbc2Rc+uDWXTpg0PRQ+tD0RZ+13rymWz18ZSxa1pyrLetmxBjKbUYy2NLy2X\n+8wqxVbYd1WF41cX9xvlXJuJIHXU1dLcfU2lOt29m+hV/WCFw8bdNndfUWW7EwtYfHe0doqIyPSm\nnl0RERERaVgKdkVERESkYTVuGkNareyONXeVtw2e/3sAurtjgFpXb5ZWsGTuPADu3W8fAOYsyebY\nt9sfAuBHF14CwO/vvbtctmkg0h1K476OWJgNXjtuSSzatDClEHQPZCkLa9KvrksXLcvad1DMELVw\n6XIA1m/YWC6766aYJu3au+4EoKM5S0d4aDj+jAtXxuDxoWwQO90DcZ6B5kh12LQ1m4rtju4sFUJE\nRESkETVssCsiO1e13FgREZGp1LDB7qyOGKD2pMc/rrxtKA0Km9UZvaLLW7Pe28GYtYtNc1KPaUfW\nO/qTO2KQ23kPxcA0X764XNY5Kwa2be6KgWDn3pUNXmsbiPMdlwaC3TeczZH/93S+3WZnq6fOWhaL\nTLWn/e/t6y6X3TM3tt01J1YwnfPg+nLZfRtjgPq16a/ZsVvWW/zYR68EoJno2V3z92xO//tTm0VE\nREQalXJ2RURERKRhKdgVERERkYbVsGkMz3r2swEwskFhpci+tELZ7XfcWS67+84YyNb6YMyJe8ml\nN5fLLlgT+/XNizl02weylcd8W9S/75IYmNbdObdcdt4tkf6wd0qX+OuctnLZwqceHm1pz1Zq87a4\nPdgc+y1bunu5rL0pVjRdvvcKAJYuWlgu2zIQK6bdcNW1ADy0Zm257EkvOi495mjzLddeXy7rH84G\n6ImIiIg0IvXsioiIiEjDatie3aaWWHGM4eHyttISSzYcU4HRnA1Cs6HY75q/XQ7AXQ/cWy7rnh2D\n1mZ1RA/tfvvuUy7rSCuS3XnbbQC0z8l6apuXx0CxKzfEdF8L5mU9tS95ybEAbMxNBbbXXntFHe1x\nnqamrH3rHozpz/r7Y1W1xz3hCeWybd1dAPxm6W8A+OlPf1ou29wV9W/dFte33J1Nm9bTnQ2AExER\nEWlE6tkVERERkYbVsD27g4PRA8pwlrNrHrcHBiN/tb+vr1y2NE3XdW17PCX35XJ9F6bpxQ5/yiEA\nHHzwoeWyAw48AIA//P53APz6vN+Wy2YtiCnKbkmLQzyxJ8v1Hdga05Dtsece5W0dabq03t5Y7GHL\ntmxqsIcffhiADRtimrF777+vXLZ+Q0xDdv0N16d9smnJvn3WN0fUOXtW1vPc3pYtTCEiIiLSiNSz\nKyIiIiINS8GuiOySzMzNbPU49l+Vjjm9sH21mXmVw0REpME1bBpDX0pR8KGh8rbBtKJZb29M1dXf\nm6UxlLb1lNIXmrOnZun8mOZrTlMMervr+uvKZW0e9R900EEAXPbnv5TLNm2JAWAPpDpta5Ze0HbJ\nRQDMmjunvG3WrFglzVO6xZBn/5+3bu1KjyEG0g0ODZbLHnxwHQDdPfEYWqxcxJo1dwAwMBD7L126\nW7msOTdAT6a/FNBd6O6rprotIiIiu4qGDXZFZMa5DFgJPDzVDRERkV1Hwwa73WlaraGBbOGEwdS7\n2dubyoayacl6Uk9wS2tkdrTknpnh9AvoAxseBGDrXdliFLfdFL28hxz9LACa2rLe0j7ifMMeA8/u\nGM56kg/siZ7aOQuynt229tYRj6GnN7d4RVoIY2Ag6igNWAPYtDEGwM2eHYPP9t57z3LZ3Pnzos1d\n8Zi7tvVm9fdkt0WmO3fvBm6a6naIiMiuRTm7IjuJmZ1oZuea2R1m1mNmW8zsEjN7fYV915jZmir1\nnJ5yU1fl6i3lvByVyrxK/upxZnaRmW1ObbjWzD5gZu3V2mBmc8zsDDO7Jx1zlZkdm/ZpMbN/NbNb\nzazXzG43s7dXaXeTmZ1sZn8zs21m1pVun2JmVT+LzGwPM/uumT2Yzn+5mb22wn4Vc3ZHY2bPNbNf\nm9nDZtaX2v+fZrag1jpERGTX1vA9u8ODWW5rKR13YCDybPM9m/390Yu6ePEiANauzab2ammLHtf2\nWbFkb9OyReWy5jR91z33xnLDXSn3F2DIS4tXpP/jlvXc3njTrdHOrmz/Ul7tli2bU1lX1vTU9ra2\n6CWeNyfrEV68OHKK29qjLQtySwkv3ysSeAcHo2e4tzfr6e7KnVt2iq8ANwAXAWuBxcALgO+a2aPc\n/cM7WO9VwEeBjwB3AWflylaXbpjZJ4APED/z/wDYBjwf+ATwXDN7trsX15BuBX4HLAJ+DrQBrwHO\nNbPnAG8FDgN+A/QBrwLONLOH3P2cQl3fBV4L3AN8g3hHvgz4MvB04HUVHttC4M/AJuDbwALgOOD7\nZranu//nmM9OFWb2b8TztgH4JfAg8Hjgn4EXmNnh7r5llCpERGQaaNhgV2QXdJC7357fYGZtRKB4\nmpl91d3vq3xode5+FXCVmX0EWOPupxf3MbPDiUD3HuBQd38gbf8A8FPgRcD7iMA3bw/gCmCVu/el\nY75LBOw/Am5Pj2tTKvsckUpwGlAOds3sNUSgeyVwpLtvS9s/BFwIvNbMfuXuPyic//HpPK/2lMtj\nZp8CLgf+w8zOdfc7xveMgZkdTQS6fwFeUGp/KjuRCKw/CpxaQ12XVyl69HjbJSIi9ac0BpGdpBjo\npm39wJeIL57HTOLp35Su/70U6KbzDwLvBYaBf6xy7LtLgW465mLgTqLX9f35QDEFnpcAjzOz/HQf\npfOfVgp00/5dwPvT3UrnH0rnGM4dcyfwBaLX+Q1VH/Ho3pmu35xvf6r/LKK3vFJPs4iITDMN27Nb\nmr5rMJfGMNAft7u74+f7DRs2lMtK03dt3RYpBPnBawN9keLQ3hopBB3Lds9OlP6fP/BgDBjbtjWX\nxpCOGxiOX4bzKRV33rEGgLvvuqe8rakpvnu0p1SFOXNmlcsWzI8UwkUpzWLxwixVYc7s2K+pJdIk\nmvJTipXjjUhncM/mJRslTVImgZntQwR2xwD7AJ2FXfbc7qD6eXK6/kOxwN1vMbN7gf3MbEEh+NtU\nKUgH7gf2I3pYi+4DmoHd0+3S+YfJpVXkXEgEtU+qUHZ3Cm6LVhNpG5WOqcXhwADwKjN7VYXyNmCp\nmS129/UVysvc/eBK21OP75MrlYmIyM7TsMGuyK7EzPYnpsZaCFwMnA9sJoK8FcAJwHaDxOpofrpe\nW6V8LRGAzyfyY0s2V9l/EMDdK5WXvtXlpxeZD2xIPdkjuPugmT0MLKtQ17oq5y/1Ts+vUj6WxcTn\n30fG2G8OMGqwKyIiu7aGDXbNogfTyHoyS7+EpiI6OzvKZU2pA7SvLwatlXpZAdavj/91D6XpvubM\nml0u6+qJX2TXPvAQAP192f/yRQujN3ZeWjhicDBb4KI5DVrr6MjaMCtNHTYnLS4xNzcIbU4qK+1f\n6v0FaG5pTtcRWwzkeqWHhqOHezhtGh7KLySVW31CJtt7iADrpPQzeVnKZz2hsP8w0btYyY7MFFAK\nSncn8myLlhf2q7fNwCIzay0OgjOzFmAJUGkw2G4VtkE8jlK9O9qeJndfNOaeIiIyrel3bJGd48B0\nfW6FsqMqbNsI7GZmrRXKnlLlHMNE+kAlV6brVcUCMzsQ2Au4s5i/WkdXEp83R1YoO5Jo9xUVyvYx\nsxUVtq/K1bsjLgUWmtljd/B4ERGZJhTsiuwca9L1qvxGM3sulQdmXUb88nJSYf8TgSOqnGM9sHeV\nsm+l6w+Z2dJcfc3AfxGfBd+s1vg6KJ3/k2ZWTkZPtz+V7lY6fzPw6fw8vGa2HzHAbBD43g6254x0\n/XUz26NYaGazzeypO1i3iIjsQho2jaE5DdKy9iwNsjwALK1U1tqW+xnfIsVgcDB+Yc3Pcbt1c/y6\neuVVVwOwaMmScllfmp+3K61Qtucey8tlj125EshWNiulLkCWZtHUlHXEtba2pP3iujU30Kw06Kz0\nGJrzZamu4TT4bHA4S1XoT3P39vXH4+rLzbObH4Qnk+7LROD6IzM7lxi4dRDwPOCHwPGF/c9M+3/F\nzI4hpgx7AvA0Yk7YF1U4xwXAq83sF8TAsUHgIne/yN3/bGafAf4FuM7Mfgx0EfPsHgT8CdjhOWvH\n4u4/MLOXEnPkXm9mPyPm2T2WGOj2Q3f/foVDryHm8b3czM4ncnSPJ1I5/qXK4Lla2nOBmZ0GfBK4\n1cx+TcwwMQfYl+ht/xPx9xERkWmsYYNdkV2Ju1+T5nb9d2IhiRbgauDlxICw4wv732BmzyLmvX0x\nEbheTMwi8HIqB7vvIgLIY9I5moi5Yi9Kdb7fzK4E3g68kRhAdjvwIeCzlQaP1dlriJkX3gS8JW27\nEfgsseBGJRuJgPwzRPA/j1iY478qzMk7Lu7+aTO7hOglfjrwUiKX9z7ga8TCGxOx4sYbb+TggytO\n1iAiImO48cYbIQZxT4iVpugSEZH6MbM+Ig3j6qlui8xYpYVNbprSVshMVY/X3wpgi7vvN5GGqGdX\nRGRyXAfV5+EVmWyl1f30GpSpsCu9/jRATUREREQaloJdEREREWlYCnZFREREpGEp2BURERGRhqVg\nV0REREQalqYeExEREZGGpZ5dEREREWlYCnZFREREpGEp2BURERGRhqVgV0REREQaloJdEREREWlY\nCnZFREREpGEp2BURERGRhqVgV0REREQaloJdEZEamNleZvYtM7vfzPrMbI2Zfd7MFo6znkXpuDWp\nnvtTvXtNVtulMdTjNWhmq83MR7l0TOZjkOnLzF5pZmea2cVmtiW9Xr63g3XV5fO0Vi2TUamISCMx\nswOAPwPLgJ8DNwGHAu8CnmdmR7j7+hrqWZzqeSTwB+Bs4NHAScALzexwd79jch6FTGf1eg3mfLTK\n9sEJNVQa2YeAJwDbgHuJz65xm4TX8pgU7IqIjO3LxAfzO939zNJGM/sccCrwH8DJNdTzCSLQPcPd\n35Or553A/0vneV4d2y2No16vQQDc/fR6N1Aa3qlEkHsbcBTwxx2sp66v5VqYu9ezPhGRhmJm+wO3\nA2uAA9x9OFc2F1gLGLDM3btGqWc28BAwDCx39625sqZ0jhXpHOrdlbJ6vQbT/quBo9zdJq3B0vDM\nbBUR7H7f3V8/juPq9loeD+XsioiM7pnp+vz8BzNAClgvAWYBTx2jnsOBTuCSfKCb6hkGzk93j55w\ni6XR1Os1WGZmx5vZaWb2HjN7vpm116+5IlXV/bVcCwW7IiKje1S6vqVK+a3p+pE7qR6ZeSbjtXM2\n8Engs8CvgbvN7JU71jyRmk3J56CCXRGR0c1P15urlJe2L9hJ9cjMU8/Xzs+BFwN7Eb80PJoIehcA\n55jZ8yfQTpGxTMnnoAaoiYhMTCn3caIDIOpVj8w8Nb923P2MwqabgX81s/uBM4lBlL+pb/NEajYp\nn4Pq2RURGV2pp2F+lfJ5hf0mux6ZeXbGa+cbxLRjT0wDhUQmw5R8DirYFREZ3c3puloO2SPSdbUc\ntHrXIzPPpL923L0XKA2cnL2j9YiMYUo+BxXsioiMrjSX5HPSFGFlqQfsCKAHuHSMei5N+x1R7DlL\n9T6ncD6Rknq9Bqsys0cBC4mA9+EdrUdkDJP+Wq5Ewa6IyCjc/XZiWrAVwNsKxR8lesG+k58T0swe\nbWYjVhdy923Ad9P+pxfqeXuq/zzNsStF9XoNmtn+ZrZnsX4zWwJ8O9092921ippMiJm1ptfgAfnt\nO/Jarkt7tKiEiMjoKixveSNwGDEn7i3A0/LLW5qZAxQn7q+wXPBlwErgpcCDqZ7bJ/vxyPRTj9eg\nmZ1I5OZeSEzsvwHYB3gBkUP5d+DZ7r5p8h+RTDdmdixwbLq7O/Bc4A7g4rTtYXf/57TvCuBO4C53\nX+7Cl9gAACAASURBVFGoZ1yv5bq0XcGuiMjYzGxv4GPEcr6LiZV+fgZ81N03FPatGOymskXAR4h/\nGsuB9cTo939z93sn8zHI9DbR16CZPQ54L3AwsAcxGGgrcD3wQ+C/3b1/8h+JTEdmdjrx2VVNObAd\nLdhN5TW/lutBwa6IiIiINCzl7IqIiIhIw1KwKyIiIiINS8HuBJmZp8uKqW6LiIiIiIykYFdERERE\nGpaCXRERERFpWAp2RURERKRhKdgVERERkYalYHcMZtZkZu8ws6vNrMfMHjKzX5jZ4TUc+yQz+56Z\n3WNmfWb2sJmdZ2avGOO4ZjN7t5ldkzvnL83siFSuQXEiIiIiNdCiEqMwsxbgx8RSngCDwDZgQbp9\nPHBuKtvP3dfkjv0n4CtkXyg2AXOB5nT/e8CJ7j5UOGcrsXze86uc89WpTdudU0RERERGUs/u6N5P\nBLrDwPuA+e6+ENgf+D3wrUoHmdnTyALdHwN7p+MWAB8EHHg98IEKh3+ICHSHgHcD89KxK4DfEuua\ni4iIiEgN1LNbhZnNBu4n1g7/qLufXihvB64AHpM2lXtZzewC4JnAJcBRFXpvP0EEutuAPd19S9o+\nB3gAmA180N0/UTiuFfgb8ITiOUVERERke+rZre45RKDbB5xRLHT3PuC/itvNbBFwdLr7yWKgm3wa\n6AXmAC/IbX8uEej2Al+ocM4B4HPjehQiIiIiM5iC3eqenK6vcvfNVfa5sMK2JwFGpCpUKifVd3nh\nPKVjS+fcVuWcF1dtsYiIiIiMoGC3uqXp+v5R9rlvlOM2jxKwAtxb2B9gSbpeO8pxo7VHRERERHIU\n7E6e9h04xmrYR0nWIiIiIjVSsFvdQ+l6j1H2qVRWOq7TzJZWKC/Zq7B//vbycZ5TRERERCpQsFvd\nFen6iWY2r8o+R1XYdiVZ7+vRFcoxs/nAwYXzlI4tnXNOlXM+o8p2ERERESlQsFvdecAWIh3hXcVC\nM2sD3lvc7u4bgD+mu+83s0rP8fuBDmLqsV/ntp8PdKWyt1U4Zwtw6rgehYiIiMgMpmC3CnfvBj6T\n7n7EzN5jZp0AaZnenwJ7Vzn8w8RCFE8GzjazvdJxc8zsX4HT0n6fKs2xm865lWyas39PyxSXzrkP\nsUDFfvV5hCIiIiKNT4tKjGKCywW/Bfgy8YXCieWC55EtF/x94IQKC060Ab8g5vkFGEjnXJhuHw/8\nJJXt4e6jzdwgIiIiMqOpZ3cU7j4IvAJ4J3ANEeAOAb8iVkb7ySjH/jdwCPADYiqxOcBm4HfAq9z9\n9ZUWnHD3fuCFRIrEdUQP8RARAB9JliIBEUCLiIiISBXq2Z1mzOwY4PfAXe6+YoqbIyIiIrJLU8/u\n9PO+dP27KW2FiIiIyDSgYHcXY2bNZvZjM3temqKstP2xZvZj4LlE7u4XpqyRIiIiItOE0hh2MWlQ\n3EBu0xagBZiV7g8Dp7j713Z220RERESmGwW7uxgzM+Bkogf3ccAyoBV4ALgI+Ly7X1G9BhEREREp\nUbArIiIiIg1LObsiIiIi0rAU7IqIiIhIw1KwKyIiIiINS8GuiIiIiDSslqlugIhIIzKzO4F5wJop\nboqIyHS1Atji7vtNpJKGDXY/e9opDtDR2Vre1t3dHzeamgFYsGBOuaytJfbb1tUNwNBgd7nMmtsB\n6B8YjH22bi2X7bZsNwCGh4cAmDW7s1zW1dUXx/kwAD392fS5g3090RSz8rbePk/7RTv7BnrKZR3t\n0eaW5uiM72ydXS5btnQZAEuXLik+DTS1tsX50rkHB/vLZU6c73Unv9+2O1BEJmpeZ2fnopUrVy6a\n6oaIiExHN954Iz09PWPvOIaGDXaHPeK33t4swGxNgeLAYASf/UNd5TJvj8Cvoz2CVW+ZWy5Lu9PS\nFE/X7La2cllHWwTCaRfmzskC6ObmCKD7uuMPNcuyp3vOovj/19qSTf22rSfa2tUbQTK5QLhz9rz0\neEpB8nC5bO7cCHw7O6PtfX195bK9990nPcB47D3d28pl/X29iMikWbNy5cpFl19++VS3Q0RkWjr4\n4IO54oor1ky0HuXsisguw8xWmJmb2Vk17n9i2v/EOrZhVarz9HrVKSIiU0fBroiIiIg0rIZNY2jv\niPQCa85SAQa64+f9ppSywKwsxWFrSmlY2NIBwOBgbmW5lH7QktIKmpuzp625JX1fGIq0gq6uLDWg\nuSmdO+XzNuW+WqQsC3p6B8vbtmzZDEDv/2fvvuMrO+r7/78+t6mvtheX9doGFzDNdgBjgpdqQgng\nAAZSMHxJIIQHLQWbQDAklDScxAQIIcS/OPClxAFS4IdDMTY2BHDBuGK8Fl7vevuutGpXuvfO94/P\nnOLrq5V2V9py/H4+HvdxpDPnzMzRaqXR5858pu79Wr5iWVq2fKlPq0imC+c3vkumZzSmfT5uV1c2\nzaIWp1lYbHxqOte/VhciR7kvAz8AHjzcHenktk3DrLv4vw93N0REDouhj7zwcHcBKPBgV0SKL4Qw\nDAwf7n6IiMiRq7CD3a6aLw5rtLLIabnq0c3KQAyLVrPIbl+p18tKHhWdamWL10otr6srZmWYnsqi\noxMxq0K54l/KqbEs20Gr5dHeWtXv7+nvTcuqMftD6OlOz03GlXBW9vqnp5tp2d6RES+L0eXunizr\nQz0uaJuu7wZgzdq1adl0jPaGmBGiuyeL5ta6sjpEjjRmdhrwEeAZQBdwM/CBEMLVuWsuAv4ZeF0I\n4Yrc+aH44eOBS4ELgGOBD4YQLo3XrAI+BLwITxF2N3AZ8IsFeygRETnkCjvYFZGj2onA94HbgH8A\n1gAXAl83s9eEEL4whzpqwLeBpcDVwAhwH4CZLQNuAE4Cvhdfa4BPxmvnzMxmSrdw2v7UIyIiC6Ow\ng93pmBN3cipLw5VEWEstj+y26rlUYANxfuyUX1OuZXN9q8QobCPO2a1m9yVR1WbwKGxuijClOE+2\nJ+be3T2a5YrbsWszAI89dV16biBGfmtJ/bnUY63YZzOvs6uazcuNQVsmJjyt2Pholgc43kar6f0L\npaxOK2U5iEWOMM8A/iqE8IfJCTP7GD4A/qSZfT2EMDJLHWuAO4DzQghjbWUfxge6fxNCeEeHNkRE\npCCUjUFEjkTDwAfyJ0IIPwY+CywGXjbHen6/faBrZlXg14G9+BSHTm3MWQjhrE4v4K79qUdERBaG\nBrsiciS6KYSwt8P5a+LxSXOoYxK4tcP504Be4Ja4wG2mNkREpAAKO42hXEoeLVuE1t3l0wT6en1a\nQblWTsuS7XdbZX+7v9bIFm9Z3H0sxAVnZtmisq5YV33CF4IlO6oBtGJ+sFLZ29m1e09a9oX/9GmB\nT7j3Uem5837Jf38n0y3yKc5aeL8aTZ+eUc9tPZykP6vFdGvNZrawbWLcg1pdXV42MZZtg5xsfyxy\nBNo6w/kt8Tg4hzq2hZBP0pdK7p2tDRERKQBFdkXkSLRqhvOr43Eu6cY6DXTz987WhoiIFEBxI7s1\nX8C1uDtbyJWm+4q/ApOILcDEqC/uSlZ7NXORU2Jasb4lSwGo781+z07F68oVj95Wu7JFX3GfCcZi\ndHXv3tG0LFk49sOb70jPHXfMGgCe+ZSzAIiZyPzeuOisq+rt3H3fA2nZ92+6xe9f47+7n3rW49Ky\nlXFjikqMFtdy0dxaTZtKyBHrTDMb6DCVYX083nwQdd8FjANPNLPBDlMZ1j/8lgNzxrGD3HiEJFUX\nEXmkUmRXRI5Eg8Cf5E+Y2dn4wrJhfOe0AxJCmMYXoQ3QtkAt14aIiBREYSO7InJUuxZ4g5k9Bbie\nLM9uCXjjHNKOzebdwLOBt8cBbpJn90Lga8CvHmT9IiJyhCjsYDdJVVuvZ9MRkgVm3T0+taFk2eNP\nNRrxGp9fMDqZ7ZJmMV/u8Ehc7FXLLxyLu6TFc6VSNk1wMradtLtocFFadvoppwAwsidbtPajm28D\n4MlPeCwAldw0gxC8/nvvfxCAf/nSV9OyjZt9Pc1Jxx8PwKknZjuoDfT5YrrYBcbHsyxM5VyuXpEj\nzH3Am/Ad1N6E76B2E76D2jcOtvIQwg4zOxffQe3FwNn4Dmq/Cwyhwa6ISGEUdrArIkefEMIQkNua\nhZfMcv0VwBUdzq+bQ1tbgNfPUGwznBcRkaNMYQe7w7v9Xc6e3r70XLXmi7SqcbFWboOydNl2Jabv\nWlTJ7S4Wdx3rjTuctXIrx5qtGBGO6b4m69kCsEaMEi9bvtzLcveNDHtEd+e27em5vuUrALjuxju9\nvVpuSrV5/bfcfp/fP5otdjs+Lmw79ZQT/b6eLDXa7l27Aah1+e5t+XRjSToyERERkaLSAjURERER\nKazCRnYpeYquUiXbOGJgYAAAMx/jj49l0dEQw7wWj+Vqdl+1mszx9fu6+7MNJypxw4jdO3cB0Gpl\nGzpUk7ZjrrOBvt60rDumRKvnct6X674xxdA2z4TUVcsitKMTnoFp+16fc7s6RosBLEaen/CYkwFY\ntWJpWpZsZFGfqPsz5Obp1ifriIiIiBSZIrsiIiIiUlga7IqIiIhIYRV2GkMtLr6abmQLsupxmkA5\nTj2wcjZVoRzTiyXTEpqtbDFZpRqnPcS0XRs2bUvLjl3t0wkGlyyK92V92LVzJwBbtu8AoKsrm5bQ\nH6dU7B7NUoH1LloMQMl8asPKVdluptU9vmBu2+bN3k5uI9TVS5cAsGywH4BGM5tKkXwdmuW4kK6U\nPXOjMdNuqiIiIiLFoMiuiIiIiBRWYSO7PTH9VpJuDKBW88VZySKycjkb6yebSVhMQpZEer3soXX/\n7N770o/v2+iR1mc99YlAFsUF+N9bbgdg+05fJPbYxzw2LVt9/EkAlP73xvRcf+zz3l2+2G165eq0\nrDntUelqjMw2cwvvnnrW4wBYPDgIQH0q20gj0RMXx1WqWbqxZrP1sOtEREREikSRXREREREprMJG\ndnt7PT1YpZo9YpI6LInsJinIAAJ+Lpnj22plO040Y3qwUoz2LhvMUoj99GcPAHBdrOund92Zlm3e\n7hs6jI75vNyTTj0jLevuSTaoyKKwlbK3WYlR2927d6VlFvswOOD3nXnGSWnZ6Y/2zSSSqHSlkqUX\nq8Zodi1GjWtdWdq06Q4RYBEREZEiUWRXRERERApLg10RERERKazCTmMoV/zRGrnUY4ZPEyjF9GLd\n3dlirca0v6U/MT4OQHM6e4u/1Yy7qlV8sduyRf1ZQy2/7vs3/wSAnbt3p0WVql8/VZ8E0o3UANi+\nfVtsdyo9NzziC9l6+jyN2fJqtrhucNDTko0N+31nP/70tKw7TtkYG/HpEl1d2d8wyTSGpC/12BeA\nxlT2tREREREpIkV2ReQhzOwaM1vwJMxmts7MgpldsdBtiYjII1dhI7ulmFasmVtolixQq8QIbWM6\nF9lMf7XH6G/uz4Bk0drO3cNeRlbn4j5f+HXPfb/wunML4uqTHkVNFsINj4ykZVP1euxflv5rz55d\nsSt+/SmnnpKWrTnmeAB2bx0CoDuXUi3ZLKO7p+ch7XlZbCfmT2u1sjFMMP2tIyIiIsVW2MGuiByw\n3wJ6Z71KRETkKKDBrog8RAjh/sPdBxERkflS2MHudFxg1spNE0jetU+mEITcirFkd7VG3FXMLPvS\nVONCtsqk19mVWzh27PJlANySLIjLTXEol/26RYv8/vG4+M3LfJHcQP9Aeq5kyb3er9tvuz0te2Dj\nJgCOX7kSgPrYRFoWql7X8mXL/dlz+XNbwfMHj42Net97soBdV6+Cd48UZnYR8GLgScAaYBr4KfCJ\nEMK/tl17DXBeCMFy59YD3wHeD3wNeB9wDrAEODGEMGRmQ/HyJwAfBF4GLAM2AJ8ELg/5/3Qz9/UU\n4PXAc4ATgEXAFuAbwAdCCA+0XZ/v21di2+cCNeBHwCUhhBs6tFMBfgePZD8G/3l4N/BPwMdDaN87\nUUREjkaatCnyyPAJYB1wLfA3wOfxgeSVZvan+1HPOcB1QDfwGeD/A6Zy5TXgm8D5sY1/BBYDfwt8\nbI5tXAC8CdgI/F/gcuAO4A3Aj8zs2BnuOxu4Ifbt08B/AU8HvmVmp+YvNLNqLP/72L/PAZ/CfyZe\nHp9LREQKoLCR3bYgKQCNhkc5m02PfFouCtvV7ZHd/v6+eE0zLSvHSG5vjIQ2JrP0XWPTMa1Ysvit\nVE7LSuYfd3V5ZHfLli1p2XHH+u/rFStXp+eG9+wEYMkSjxZv2PDztCxMezT61b/6XCDbEQ2gFvuV\n7NTW378o94XwQ3e8vp6L+j5kgZ4U3RkhhHvzJ8ysBnwduNjMPhlC2DSHep4HvCmE8A8zlK/BI7ln\nhBDqsZ334RHWN5vZF0II187SxpXAZcn9uf4+L/b3PcDvdrjvhcDrQghX5O55Ix5Vfhvw5ty1f4wP\nyD8GvD0EfwvEzMr4oPf1ZvZvIYSvztJXzOzGGYpOm+1eERFZeIrsijwCtA9047kpPLJZAZ49x6pu\n2cdAN3FJfqAaQtgFJNHj182hr5vaB7rx/NXA7fggtZPr8wPd6DNAA3hycsI8Xclb8KkR70gGurGN\nJvD7+J/Jvz5bX0VE5MhX2MhuI0YwzbLoLXEKYrnkj52fPjg97dPzurr8mtxUX0KcxzsV5wHv3jOc\nlm3btdfbi5Fgy6X2KsUobzWmI9sZN5IAmBjz+1auWpmee/SjTwJg6dKl3s9KLS0bjCnOVq1aAUAt\ntyFGd49vctGY9g0tSrXsn7UcI87Jxhbk+5eLbEuxmdla4F34oHYt0NN2yUxTA9r9cJbyBj6VoN01\n8fik2Row/0/768BF+PzfJUA5d8lUh9sAftx+IoQwbWZbYx2JU/C5xPcA73nIz4jMBHB6p4IObZzV\n6XyM+J45lzpERGThFHawKyLOzE7CB6lL8Pm2VwPDQBOfx/taoGum+9tsmaV8Rz5S2uG+wTm08VHg\n7cCD+KK0TfjgE3wAfMIM9+2Z4XyDhw6Wl8Xjo/GFdjPp30eZiIgcJTTYFSm+d+IDvNe1v81vZq/G\nB7tzNVs2heVmVu4w4E0mpw+339DWn5XAW4HbgKeFEPZ26O/BSvrw5RDCBfNQn4iIHMEKO9idmPAp\nf9VaFtDpjYu0kl3LWiE3ZTn+bh4f86kKjUY2jyFZ2NaIO6lNTEw8rKyZpizLxgLJgrZKTEvWamW/\n/zds2ADAunXr0nO/+qsvAWDzg57mNDSyuvoq/lbrxLi3PTiYBcgWL/F3aEux7UZuDkZPsqta/Lye\nW1zXQpmVHiEeFY9XdSg7b57bqgBPwyPIeevj8eZZ7j8JX0twdYeB7nGx/GDdhUeBn2pm1RDC9Gw3\niIjI0UsL1ESKbyge1+dPmtn5eDqv+fZhM0unRZjZUjyDAsA/z3LvUDw+PWZGSOrox9OYHfQf6CGE\nBp5ebA3wd2bWPn8ZM1tjZo852LZEROTwK2xktxQ3bejpz6bdJRs5NCY8YDQ9lUVap2MQNcQYaBLF\nhdxit3Ky4CzbVKKS1Nn067tq2aKy3rZNG2q5sqQvmzZvTs+NjPrGD8uW+SK0ifg5wOSe7f5cMQJd\nn87W6Ew3Yv9K/rdLNZf+bLI+FZ8hiXTn+lCd6zRNOcp9HM+C8CUzuwqfA3sG8Hzgi8CF89jWg/j8\n39vM7D+AKvByfGD58dnSjoUQtpjZ54FXAbeY2dX4PN/nApPALcAT56Gff4ovfnsT8GIz+zb+dVmJ\nz+U9F09Pdsc8tCUiIoeRIrsiBRdCuBV4Jp4l4QV4jtpF+OYNn5zn5qbwnc+uxgesb8TnyL4NT/c1\nF/8H+BCeMeL38FRj/4VPj9jnnN+5ilMXXorvnnY38CI85djz8Z+L7wU+Ox9tiYjI4VXYyG4y27Ux\nnU3Ha8ZNFKZjGDdJNwZQwiOmyeYQzWYushvTitUnfL5rPupbiX8uVMr+Qf9AtqFDrepR1Knph0dV\nk7m0IyPZ7+7xcd8UYumytQCMDY9kDxT7kKQcS+buAkym83CTtGnZc01PxWeOqcfyUelka2Qpvrhd\n7rNmKLa2a9d3uP+a9uv20dYwPkj9vVmuG+pUZwhhHI+q/nGH2/a7byGEdTOcD/gGFlfuq58iInJ0\nU2RXRERERApLg10RERERKazCTmNYNOjTCXbt3JmdbPr0hWZc0FWNqcggm7YwOu5v99db2QKwEBd+\nlVq+8KvVzBa21WJKsIGervh59iVtNb2dWpziUKtkC8eSBXTlUvb3xujYuLcXWrG9bLpEclWy29P0\nVDY9YzROhbB4VTk3VSFZtNaMO6eFqey5yrV8nn0RERGR4insYFdEDq2Z5saKiIgcToUd7Nbjhgzd\n/dmCse6aRzy3xXRfpUYWHS0lKcdi1HZ4OstnH1petqzLN2+wahYR7YvR4XXHrAJgx3B239iwLzhb\n1OeL0fKB1FMffTIAp592WnpuOqYJ27vLo9ETIzvSslpMJ2ZxAV2tlkVvk80qxvbujZ9nC89q3d6/\nrnh9rSuXbqxS2H9+EREREUBzdkVERESkwDTYFREREZHCKuz72H3d/rb/8WtPTs9Nx4Vf4+Oev3Zi\n55a0bKTh4/6RKZ8KMBmyKQ7LaoMAWFzk1QhpEStW+vQFK/uXMrSyxWsPjPk0hvExPzcwmE2pWLN6\nmd+/JDt374YNAGwZugeAVYuzXUxL5vWvXLUagGot+6erxIVv5XjcvX1XWtY94bl9h+Ozd/dmdS5d\nuRoRERGRIlNkV0REREQKq7CR3ePXecR0KvSl5ybHfOHXcWtPAmBoMkvD1drt0dBmwyOgNpWtJqvU\nfJFXKPlCta5caq9qjPYmO5OtWpVFS7fvjDugtTxKXA5Z1HfT0BAAm++/Pz23ebv3b92xxwBwyton\npmWjo6NeR9n7MD6W7aBWqvhzTMV0ZMluaQA9Xf5PXI1/1kzFXdoAdu/YhoiIiEiRKbIrIiIiIoVV\n2Mju7T/1+bil7vH03GBMATYw4NHeVcevS8vqU76BQ5K2q5xLSzYVN2IY6O8FoC8373XPsEdvSzEl\nWL2e3Tc+4RHW3j5P99XVk6X9qkz7dclmDwADsX99/QPeh2otLVtz7BogiyBPTmTR26R/MfBMrVbL\nlflzdXVVY1+ySDemv3VERESk2DTaEREREZHC0mBXRERERAqrsNMYfnr7bQBUK/3pubOf/GQASnGR\nV8gtGOsfXAzAju3bgSzNGGS7j/X1xOkLIduhrDntUwjK5teXLKvzKU/z9pqxndHcgrBlq5cDMN1o\npOdOW7wSgKeeex4ADw7dnT1QbLNkcae36WxxXTOZEhH7kN8lzeLfM8nTTDWyvpdK2ZQLkSONmQXg\nuyGE9XO8fj3wHeD9IYRLc+evAc4LIdj891JERI50iuyKFISZhTiwExERkaiwkd3phkc+QyNboLZ9\nl6cXW7TYF2mNT2TR0f5+T1VW6/Y0Y8O7h9OyJBzUbHpUtNHI7mvEqGqIkdfFiwbSsmec/wIAdu/e\nAcCN130nLRuMi+RqtSyN2fJVxwLQ1+N9qJSy9GfVZPVZXEBXIgtSVWMktxrrSiLXkKVSS5Qr2T95\ns9lApEB+CJwO7DjcHRERkSNHYQe7IvLIEkIYB+463P0QEZEjS2GnMfT39Pirt5q+tm59kK1bH2Tz\nA5vZ/MBmSlZKX9WuGtWuGr0D/fQO9FOrVdJXCBACjI2NMzY2Tn1yKn1Vy2V/VapUK1Vq1XL62jh0\nHxuH7mPvzh3s3bmD5UuXpK+e7h56unvo681eUxMjTE2MMHTXTxi66yeUaaQvs4BZoNls0Gw26Omu\npq/FiwdYvHiA7p5uunu60/6GAM1Wi2arRQiBEAKNxnT6Ss7JoWFmF5nZVWa2wcwmzGzEzK43s9/o\ncO2QmQ3NUM+lccrC+ly9yT/kebEseV3adu8rzexaMxuOffipmV1iZl1tzaR9MLN+M7vMzDbGe24x\ns5fGaypm9m4zu8fMJs3sXjN7ywz9LpnZm8zsR2Y2amZj8ePfNZs5D56ZHWNmV5rZttj+jWb2mg7X\nre/0zPtiZueb2dfMbIeZ1WP//9LMFs+1DhERObIpsity6HwCuAO4FngQWAa8ALjSzE4NIbz3AOu9\nBXg/8D7gF8AVubJrkg/M7EPAJfjb/J8DRoFfAT4EnG9mzw0htK9arAL/AywFvgrUgFcDV5nZ84A3\nA08Bvg7UgVcAl5vZ9hDCF9rquhJ4DbAR+DS+bvJlwMeBpwO/3uHZlgA3AHuAfwYWA68EPmtmx4YQ\n/nLWr84MzOxP8K/bLuC/gG3A44E/AF5gZueEEEYOtH4RETkyaLArcuicEUK4N3/CzGr4QPFiM/tk\nCGHT/lYaQrgFuMXM3gcM5TMR5No5Bx/obgSeHELYEs9fAnwZeBHwh/jAN+8Y4CZgfQihHu+5Eh+w\nfwm4Nz7Xnlj2UXwqwcVAOtg1s1fjA92bgWeEEEbj+fcA3wVeY2b/HUL4XFv7j4/tvCrEifFm9hHg\nRuCDZnZVCGHD/n3FwMyeiQ90vw+8IOl/LLsIH1i/H3jHHOq6cYai0/a3XyIiMv8KO9htxIVZzVKW\nCmx8WxxHxEVlJ5ywLi1rxkVnA3GHsd4T1qZlY2NjANTrcdFbK1v0VYkLvvr7/F3Ycm7B2fCOzQBM\n7N0LwKKBbPeyciVeX8kWoVXKXld3ly8wK5eyf576lPe5VouL0XL3tUjSkSXXZH1I3ttuNf3rkOy2\nBlAuF3YWyxGpfaAbz02Z2d8DzwKeDfzLAjX/+nj8s2SgG9tvmNnv4xHmN/DwwS7A25OBbrznOjO7\nDzgReFd+oBhC2GBm1wO/bGblkOX3S9q/OBnoxuvHzOxdwDdj++2D3WZso5W75z4z+zs8kv2b+KB0\nf701Hn873/9Y/xVm9jY80jzrYFdERI5shR3sihxpzGwt8C58ULsW6Gm75NgFbP7MePx2e0EI4Wdm\n9gBwopktbhv87ek0SAc244PdTlHNTUAZWB0/TtpvkZtWkfNdfFD7pA5l94cQ7utw/hp8sNvpjs2U\n3wAAIABJREFUnrk4B5gGXmFmr+hQXgNWmNmyEMLOfVUUQjir0/kY8T2zU5mIiBw6hR3sTk3+HIBN\nO3rTc901T+nVmo7xzlxqr+OPXQ1AuVIDYGAgW5+SpCXbus0DYuNjWTqzUqwjifCWcmv+enq9zJrd\nse6sPYubQ9SquQ0g4hqjauxDFpeFrpheLLT8vnyE1sqxnbRPWR/6en081YyR3fGxibSsldtUQxaW\nmZ2Ep8ZaAlwHXA0M44O8dcBrgYctEptHg/H44AzlD+ID8EF8fmxiuPPlNABCCJ3Kk5x21dy5QWBX\nCGGq/eIYXd4BrOxQ19YZ2k+i04MzlM9mGf7z732zXNcP7HOwKyIiR7bCDnZFjjDvxAdYrwshXJEv\niPNZX9t2fQuPLnZyIJkCkkHpanyebbs1bdfNt2FgqZlV2xfBmVkFWA50Wgy2aob6VufqPdD+lEII\nSw/wfhEROUpo0qbIofGoeLyqQ9l5Hc7tBlaZWbVD2dkztNHCpw90cnM8rm8vMLNHAccB97XPX51H\nN+M/b57RoewZeL9v6lC21szWdTi/PlfvgfgBsMTMHnuA94uIyFGisIPdFdVNrKhu4vRH1dNXqbSX\nUmkvlVKgUgrs2bk9fe3eM8buPWO0rEzLyoyMj6UvK1ewcoW+vj76+vool0hfk/UJJusTVEuBailg\n1kpfpXKVUrlKd08v3T29VKq19FWt+avZbKUvn4hgD/vcXwEITDem/DVVT1+TExNMTkzQarVotVrU\n6/X01WoFWq1AaEFoQSu00lez2UynN8iCG4rH9fmTZnY+vjCr3Q/xd15e13b9RcC5M7SxEzh+hrLP\nxON7zGxFrr4y8Ff4z4J/mqnz8yBp/8Nmls4tih9/JH7aqf0y8Of5PLxmdiK+wKwB/OsB9ueyePxH\nMzumvdDM+szsqQdYt4iIHEE0jUHk0Pg4PnD9kpldhS/cOgN4PvBF4MK26y+P13/CzJ6Npwx7AvA0\nPCfsizq08S3gVWb2n/jCsQZwbQjh2hDCDWb2F8AfAbeZ2b8BY3ie3TOA7wEHnLN2NiGEz5nZS/Ac\nubeb2Vfwv+Beii90+2II4bMdbr0Vz+N7o5ldjc/RvRCfyvFHMyyem0t/vmVmFwMfBu4xs68B9+Fz\ndE/Ao+3fw/99DtS6O++8k7PO6rh+TUREZnHnnXeCr2s5KIUd7F7yDxtt9qtEDo0Qwq0xt+uf4Wm+\nKsBPgAvwBWEXtl1/h5k9B08F9mJ84HodnkXgAjoPdt+GDyCfHdso4Wm5ro11vsvMbgbeAvwWvoDs\nXuA9wF93Wjw2z16NZ154PfDGeO5O4K/xDTc62Y0PyP8CH/wvwjfm+KsOOXn3Swjhz2OatLfim1q8\nBJ/Luwn4FA9Pg7a/+icmJpo33XTTTw6yHpEDleR61jbacjjMx/ffOjqv59gvpu1iRUTmX7LZxEyp\nyUQWmr4H5XA6kr7/CjtnV0REREREg10RERERKSwNdkVERESksDTYFREREZHC0mBXRERERApL2RhE\nREREpLAU2RURERGRwtJgV0REREQKS4NdERERESksDXZFREREpLA02BURERGRwtJgV0REREQKS4Nd\nERERESksDXZFREREpLA02BURmQMzO87MPmNmm82sbmZDZvY3ZrZkP+tZGu8bivVsjvUet1B9l2KY\nj+9BM7vGzMI+Xt0L+Qxy9DKzl5vZ5WZ2nZmNxO+Xfz3Auubl5+lcVRaiUhGRIjGzk4EbgJXAV4G7\ngCcDbwOeb2bnhhB2zqGeZbGeU4BvA58HTgNeB7zQzM4JIWxYmKeQo9l8fQ/mvH+G842D6qgU2XuA\nJwCjwAP4z679tgDfy7PSYFdEZHYfx38wvzWEcHly0sw+CrwD+CDwpjnU8yF8oHtZCOGduXreCvxt\nbOf589hvKY75+h4EIIRw6Xx3UArvHfgg9+fAecB3DrCeef1engsLIcxnfSIihWJmJwH3AkPAySGE\nVq5sAHgQMGBlCGFsH/X0AduBFrAmhLA3V1aKbayLbSi6K6n5+h6M118DnBdCsAXrsBSema3HB7uf\nDSH8xn7cN2/fy/tDc3ZFRPbtWfF4df4HM0AcsF4P9AJPnaWec4Ae4Pr8QDfW0wKujp8+86B7LEUz\nX9+DKTO70MwuNrN3mtmvmFnX/HVXZEbz/r08Fxrsiojs26nx+LMZyu+Jx1MOUT3yyLMQ3zufBz4M\n/DXwNeB+M3v5gXVPZM4Oy89BDXZFRPZtMB6HZyhPzi8+RPXII898fu98FXgxcBz+TsNp+KB3MfAF\nM/uVg+inyGwOy89BLVATETk4ydzHg10AMV/1yCPPnL93QgiXtZ26G3i3mW0GLscXUX59frsnMmcL\n8nNQkV0RkX1LIg2DM5QvartuoeuRR55D8b3zaTzt2BPjQiGRhXBYfg5qsCsism93x+NMc8geHY8z\nzUGb73rkkWfBv3dCCJNAsnCy70DrEZnFYfk5qMGuiMi+JbkknxdThKViBOxcYAL4wSz1/CBed257\n5CzW+7y29kQS8/U9OCMzOxVYgg94dxxoPSKzWPDv5U402BUR2YcQwr14WrB1wO+1Fb8fj4L9Sz4n\npJmdZmYP2V0ohDAKXBmvv7StnrfE+r+hHLvSbr6+B83sJDM7tr1+M1sO/HP89PMhBO2iJgfFzKrx\ne/Dk/PkD+V6el/5oUwkRkX3rsL3lncBT8Jy4PwOelt/e0swCQHvi/g7bBf8QOB14CbAt1nPvQj+P\nHH3m43vQzC7C5+Z+F0/svwtYC7wAn0P5Y+C5IYQ9C/9EcrQxs5cCL42frgbOBzYA18VzO0IIfxCv\nXQfcB/wihLCurZ79+l6el75rsCsiMjszOx74AL6d7zJ8p5+vAO8PIexqu7bjYDeWLQXeh//SWAPs\nxFe//0kI4YGFfAY5uh3s96CZPQ74feAs4Bh8MdBe4Hbgi8A/hBCmFv5J5GhkZpfiP7tmkg5s9zXY\njeVz/l6eDxrsioiIiEhhac6uiIiIiBSWBrsiIiIiUlga7IqIiIhIYWmwW0Bmdo2Zhbjydn/vvSje\ne8181isiIiJyOFQOdwcWkpm9HVgMXBFCGDrM3RERERGRQ6zQg13g7cAJwDXA0GHtydFjGN/O7/7D\n3RERERGRg1X0wa7spxDCl4EvH+5+iIiIiMwHzdkVERERkcI6ZINdM1tqZq81s6vM7C4z22tmY2Z2\nh5l91MyO6XDP+rggamgf9T5sQZWZXRp3jzkhnvpOvCbsY/HVyWb2D2a2wcwmzWy3mV1rZm8ws/IM\nbacLtsxskZn9hZnda2YTsZ4PmFl37vpnm9k3zGxHfPZrzeyXZ/m67Xe/2u5fYmaX5e5/wMw+ZWZr\n5vr1nCszK5nZb5rZ/5jZdjObMrPNZvYFM3vK/tYnIiIicrAO5TSGd+PbFCZGgB58X/jTgd8ws+eE\nEG6dh7ZGga3ACnxAvxvIb4HYvq3ii4AvAcnAdBjoA345vi40s5eGEMZmaG8J8L/AacAYUAZOBN4L\nPBH4VTN7M/AxIMT+9ca6v2lmzwohXN9e6Tz0axnwI+BkYAJoAMcCvw281MzOCyHcOcO9+8XMBoB/\nB54TTwV8G8o1wCuBl5vZ20IIH5uP9kRERETm4lBOY9gEfAQ4ExgIIQwCXcDZwDfwgennzOxhe8nv\nrxDCX4UQVgMb46kLQgirc68LkmvN7GTg8/iA8rvAaSGExcAA8Eagjg/g/nYfTb4PMOCXQwj9QD8+\noGwALzaz9wJ/E59/WXz2dcD3gRpwWXuF89Sv98brXwz0x76tx/erXgF8ycyq+7h/f/xL7M+twAuB\nvvicS/A/dBrA35rZufPUnoiIiMisDtlgN4RwWQjhkhDCzSGE0XiuGUK4EXgJcAfwWOAZh6pP0bvx\naOm9wAtCCHfHvtVDCJ8C3hqve72ZPWqGOvqAF4UQvhfvnQohfBofAAJ8APjXEMK7Qwh74jW/AF6N\nR0B/yczWLkC/FgEvDyH8VwihFe//LvAreKT7scCFs3x9ZmVmzwFeime8eGYI4WshhInY3p4Qwofx\ngXcJuORg2xMRERGZqyNigVoIoQ78T/z0kEX+YhT51+Knl4UQxjtc9mk8Km3Ay2eo6kshhJ93OP/N\n3Mcfbi+MA97kvjMWoF/XhRCu69Du3cC/xU9nund/vDYerwgh7Jrhms/F4zPnMtdYREREZD4c0sGu\nmZ1mZh8zs1vNbMTMWsmiMeBt8bKHLVRbQCcBg/Hj73S6IEZEr4mfnjlDPT+d4fy2eJwkG9S22xqP\nSxagX9fMcB58asS+7t0fT4vHd5jZlk4v4Mfxml58LrGIiIjIgjtkC9TM7FX42/rJHNEWvuCqHj/v\nx9+27ztUfcLnrSY27eO6Bzpcn/fgDOeb8bg1hBBmuSY/d3a++rWve5Oyme7dH0lmh0GyQfq+9M5D\nmyIiIiKzOiSRXTNbAfwjPqD7Ar4orTuEsCRZNEa2SOugF6gdoK7D1O5sFqpf8/l1Tr6PXhJCsDm8\nhuaxbREREZEZHappDL+CR27vAF4TQrgxhDDdds2qDvc14rG7Q1liLpHEmWzPfXzCjFfBcR2uX0jz\n1a99TQlJorHz8UzJVIzHzENdIiIiIvPmUA12k0HZrUlWgLy4IOtZHe7bE48rzaw2Q92/tI92k7Zm\nimJuyLXxzE4XmFkJT9cFcNM+2ppP89Wv8/bRRlI2H8/0/Xj8tX1eJSIiInKIHarB7nA8njFDHt3f\nxjc+aPczfE6v4bliHyKm3NrXAGskHhd3KozzaP89fvo2M+s0l/QN+EYMgSyDwYKax36dZ2ZPaz9p\nZo8my8LwpYPsLsAV8Xi2mf3Wvi40syX7KhcRERGZT4dqsPtNfFB2BvB3ZrYYIG6x+4fA3wM7228K\nIUwBX42fXmZmT49b0pbM7Hl4urKJfbR7ezy+Or9tb5sP4bueHQP8t5mdGvvWZWa/DfxdvO6fZkgv\ntlDmo18jwL+b2QuSPzLi9sRfx+cC3w588WA7GkL4/8kG558xs/fntyOOWxa/xMy+Cnz0YNsTERER\nmatDMtiNeV3/Jn76FmC3me3Ct+39C+BbwCdnuP0SfCB8PHAdvgXtGL7r2h7g0n00/U/x+Apg2Mw2\nmtmQmX0+17d78c0dJvFpAXeZ2e7YzqfwQeG3gLfP/YkP3jz160/xrYn/Gxgzs73AtXgUfTvwyg5z\npw/UbwFfwbdK/hNgs5ntMbNh/N/5K8CvzlNbIiIiInNyKHdQeyfwO8DN+NSECnALPlh7IdlitPb7\nNgBPAf4vPkAr4ym3PohvQDHS6b5477eBl+E5ZSfwt/1PAFa3XfefwOPwjBFDeGqsceB7sc/nhxDG\n9vuhD9I89GsnPqf5b/BFZDVgc6zviSGEO+axr2MhhJcBL8KjvJuAntjmz/FNJV4OvHm+2hQRERGZ\njc2c/lVERERE5Oh2RGwXLCIiIiKyEDTYFREREZHC0mBXRERERApLg10RERERKSwNdkVERESksDTY\nFREREZHC0mBXRERERApLg10RERERKSwNdkVERESksCqHuwMiIkVkZvcBi/CtvkVEZP+tA0ZCCCce\nTCWFHez+5sueHwBaVs5OlqsATE1OAzA+OpKVWdMvqRgAExMTadHUlJctWbIMgEWDg2lZpexfQsPb\naTSbufumAJiergNQ7ZrKulJp+X2l7PquLq+rO/ahWm6kZbWyX18t+fbOjansuUYn/ePh+igAu7bu\nScv6qgPeXk83AD3VRVlZt5+7/LNfMURkvi3q6elZevrppy893B0RETka3XnnnQ8Zjx2owg52ReTo\nZmYB+G4IYf0cr18PfAd4fwjh0tz5a4DzQgiH+o+6odNPP33pjTfeeIibFREphrPOOoubbrpp6GDr\nKexgN4mqhlL2iBanKNdqHuFt1Gpp2cTk6EPu7+ruSj/uW+QR0IGBxQBUKj3ZhfH3ZzCPvIZWLrLb\n8Mhss+VltXLWXqniEdoQsuubsa56jA43miEtmy55Hb1d/gy9fbm+x3bCeK/XXZpMy2o1r6sSI8Nj\nY1k0e2z84P9akiPH/g4ORUREHgkKO9gVkUecHwKnAzsOd0cSt20aZt3F/324uyEiclgMfeSFh7sL\ngAa7IlIQIYRx4K7D3Q8RETmyFDb12MjICCMjI2zZsiV9bd60ic2bNrFx40Y2btzInuHh9FWv1+Nr\ninp9iunp7NVoxlejEV+t9IVVwCq0Wi1arRbN3KvRbNBoNrBSCSuVKJUq6YtQhlCm1bL01WhAowHT\n0y2mp1s0muX0hXWBdVGu+qt3oJa+urv9VassolZZRE/XQPpat24169atZuWyRaxctohy2dIX5Yq/\n5JAws4vM7Coz22BmE2Y2YmbXm9lvdLh2yMyGZqjnUjMLcY5qUm8y5+W8WJa8Lm2795Vmdq2ZDcc+\n/NTMLjGzrrZm0j6YWb+ZXWZmG+M9t5jZS+M1FTN7t5ndY2aTZnavmb1lhn6XzOxNZvYjMxs1s7H4\n8e+a2Yw/i8zsGDO70sy2xfZvNLPXdLhufadn3hczO9/MvmZmO8ysHvv/l2a2eK51iIjIkU0jHZFD\n5xPAHcC1wIPAMuAFwJVmdmoI4b0HWO8twPuB9wG/AK7IlV2TfGBmHwIuwd/m/xwwCvwK8CHgfDN7\nbghhuq3uKvA/wFLgq0ANeDVwlZk9D3gz8BTg60AdeAVwuZltDyF8oa2uK4HXABuBTwMBeBnwceDp\nwK93eLYlwA3AHuCfgcXAK4HPmtmxIYS/nPWrMwMz+xP867YL+C9gG/B44A+AF5jZOSGEkX1UISIi\nR4HCDnZrVV+ENjpRT8+1gge/YvYuJiayhVyluACsXI5pv8ilLItGpncCUCmP587GgJTFxWjNLF1Y\nve5t9/f3+3254FkrLmSzVrYIrWTedrlSztcMQGh6mcUFbeVSdp+VvKyr5gvpGBhIy/oHfNHa7sYY\nALWubGFb4+HBPFlYZ4QQ7s2fMLMaPlC82Mw+GULYtL+VhhBuAW4xs/cBQ/lMBLl2zsEHuhuBJ4cQ\ntsTzlwBfBl4E/CE+8M07BrgJWB9CqMd7rsQH7F8C7o3PtSeWfRSfSnAxkA52zezV+ED3ZuAZIYTR\neP49wHeB15jZf4cQPtfW/uNjO68KIbTiPR8BbgQ+aGZXhRA27N9XDMzsmfhA9/vAC5L+x7KL8IH1\n+4F3zKGumdItnLa//RIRkflX2GkMIkea9oFuPDcF/D3+h+ezF7D518fjnyUD3dh+A/h9oAW8YYZ7\n354MdOM91wH34VHXd+UHinHgeT3wOLN8kuu0/YuTgW68fgx4V/y0U/vN2EYrd899wN/hUeffnPGJ\n9+2t8fjb+f7H+q/Ao+WdIs0iInKUKWxkdzwmIR4Zyd6FDHFTiUr8HVyrZI9fKvm4v1yJx9zfAeUY\nfa1WYhovyyLCzWYrHj3V2dRkVlaPH/d3e+S11KimZYbfVwpZhDYGdCnF3+ulVpYWNAac6YobTvT2\nZFHZZozatuJ4ZHo668P27XEDjSmvs9KVpU0rFfef/4hkZmvxgd2zgbVAT9slxy5g82fG47fbC0II\nPzOzB4ATzWxx2+BvT6dBOrAZOBGPsLbbBJSB1fHjpP0WuWkVOd/FB7VP6lB2fxzctrsGn7bR6Z65\nOAeYBl5hZq/oUF4DVpjZshDCzn1VFEI4q9P5GPE9s1OZiIgcOhrtiBwCZnYSnhprCXAdcDUwjA/y\n1gGvBRZyXkmy7d+DM5Q/iA/AB/H5sYnhGa5vAIQQOpUnc3mquXODwK4YyX6IEELDzHYAKzvUtXWG\n9pPo9OAM5bNZhv/8e98s1/UD+xzsiojIkU2DXZFD4534AOt18W3yVJzP+tq261t4dLGTA8kUkAxK\nV+PzbNutabtuvg0DS82s2r4IzswqwHKg02KwVTPUtzpX74H2pxRC0Fa+IiIFV9jB7vS0/z7dtGlz\nei55K7+cLASzbJrAqlVLAFi5ajkAXaVsGkPchIzeeCrZlQxgsu6L1azscxB6K9n4ZCrG6UJ9LwC7\ntmQL25IpiGZZXaWyT2koV+PRssBYnL3Akr74znejLy0bG/NpC1MNn8ZQnxpLy7Zv93/iqar3r5Wt\nn8OyaZiy8B4Vj1d1KDuvw7ndwOM7DQ6Bs2doowUdVla6m/G31NfTNtg1s0cBxwH3tc9fnUc349M3\nngF8q63sGXi/b+pw31ozWxdCGGo7vz5X74H4AfBCM3tsCOH2A6xjVmccO8iNR0hSdRGRRyotUBM5\nNIbicX3+pJmdT+eFWT/E/xh9Xdv1FwHnztDGTuD4Gco+E4/vMbMVufrKwF/hPwv+aabOz4Ok/Q+b\nWW+u/V7gI/HTTu2XgT/P5+E1sxPxBWYN4F8PsD+XxeM/mtkx7YVm1mdmTz3AukVE5AhS2Mhub69H\nQEul/Hi+9ZBz5VIW2Z2e9gjrZFxUFnIpy7rioq5QjVHYchaNHej2uqZj1NgqWXvdccOGpL1aJf0d\nTytGdn0xfHKuEY/edmhldYUYku2K9e/ekQXgdmz3j6fLHgBsTeWeq+l9n+qK0eLc3zeVbIG7LLyP\n4wPXL5nZVfjCrTOA5wNfBC5su/7yeP0nzOzZeMqwJwBPw3PCvqhDG98CXmVm/4kvHGsA14YQrg0h\n3GBmfwH8EXCbmf0bMIbn2T0D+B5wwDlrZxNC+JyZvQTPkXu7mX0Fz7P7Unyh2xdDCJ/tcOuteB7f\nG83sanyO7oX4VI4/mmHx3Fz68y0zuxj4MHCPmX0NzzDRD5yAR9u/h//7iIjIUaywg12RI0kI4daY\n2/XP8I0kKsBPgAvwBWEXtl1/h5k9B897+2J84HodnkXgAjoPdt+GDyCfHdso4blir411vsvMbgbe\nAvwWvoDsXuA9wF93Wjw2z16NZ154PfDGeO5O4K/xDTc62Y0PyP8CH/wvwjfm+KsOOXn3Swjhz83s\nejxK/HTgJfhc3k3Ap/CNN0RE5ChnIZf6qkgueN6TA8Btd96dntu1xyOmA4t8fU9vTza/1vCvQ1+f\nb8wwuXtvWrZ60SIAlq7xKGm9nqYJpdv8+qkJv7+nJ5tnW41zdqtVj7TuHc+mXlqcWrl4cFF6LktD\n5nN7J6az6/dOejR55SrfoGLL1qx/dw+Nxut9ru7SJUvSsgc37fA+N72udOMJoD9Gv+8Y2pyFgkVk\nXpjZjWeeeeaZN944054TIiKyL2eddRY33XTTTTOleJwrzdkVERERkcLSYFdERERECquwc3a7u/zR\nenuzqQrT0/Fct7+VX61kUzgs+Dv5/b1eZhPZFILRepz+UB4A4MFdWWqvvVu3AdDX42WrVmdpO5tj\nfl0p7rzWDFlWqP5ev/6BzenOrezZ6QvNjl3uacX6F2f58rdv93SiD2zaBcBEPZt5MDbp9Y5P+IKz\nZUuzvQnqcSrE2Lg/Q3kg+3o0Gpq9ICIiIsWmyK6IiIiIFFZhI7urVvrmEM3c+rtt2z2tWDXu0DA2\nmu0CWip5dNRaHgltNLOF6aHs0dB63NCqtnh1WtZf8pRg/d2+2KtvZba5VQuPprZaMZ1Zri/Vii9k\n2zmcLTTbPuqR4FWDnqJsUddAWlYOXtase5R4eipLWdaI0dv+uLhucFF/WrZmjadUnZz065cuXZaW\ndXcv5O60IiIiIoefIrsiIiIiUliFjex21zwKO1WfTM+VSh5abdY9StqTm7Nbjps19PZ7dHT50nST\nKaq9nsprvOwR4VUrs8hpJXg7i+Lc4HIlmwdr5hHXZLfXEtlmFNOxX41SlqpsYPkqAI7r9TqW5CK0\nJeKmEl2lpMNp2UTTywZi35NnB6gl13Unf9dkfajXJxAREREpMkV2RURERKSwNNgVERERkcIq7DSG\nyQmfqtDITWOoj/vb/dW4cKy7q5WWLV3uUxVOfcwZAExMZ38H7I5VLF220j8oZbuQVYJf11NLpi9k\n0wSymQbejoWsL+W4Wu2Y405Mzw3v2g1A18iQ968/64NV/d4TjlkRW8n+6fbedb/fV/Prly7JFslN\nTfmz7h33Xdme+MQnpWUrV65EREREpMgU2RURERGRwipsZLeER067u7LFWiXzqOtAn6f26uvLxvrL\nVniqsh17fPOGrXuyxVs24BHQY3viJg9ZQJjeuGitKy5wI7dPw+joKADluJdEtZr1pRUrCbkblq04\nBoDFqz1y3Krvzvre7dc9+tFrAJjObQhRHfCNLPbuHY/PmVt4V/J+dXV7281WtllGtZZtciEiIiJS\nRIrsioiIiEhhFTay29UVU3qFbA5ttRzn1/b6BhDVriw62rvI57l2xShp96psw4Vt4zFK3OupwGqt\nbEOHpT0eHS1Z3JSilEVLS60kuux1dfVmX+5K7Mv4SLapRG+XR3QHerwPE+O70rJjT9gMwFTcHMLI\norchboDR012Jz5k9155dvnHGjr3ezvR0FtndsWMHIiIiIkWmyK6IPISZXWOWmwuzcO2sM7NgZlcs\ndFsiIvLIpcGuiIiIiBRWYacxWEzt1Wplq8mSqQ1dPT6NgXI2xWFgyTIAnvzLzwLglnuG0rLdm3w6\nQVe337csN/1hRV+cthDil9KyaQyNCU/71dvbB0DfYE9aVomr1gZ7BtJz46OeLm318ad6lbmVcJPj\nfv39P/keAMPD29KyjZt9OkJPr9e1bPHytCx5/lrV7z/ttFPSsr6+PkQ6+C2g93B3QkREZD4UdrAr\nIgcmhHD/4e6DiIjIfCnsYLcVJ2gsXrYkOzni6cTKVY/oTuWuX/soj3j2L/JoZ6hnC8cGFnnarumS\nR2rH6tl9vf2+aG15nx8r2U4S7Njpi8O6e/3+Y3KbOLSa3ofp6awXU5Pev55uj0D39eYiwU/wzSAs\n+AK1W354Q1q2YqnXVe32hy6Xs+mWO7b5c9TN2xnenS16I7eQTYrNzC4CXgw8CVgDTAM/BT4RQvjX\ntmuvAc4LIVju3HrgO8D7ga8B7wPOAZYAJ4YQhsxsKF7+BOCDwMuAZcAG4JPA5SGEWec2u6MIAAAg\nAElEQVQCm9kpwOuB5wAnAIuALcA3gA+EEB5ouz7ft6/Ets8FasCPgEtCCDfQxswqwO/gkezH4D8P\n7wb+Cfh4CKHVfo+IiBx9CjvYFZGH+ARwB3At8CA+CH0BcKWZnRpCeO8c6zkHuAT4HvAZYDkP/bux\nBnwTWAx8Pn7+a8DfAqcCvzeHNi4A3oQPYG+I9T8WeAPwYjM7O4SwqcN9ZwN/BHwf+DSwNrb9LTN7\nYgjh7uRCM6sC/wmcjw9wPwdMAs8ELgeeAvzmHPqKmd04Q9Fpc7lfREQWVmEHu60YlNq5ezg9Nzbu\nIdlyJW73W62mZcMjHvFsTPr1S0pZerGRut9X7fatehv1XOR07wgA48N+7MptHDHZ9C1+a01vZ6qR\nhYRrVf/SD/Rmc3b3jPgmEj09XtbTm83/tRUeoV57qv/+3JpLG2ZDcRvkLj+WchHbcsXbXjzgkedS\nbk5xkhJNHhHOCCHcmz9hZjXg68DFZvbJGQaQ7Z4HvCmE8A8zlK/BI7lnhBDqsZ334RHWN5vZF0II\n187SxpXAZcn9uf4+L/b3PcDvdrjvhcDrQghX5O55Ix5Vfhvw5ty1f4wPdD8GvD0Ez1FoZmXgU8Dr\nzezfQghfnaWvIiJyhFM2BpFHgPaBbjw3Bfw9/kfvs+dY1S37GOgmLskPVEMIu4A/jZ++bg593dQ+\n0I3nrwZuxwepnVyfH+hGnwEawJOTE2ZWAt6CT414RzLQjW00gd8HAvDrs/U13nNWpxdw11zuFxGR\nhVXYyK6IZMxsLfAufFC7Fuhpu+TYOVb1w1nKG/jUg3bXxOOTZmvAzAwfaF6Ez/9dAuT3tp7qcBvA\nj9tPhBCmzWxrrCNxCj6N4x7gPd7cw0wAp8/WVxEROfIVdrAbWv5o01PZlIN63QM4K1YdA8DZTz07\nLXvS2WcBsHPrRj/xYLYGZlUpTjVY7NMfKqXutKza8KkDtRgcSlKKAVRiWrJ6nAYxtHkoLevr9sxO\nlUr2TzDR8GkPm7dvie2uSMuSX8hDm71f23bvTssWLVvl90/5grhKTzY9o3fAp1WM1r2fGzb8Ii3r\n6c+mUEhxmdlJ+CB1CXAdcDUwDDSBdcBrgbnOadkyS/mOfKS0w32Dc2jjo8Db8bnF3wA24YNP8AHw\nCTPct2eG8w0eOlheFo+PxhfazaR/Dn0VEZEjXGEHuyKSeic+wHtd+9v8ZvZqfLA7V7NlU1huZuUO\nA97V8TjcfkNbf1YCbwVuA54WQtjbVv7q/ejrTJI+fDmEcME81CciIkewwg52SyWPblbKWcCqXPZM\nQk8551wA1j/3l9OysVFfYNaa8qPtzDZtGN7qaUfH1ywC4MTTn5CW9cQIcs38d3t/fxYMKsco78/v\nuQeA7buytF/nnPqUh1wDMDU9DcDkuG8ucc/9G9KyJAJ8zy986uXSY1anZeec6e8Mb978MwCGNgyl\nZWvW+nXbYgqywf5sAV1fd/s72VJQj4rHqzqUnTfPbVWAp+ER5Lz18XjzLPefhK8luLrDQPe4WH6w\n7sKjwE81s2oIYXoe6hQRkSOUFqiJFN9QPK7PnzSz8/F0XvPtw2aW/pVpZkvxDAoA/zzLvUPx+PSY\nGSGpox/4R+bhD/QQQgNPL7YG+Dsze9hffWa2xswec7BtiYjI4VfYyK6IpD6OZ0H4kpldhc+BPQN4\nPvBF4MJ5bOtBfP7vbWb2H0AVeDk+sPz4bGnHQghbzOzzwKuAW8zsanye73PxPLi3AE+ch37+Kb74\n7U147t5v41+Xlfhc3nPx9GR3zENbIiJyGBV2sJvkky2V8nllPYBTirucTTez7Eaj4z7FYOeurQAs\nX7QoLdu92acV3L7BpwlUF2drbE499nivs+ILyManx9KyVt2nTZx46okAnHDyuqyDXd6vRm5DqVZc\nFD4Rt2j70a1ZrvpTT/P8uo//Jf89f/wxa9OywbjYbeXa5QAMLFmelt1z72Z/rrt8SkSXZc81XZ9A\nii+EcKuZPRP4M3wjiQrwE3zzhj3M72B3Ct/57EP4gHU5nnf3I3g0dS7+T7znQnwTiu3AfwB/Quep\nGPstZml4KfAb+KK3F+EL0rYD9wHvBT47H22JiMjhVdjBrohk4na5z5qh2NquXd/h/mvar9tHW8P4\nIHWfu6WFEIY61RlCGMejqn/c4bb97lsIYd0M5wO+gcWV++qniIgc3Qo72G22fM1Js5lLyRl/HbZa\nHnGdnMoim9v3+I5kW+NOaOsGs7ScrT4/NzXhEddf/DzLFR9GPQXYmuVrAOjr7UvLurs9Rdndd/ou\npRu3ZOnMGq2HZ2dqxDRmoellrdyC9h/f5ut6eno8On3fpo3ZYzX8ec447RQAHnX649KyF1/g/8Rb\ntm4HoDKdPXNvdz4bk4iIiEjxaIGaiIiIiBRWYSO7a47xDRlCOXt3sz4dI6Yx6nvbHbelZVt2e877\n8eDRzu/e8fO0bNtOT8tZXbUOgNJ0lqmoFTxyPLHXI6Y/uiHbxGlgwDdtOG7tcd5+Lso83vAo8VQ9\nmzc8HettNeIGFbn5xiHO7R2PG0/s2pttKtFT9XRiK/f45hK3/+SbadnywcUAPO/5zwTgZzf/KC1b\nPDjXfQREREREjk6FHeyKyKE109xYERGRw0nTGERERESksAob2T1xnU8deOzjz0jPDd2/yY9DnoZr\nTXVVWjYVpwlMmU8JaC7N0nf19i0FYLLib/v35HY9273LF7bd8MMfAvDTW7KpEWY+heLxZz4egMUn\nZXVal/+d0Wpmi9CShXPlUizLry+PdU3HaxqNbKHZ1NQ4ANd+73oAbv/h7WlZf82f5yUv8t3ifu3X\nXpyWLR3IUqiJiIiIFJEiuyIiIiJSWIWN7FZjVHRxf7YTaK3kqb12j3oqsea27rQsVPxLsfGBBwHY\n9ODWtKxUqvpxwo97dmUh11bwSOuuLZ7a62lPPDMt277do76bhzxNWDW3IKxR8vvykd1yjBiHlkeZ\nkwgvZFHiJJ1ZdzWra3rcN7KY3OPR3uc/4+lp2U9/8hMAbr3tTgBOOX5NWlapFvafX0RERARQZFdE\nRERECqywob2d23YCsHcsm9taNh/br1t3MgA7JibTsokxT/u1c7unGRsdH0/LajGKWml5ZLg5lc3Z\nHR3eC8CaFSsBeM3LX5mW3X23bybx1au/DsDkcK4vXf6lb0w30nONkKU0g4dudRwDu9RHPH3ZKNk2\nw7V4PPWEkwA4+ZjjH1Z27Y99Pu+99/4iLQuTD21PREREpGgU2RURERGRwtJgV0REREQKq7DTGPbu\n9bf7rd5Kz42MjgIwtH0bALvGs2kFye5qe0f8msZENr2g1O0L08px5kB+V7aJUd8Bbd2JxwJQrWZT\nDx53xmMBuP7GHwCwe89oWtbd2wvA1FS2q1qSeiyRLEp7iNiHXBdoVbzNrpga7ZjVx2VlcabC4n5f\noDbQm6Ub6+9T6jEREREpNkV2ReSIYWbrzCyY2RVzvP6ieP1F89iH9bHOS+erThEROXwKG9l9YJOn\nDqs3swjt+KQvSNu80xevNcjCo9t27AZgV4zsJpFegEqXR0z7+voA6O7KUpZNxgVwT3vqUwA46cR1\nadkdt/vmDoMDAwBMh+xvi1rZU6KV8sHbGK5NArr5SG8zpihrNv2ckYsCJ5tRtDzCu3RwaVq0dbN/\nHVYv95RjSxdnG1tM1x8aSRYREREpmsIOdkXkEeHLwA+ABw93R0RE5MhU2MHuyY86FYB6M0uvNd3y\n6OjJMTra1d2Xlm3a4vN4v/pfniZs04Nb0rJy1efsjvV6NHbR4KK0rKvmZT+76y4AHnfyKWnZY04/\nHYBvXftdAH7+86G0rKc7mbNbT88lycTqdT83Opqb4xujy8tXrACgVsnmBk+Pepq04R0esb73np+l\nZTu2+XMti1sDbxp6IC0rkUWvRY5GIYRhYPhw90NERI5cmrMrIkckMzvNzL5iZrvMbMzMvvf/2rvz\nOMuq+t77n2/NQ3dXD0wNCA2ogMGIwEsjJNJOOMVofDQ43YAZHtFwHTCJwzXaeBP15qqYkBjURI0E\nHzSXR82gj9xEGQR5DJMEaJChm6Gh6W66u6qrazyn1v1jrX327tOnqk53nequ2v19v171OlV77b32\nOtWb4le/+q21JJ1Xd07Dml1JG9PHMkmfT59PFutwJR0p6e8kPSVpVNKdki44MO/OzMwOlNJmds1s\nUTsB+ClwN/AlYDVwPvADSW8LIXyriT66gB8BK4FrgSFgA4CkVcDNwInAT9LHauCKdK6ZmZVEaYPd\nnUM7ARir5GUM4+nzbOJXYEetbWoqzgo7/LAVADzyaP7n/up4XB5s6ao48etXX/CCWttDDz8IwOR4\nnPwWpvLSgAdTOUFPV/w2n/6cU2ptfd2xJKK9UI6Qufvuu+MY7n+wduyss04H4LRTY3lGsYxhy+ZY\nrvjIww8AcNThK2ptmzY9AcAvnfocAI5c2Vtr6+x0Yt8WrBcDnw0h/FF2QNJfEQPgKyT9IIQwNEsf\nq4F7gXNDCLvr2j5NDHS/EEL4QIN7NE3SbdM0nTLNcTMzO4Ac7ZjZQjQIfLJ4IIRwK3AVsBz4zSb7\n+WB9oCupE3g7sAtYN809zMysJEqb2e3tT8uDTebxfNtUfLtTaRmvqRBqbZ1pGbJnnXIiAA9v2Fhr\nG9oZE0hrjj0KgKV9XbW2vu6YYR3cGbPET299qtY2vDttYrHhYQAmCptYTE3GLHOlki+NNjlZ2ePY\nyScek7+hyXjtw/fH5cyWLskn161cHiefrVwWz9+5Y0ut7VnPPB6AFQNL0njz70d7J2YL1e0hhF0N\njl8HXAA8H/j7WfoYA+5qcPwUoA+4MU1wm+4eTQkhnNnoeMr4ntFsP2ZmNj+c2TWzheipaY5ny6Q0\ns/3flhAKv9Hmsmtnu4eZmZVAaTO7HaketT3kuzaE8ZgxHU9Z0t0jI7W2oeF4bHBXzMaedFKeVR0Z\njP9vnByLbXfc+v/X2sbS0mGDQ9sAuPaH/1+tbTLVCPd0x0zw0Yctr7WtXBaXL1uSNpwAWJI2rehL\nr/1pS2GA7rT0WGdaBq2rM/+nq21MkT7p7CqkbBUzz6qkDSuqhb/otnlTCVuwjpzm+FHptZnlxhoF\nusVrZ7uHmZmVgDO7ZrYQnSFpaYPja9PrHXPo+z5gBDhdUqMM8doGx8zMbJFysGtmC9EA8PHiAUln\nESeWDRJ3TtsvIYRJ4iS0pdRNUCvcw8zMSqK0ZQy9PfFP+e1t+V8yl/TEcoJqNSaMKgP5MmFjaXmx\nsYn4yjOfVWtrS38NFXlJRKaalhqbUiwJ6OvpqbUdccQRAPQvjZPDujry3y062uPnaitMoFPsP0yF\nNPa8LSs97OiI/2Rt7fnSY0qlCu3pX1OFX2FCiONSZzp/Kh9ftZpPjjNbYG4Afk/SC4GbyNfZbQPe\n1cSyY7P5KPAy4P0pwM3W2T0f+D7wG3Ps38zMFojSBrtmtqhtAC4CPpNeu4HbgU+GEH44185DCNsk\nnQN8CngdcBZwP/BuYCOtCXbXrF+/njPPbLhYg5mZzWL9+vUAa+bajxpPVjYzs7mQNA60Az8/2GOx\nQ1a2scl9B3UUdqhqxfO3BhgKIZwwl4E4s2tmNj/uhunX4TWbb9nufn4G7WBYSM+fJ6iZmZmZWWk5\n2DUzMzOz0nKwa2ZmZmal5WDXzMzMzErLwa6ZmZmZlZaXHjMzMzOz0nJm18zMzMxKy8GumZmZmZWW\ng10zMzMzKy0Hu2ZmZmZWWg52zczMzKy0HOyamZmZWWk52DUzMzOz0nKwa2ZmZmal5WDXzKwJko6V\n9FVJT0gal7RR0hckrdjHflam6zamfp5I/R47X2O3cmjFMyjpOklhho+e+XwPtnhJepOkyyXdKGko\nPS//sJ99teTnabM65qNTM7MykXQScDNwBPA94D7gBcD7gFdJOieE8HQT/axK/Twb+BFwNXAK8E7g\ntZJeFEJ4eH7ehS1mrXoGCy6d5nhlTgO1MvsY8DxgGHic+LNrn83DszwrB7tmZrP7IvEH83tDCJdn\nByV9HvgA8GfARU308ylioHtZCOGSQj/vBf4i3edVLRy3lUernkEAQgjrWj1AK70PEIPcB4FzgR/v\nZz8tfZaboRBCK/szMysVSScCDwEbgZNCCFOFtqXAk4CAI0IIu2fopx/YCkwBq0MIuwptbekea9I9\nnN21mlY9g+n864BzQwiatwFb6UlaSwx2rwohvGMfrmvZs7wvXLNrZjazl6bXa4s/mAFSwHoT0Af8\nyiz9vAjoBW4qBrqpnyng2vTlS+Y8YiubVj2DNZLOl/RhSZdIerWk7tYN12xaLX+Wm+Fg18xsZien\n119M0/5Aen32AerHDj3z8excDXwa+BzwfeBRSW/av+GZNe2g/Bx0sGtmNrOB9Do4TXt2fPkB6scO\nPa18dr4HvA44lviXhlOIQe9y4FuSXj2HcZrN5qD8HPQENTOzuclqH+c6AaJV/dihp+lnJ4RwWd2h\n+4GPSnoCuJw4ifIHrR2eWdPm5eegM7tmZjPLMg0D07QvqztvvvuxQ8+BeHb+lrjs2OlpopDZfDgo\nPwcd7JqZzez+9DpdDdmz0ut0NWit7scOPfP+7IQQxoBs4mT//vZjNouD8nPQwa6Z2cyytSTPS0uE\n1aQM2DnAKHDLLP3cks47pz5zlvo9r+5+ZplWPYPTknQysIIY8G7b337MZjHvz3IjDnbNzGYQQniI\nuCzYGuAP6povJWbBvlFcE1LSKZL22F0ohDAMXJnOX1fXz8Wp/x96jV2r16pnUNKJko6p71/SYcDX\n0pdXhxC8i5rNiaTO9AyeVDy+P89yS8bjTSXMzGbWYHvL9cALiWvi/gI4u7i9paQAUL9wf4Ptgn8G\nnAq8HtiS+nlovt+PLT6teAYlXUiszb2euLD/duA44DXEGspbgVeEEHbO/zuyxUbSG4A3pC+PAl4J\nPAzcmI5tCyH8YTp3DbABeCSEsKaun316llsydge7Zmazk/QM4JPE7XxXEXf6+S5waQhhe925DYPd\n1LYS+ATxfxqrgaeJs98/HkJ4fD7fgy1uc30GJT0X+CBwJnA0cTLQLuAe4NvAl0IIE/P/TmwxkrSO\n+LNrOrXAdqZgN7U3/Sy3goNdMzMzMyst1+yamZmZWWk52DUzMzOz0jqkgl1JIX2sOQj3XpvuvfFA\n39vMzMzsUHVIBbtmZmZmdmjpONgDOMCynTsmD+oozMzMzOyAOKSC3RDCKbOfZWZmZmZl4TIGMzMz\nMyutRRnsSlop6QJJ10i6T9IuSbsl3Svp85KOnua6hhPUJK1Lx78uqU3SxZJ+JmlnOn56Ou/r6et1\nknokXZruPyppi6T/R9Kz9+P9LJH0ZklXSbo73XdU0oOSvizpWTNcW3tPko6T9BVJj0sal7RB0mcl\nLZvl/qdJ+mo6fyzd/yZJF0nq3Nf3Y2ZmZrZQLNYyho8Sd4HJDAG9xG03TwXeIenlIYS79rFfAf8v\ncevOKnFnmUa6gR8DvwJMAGPA4cBbgN+Q9OoQwg37cN8LgcsLX+8i/iJyUvp4m6Q3hBD+bYY+ngd8\nFVhZuH4N8ft0rqSzQwh71SpLuhj4C/JffHYDS4Cz08f5kl4bQhjZh/djZmZmtiAsyswusAn4DHAG\nsDSEMEAMQM8CfkgMPL8paa+tOmfxRuLWde8BloUQVgBHEvd+Lno38MvABcCSdP/nA7cDfcC3Ja3Y\nh/s+TQx2zwaWhxCWAT3EwP0qoD+9n/4Z+vg6cCfw3HT9EuB3gXHi9+X36y+Q9Pp031HiLxBHhhCW\nEH9xOI84oW8tcNk+vBczMzOzBaN02wVL6iYGnc8B1oYQri+0ZW/2hBDCxsLxdeT7Pb8rhPDlafr+\nOjHABXhHCOGquvbDgPuI+zz/SQjhTwtta4nZ4Ib7RM/wfgRcC7wcuDCE8Pd17dl7ugc4M4QwXtd+\nOXAx8OMQwksLx9uBh4DjgTeGEL7T4N4nAP9J/EXiuBDCk82O28zMzGwhWKyZ3WmlYO9/py/P2cfL\nnyaWAszmEeCbDe69DfhS+vJN+3jvhkL8beRf05czvZ/P1we6yXfT62l1x9cSA92NjQLddO8NwC3E\ncpe1TQ7ZzMzMbMFYrDW7SDqFmLF8MbE2dQmx5rao4US1GdwaQqg0cd71YfqU+PXEkoDTJHWFECaa\nubGkY4H/SszgngQsZe9fRmZ6P/8xzfFN6bW+rOLsrE9Jm2fodyC9PmOGc8zMzMwWpEUZ7Ep6C/AN\nIFspYAoYJNanQgx8+9PHvtja5HmbmmhrJwaYT83WmaRzgX8hjjszSJz4BrGGdhkzv5/pJtNlfdT/\nW69Or13EuuTZ9DVxjpmZmdmCsujKGCQdDnyFGOh+izj5qieEsCKEcFQI4SjyCVX7OkGt2ooh7tPJ\ncWmvfyAGuv9GzFT3hhCWF97PJfvT9yyyf/vvhBDUxMe6Ft7bzMzM7IBYjJndVxMDw3uBt4UQphqc\n00ymci5mKifIMqZVYEcTfb0IOBbYDrx+miW+5uP9ZBnn58xD32ZmZmYLwqLL7BIDQ4C7GgW6afWC\nl9Yfb7Fzm2i7u8l63ez9/GKGtWxf3vTImvfT9HqypF+ah/7NzMzMDrrFGOwOptfTpllH9/eJE7zm\n0xpJb60/KGkl8H+nL/+xyb6y9/MsST0N+jwPeMl+jXJm/w48mj6/LC1F1tA+rhlsZmZmtmAsxmD3\n34BAXErrLyUtB5C0TNIfAX9NXEJsPg0CX5H0Dkkd6f6/TL6hxRbgi032dRMwQlyb9xuSVqf+eiX9\nDnAN8/B+0m5q/5X4vXwFcK2kF2a/QEjqkHSmpM+w96YaZmZmZovCogt2Qwj3A19IX14M7JC0nVjz\n+ufEjOUV8zyMvyFutnAlMCxpEPg5cbLcCPDmEEIz9bqEEHYCH0lfvhl4QtJO4hbIfwc8CFza2uHX\n7v1PxF3WJoilH7cAI5K2EVdxuBX4ELB8Pu5vZmZmNt8WXbALEEK4hFgucAdxubEO4la57wdeCzSz\nVu5cjBNLCz5J3GCii7hs2dXAGSGEG/alsxDCXxK3Ks6yvB3Endg+QVwPd7plxeYshPA14GTiLxD3\nEL93A8Rs8o+BPySuY2xmZma26JRuu+D5VNgu+FIvxWVmZma28C3KzK6ZmZmZWTMc7JqZmZlZaTnY\nNTMzM7PScrBrZmZmZqXlCWpmZmZmVlrO7JqZmZlZaTnYNTMzM7PScrBrZmZmZqXlYNfMzMzMSsvB\nrpmZmZmVVsfBHoCZWRlJ2gAsAzYe5KGYmS1Wa4ChEMIJc+mktMHuJz/3tQAgqXasrS0mstuyY9Wp\nWltXV9ce109N5W3dqa06PrnXuWNjYwD09fUBMD4+nl/X3b1Hn8W2FStWpGNjtWNbtz0FQH9/PwDL\nli2rtbW3twOwfftWAG6785Za2613/xyAE086KV7X01dre8sbfyt+EuI/9fDwUK3t2GccDcDaXzsr\n/yaZWass6+3tXXnqqaeuPNgDMTNbjNavX8/o6Oic+yltsDuTagpkuzvyt5+tN5y9FgPayUoFgI4U\ncBbXJs6C0Czo7e/PA83h3buBPLCtTlVqbVu2PrXXuHp7ewBYunQJAD09ebCcBc7bt2+JYymMPfs8\nC6bV21/oVWnM8T0vXz5Qa9m8+cm9xmAm6Trg3BDCvP4SJGkNsAH4+xDChfN5r4Nk46mnnrrytttu\nO9jjMDNblM4880xuv/32jXPtxzW7ZmZmZlZah2Rm18xm9NtA36xn2azu3jTImg//68EehpnZQbHx\nM6892EMAShzsVqtVYM+a3UxWejBVKEeYSqUKnZ2de1wPef1uWyoXKLZl5QUTExOprVgHHM/fsmUz\nkJcSAFQqsY8jjzyicH4cF1lJcXWi8H7iwRUrBlJf+di7u7pTn/E99BVKKXbs2A7AYavifZYNLK21\njY7twqxeCOHRgz0GMzOzVnEZg9khQNKFkq6R9LCkUUlDkm6S9I4G514nKdQdWyspSFon6QWS/lXS\n9nRsTTpnY/oYkPRXkjZJGpN0r6T3qtFvno3H+mxJn5F0q6StksYlPSLpy5KObXB+cWynp7HtlDQi\n6XpJZ09znw5J75F0S/p+jEi6Q9LFkvyz0cysJEqb2Z2cjCsnZFncomrKqnYU/t/bUZe1zbKkkE9W\nm0gTwLJVHYDaLMHe3l4AhofzbOnYeJyglv0//rDDDqu1ZX1UKnn2tr0jZpWz7PLIyEitbSplhfvS\nSg3F8WX9V7OJdKkfgC1b4kS41auPTtdNFu7n/58fQv4GuBe4AXgSWAW8BrhS0skhhD9psp8XAR8B\nfgJ8FTgMmCi0dwH/BiwHrk5f/1/AXwAnA3/QxD3eCFwE/Bi4OfX/S8DvAa+TdFYIYVOD684C/hj4\nKfC3wHHp3v8u6fQQwv3ZiZI6gX8GXgncD3wTGANeAlwOvBD4L02MFUnTzUA7pZnrzcxsfpU22DWz\nPZwWQnioeEBSF/AD4MOSrpgmgKx3HnBRCOFL07SvBh5O9xtP9/kE8B/AeyR9K4Rwwyz3uBK4LLu+\nMN7z0ng/Bry7wXWvBd4ZQvh64Zp3AVcA7wPeUzj3vxED3b8C3h9CqKbz24EvA78j6X+FEL43y1jN\nzGyBK32wW1wvN5Ots6u2/O3X1/gW/+I6lepws8xpT09PrS3L7GaZ5LHxfD24gYGYhc3qeqcKS4/1\npLVwxyfy+t9s+bLs3tnavQAjIzFL3NmZ1Rvn76utPWZosyXSilnfrLZ3x46nAejv76219fXmn1u5\n1Qe66diEpL8GXgq8DPhGE13dOUOgm/lIMVANIWyX9N+BrwHvJGaXZxprw6A7hHCtpHuIQWojNxUD\n3eSrxID2BdmBVKJwMbAZ+EAW6KZ7VCV9MI3z7cCswW4I4cxGx1PG94zZrjczs/lV+mDXzEDSccCH\niEHtcUD9bzrHNNnVz2ZprxBLD+pdl16fP9sNUm3v24ELgecBK4BiPdJEg8sAbh7rufwAAB20SURB\nVK0/EEKYlPRU6iPzbGIZxwPAx6YpJR4FTp1trGZmtvA52DUrOUknEoPUFcCNwLXAIFAlbsV4AdA9\n3fV1Ns/Svq2YKW1w3UCDtnqfB95PrC3+IbCJGHxCDICPn+a6ndMcr7BnsLwqvT4L+MQM41jSxFjN\nzGyBK22wW9sauDCZLJuEVitjKMw3z8odsnOKS3tlk7qybYN3p53RYltlj/sccfjhtbbxyXheRyo9\nyEodACYmx9P98v8HZxPgH3ssrvx0+OH5hLZsAtzgzvj/82IpRVc2oS2VQbS15ZmqbOvhgYG49fBT\nT+W7ph1//DOwQ8IlxADvnfV/5pf0VmKw26wwS/thktobBLxHpdfBmS6WdATwXuBu4OwQwq669rfu\nw1ink43hOyGEN7agPzMzW8BKG+yaWc0z0+s1DdrObfG9OoCziRnkorXp9Y5Zrj+RuCTitQ0C3WNT\n+1zdR8wC/4qkzhDC5GwX7K/TjhngtgWyqLqZ2aGqtMFultsMhQlq2SS0kJbQDIUkVUd7/FbUZ2oB\n2tPn2WS0Ymb3yCOP3OO+xY0qerr33OyhuAzaZNqEorOwAURn2hxi06bHANi5c3ut7dd+7dcA2LUr\nJqU6C8uLdXfHLO/ukTi+4hKhXV2d6bUj3SP/Jx/cNWOSzcpjY3pdS1xuCwBJryQu59Vqn5b0ssJq\nDCuJKyhAnKQ2k43p9VeLGWJJS4Cv0IKfWSGEiqTLgT8B/lLSJSGE0eI5klYDK0II9871fmZmdnCV\nNtg1s5ovElcX+EdJ1xBrYE8DXgV8Gzi/hfd6klj/e7ekfwI6gTcRlyT74mzLjoUQNku6GngLcKek\na4l1vq8groN7J3B6C8b534mT3y4irt37I+L35QhiLe85xOXJHOyamS1y3lXArORCCHcRN0u4mbiR\nxLuBZcTNG65o8e0mgJcTJ8G9BXgXsUb2fcTlvprxu8CniCtG/AFxqbF/IZZHtOTPEal04Q3AbxM3\nlfh14IPEXwDaiFnfq1pxLzMzO7hKm9kN2XJChbKCbNLZVJo7095WmKCd2trbs53N8rVqR4bjxK/2\n1Ofy5fmE8mo1nlfbZW1irNbW2xdLCLKShZ6efLWnrlTiMDlZWEUpxHtPjI+l6/K+Kum8ZUvjBPFq\nYXydnfGfMZv01lmY9JaNb/fwMACHrVxZayuO1cothHAzcT3dRlR37toG119Xf94M9xokBqkz7pYW\nQtjYqM8Qwggxq/rfGly2z2MLIayZ5nggbmBx5UzjNDOzxc2ZXTMzMzMrrdJmdispe9tZyN52pIlb\n2dJj7RR2IUuvk+NxYvbISD5fpae7K73GbGx3d2GiWcq4xl1Gob2w7NfEZBxDd29c/mt8Is/idqbl\nwjrbC7u4TcSJbxOjMQtbXP5saMc2AFamzGxXR2GHt8pE9gkAS/vyDPJY2nmNNFFvSW8+Ia7a4d91\nzMzMrNwc7ZiZmZlZaZU2s1vNNolQoX51Kq1znzK7XZ1521jakGE8vS5btrTWli1D1p2W7Zqaypfl\n7OzManxjvWxfYSmxXSMj8b7VlPVtLyxnlj5vb88ztNu3DQF5fe3y5ctrbSNpubNs04qpwpJq2QYT\nlUrKJKdMNMBUqtmtLX9WWFIt24TCrBWmq401MzM7mJzZNTMzM7PScrBrZmZmZqVV2jKGzqxkICtd\nAEgT1LKd1HaN7S5cESeD9S+Jk8kalRxk52TXQ15CMJJKFsbHxmttvaktK0so7qD28EMPAbDj6a21\nY1s3x53Tnti0CYDBwXxJ0WzXti1btgCwc+fOfHy1pcbCHucCjO+K4zrpxJPi9VufqrV1tTe1kpSZ\nmZnZouXMrpmZmZmVVmkzu0rLdnUUJmRlx0ZHY7azrS2f5LU0bdbQkbKk2WYMAB2dccLXSFrGq7+/\nv9aWZVH708S0waGhWtv2x7YD8PDDDwPw+OOP19qGU7Y3VAuT3Tri+EKafLZ9+/Za21Dqd8PGDQBU\nCsuStfXF8WWT0LK+AZb1xHGtOmwFAI89urHWtnJgCWZmZmZl5syumZmZmZVWaTO7UyljWinsIjo5\nGutps40flizJM7TZBg6VdJ0K102Mp+s69qzdBZgKMZtaTRs6dBQ2ajj26NUA9HTFDSS6CnXAfX0x\n47pp02O1Y09v2xzvlzafKNb4ZrXBJ598MgBbtm+rtQ2nZc+e3rEDgPHxfPOKSkfXHuPr6cmXJZuY\nyOuLzczMzMrImV0zMzMzKy0Hu2ZmZmZWWuUtY6ikMobJfKJZd0d8u309vXudn5UxKFUoqK34e0A8\nmJUATE7mf/7PTpuYiLuR9fZ219omRuOxZxwTyxlWrcx3ROvuin2deMJxtWP33HMXkC9j1tGR//Mc\nffTRADzvec8D4P4HH6i13fXAfXHMaWe4ycl80tvw7l2pz+E93gNAd0deJmG20EgKwPUhhLVNnr8W\n+DFwaQhhXeH4dcC5IQSvtWdmdghyZtesJCSFFNiZmZlZUtrM7mSafLWkt692rLMtZjJTArQ2GQ3y\nLKpquZ98WbK2NKGtUokTv0LI23pSlnj37pg57e7urLVlE+GyTPCK5ctqbdmGEcekrC/A0qV9aQzx\nuoGBgVpbtgzZ8uUpO1zIUWUT2qaySXaV/H0NpWXWtmyJm0kc94xjam3eVMJK5mfAqcC22U40M7ND\nR2mDXTM7tIQQRoD7DvY4zMxsYXEZg9kBIulCSddIeljSqKQhSTdJekeDczdK2jhNP+tSycLaQr/Z\nenjnprbsY13dtb8l6QZJg2kM/ynpI5K6625TG4OkJZIuk/RYuuZOSW9I53RI+qikBySNSXpI0sXT\njLtN0kWS/kPSsKTd6fN3S5r2Z5GkoyVdKWlLuv9tkt7W4Ly1jd7zTCS9UtL3JW2TNJ7G/z8lLZ/9\najMzWwxKm9nt7Y3lBV2deVkBlSoAU2mnsVDYQS2bhFabqFb4f292bGR0FICe7sIktDQxrbMzfisn\nxsdqbf3dcW3c8bRO79RUtdaWlUtka/gCrFq1CoAdab3c/v58h7Ph4bhT22gaw0RhLd3aONPOa8UJ\nat11JRjFMYxXKtgB9TfAvcANwJPAKuA1wJWSTg4h/Ml+9nsncCnwCeAR4OuFtuuyTyR9CvgI8c/8\n3wSGgVcDnwJeKekVIYRJ9tQJ/G9gJfA9oAt4K3CNpPOA9wAvBH4AjANvBi6XtDWE8K26vq4E3gY8\nBvwt8T+63wS+CPwq8PYG720FcDOwE/gasBz4LeAqSceEEP7nrN+daUj6OPH7th34F2AL8MvAHwKv\nkfSiEMLQDF2YmdkiUNpg12wBOi2E8FDxgKQuYqD4YUlXhBA27WunIYQ7gTslfQLYWFyJoHCfFxED\n3ceAF4QQNqfjHwG+A/w68EfEwLfoaOB2YG0IYTxdcyUxYP9H4KH0vnamts8TSwk+DNSCXUlvJQa6\ndwAvDiEMp+MfA64H3ibpX0MI36y7/y+n+7wlpGJ5SZ8BbgP+TNI1IYSH9+07BpJeQgx0fwq8Jht/\naruQGFhfCnygib5um6bplH0dl5mZtV5pyxhEQAQmJ8ZrH7QBbaB2oXbR1tZW+6hUKlQqFarVKtVq\nlRBC7aNaqVCtVOjp7KGns4eu9q7aR2V8gsr4BP29vfT39qKpkH8Q55F1dXbS1dnJ7qFdtY+lff0s\n7euv9V2tVIiT4qYIoUoIVYaGdtQ+BgaWMDCwhMmJsb0+xBRiijaJNokQqH2Mj48zPj5OX18ffX19\n7Ni+vfaxa2iIXUNOXB0o9YFuOjYB/DXxF8+XzePtfye9/mkW6Kb7V4APEh++35vm2vdngW665kZg\nAzHr+qFioJgCz5uA50oqrm2X3f/DWaCbzt8NfCh92ej+1XSPqcI1G4C/JGad/8u073hm702vv18c\nf+r/68RseaNMs5mZLTLO7JodIJKOIwZ2LwOOA+oXfD5mr4ta54z0+qP6hhDCLyQ9DpwgaXld8Lez\nUZAOPAGcQMyw1tsEtANHpc+z+09RKKsouJ4Y1D6/QdujKbitdx2xbKPRNc14ETAJvFnSmxu0dwGH\nS1oVQnh6po5CCGc2Op4yvmc0ajMzswOntMFuVmdb3fMgAG2pPldTxeXFYpK7Pb3uUV+bTuvtirFJ\nqObXdSnWBE+OxFrd4kYN46Oxznbp0qUADI2P1tqqlVTHW8lrdkeHY78rl6fz0/JkAArx3vf+590A\nbHwwn3Q+PDycxhXLLatTeS1uZyoOXrZkSbpf3jaW6oBt/kk6kbg01grgRuBaYJD4iK4BLgD2miTW\nQtk6dk9O0/4kMQAfINbHZgYbn04FIITQqD17yAoF8wwA21Mmew8hhIqkbcARDfp6apr7Z9npgWna\nZ7OK+PPvE7OctwSYMdg1M7OFrbTBrtkCcwkxwHpn+jN5TapnvaDu/ClidrGR/VkpIAtKjyLW2dZb\nXXdeqw0CKyV11k+Ck9QBHAY0qqk5cpr+jir0u7/jaQshrNzP683MbJEobc2u2QLzzPR6TYO2cxsc\n2wEcKamzQdtZ09xjilg+0Mgd6XVtfYOkZwLHAhvq61db6A7iz5sXN2h7MXHctzdoO07SmgbH1xb6\n3R+3ACsk/dJ+Xm9mZotEaTO7tfkshSXEptKxbEHSDqbYW/yzf3FJsM72+G2qZkuXFcoEenriX57H\nxmNJQE93f61tbHQ4tcX4o60t5G3jcWezvv58h7fdu3cBsGxZLGMo7oT2i/sfBeDee+8BYPNT+V+j\nd6dyjOz8sdF8+bOQ3v74WFoirT2PhRotX2bzZmN6XQv8c3ZQ0itpPDHrZ8R6z3cCXy6cfyFwzjT3\neBp4xjRtXwV+F/iYpH8KIWxN/bUDnyUGon/X1DvZP18l1ip/WtLatAEEkvqAz6RzGt2/Hfgfkt5a\nWI3hBOIEswrwD/s5nsuA1wJfkfSmEMITxUZJ/cBzQwi37Gf/Zma2QJQ22DVbYL5IDFz/UdI1xIlb\npwGvAr4NnF93/uXp/L+R9DLikmHPA84mrgn76w3u8e/AWyT9M3HiWAW4IYRwQwjhZkl/DvwxcLek\n/wXsJq6zexrwE2C/16ydTQjhm5JeT1wj9x5J3yX+3vkG4kS3b4cQrmpw6V3EdXxvk3QtsUb3fGIp\nxx9PM3mumfH8u6QPA58GHpD0feIKE0uA44nZ9p8Q/33MzGwRK2+wW41Z2KA8e9velv4inBKsbcoz\nre3Z5ylr215oU8oAj43F7G1nV/5tCykTnHU6NjZSa+vpjVnfbALZkiX5JhHVNL5sc4liH9u2bQHg\nttturbVs3RqPPfFETEBVqnl2Ofusrb1tr7bO9Fft8YmYxR0vTK7r78+z0Da/Qgh3pbVd/5S4kUQH\n8HPgjcQJYefXnX+vpJcT1719HfGf+UbiKgJvpHGw+z7iQ/SydI824lqxN6Q+PyTpDuBi4LeJE8ge\nAj4GfK7R5LEWeytx5YXfAd6Vjq0HPkfccKORHcSA/M+Jwf8y4sYcn22wJu8+CSH8D0k3EbPEvwq8\nnljLu4mYTZ9T/2ZmtjCUN9g1W2BCCDcDL52mWfUHQgg/oXGN613AugbnbyFu3DDTGK4Grp5trOnc\nNTO0rZ2h7ULgwgbHp4gZ7i82ef/i92SvLZUbnH8djb+Pa2e45ifEDK6ZmZVUaYPdLGtbzJxWJmMd\nbkdaHiyEfHmxavr/6kTKgGaZV4AladvetvaYFW0rbDM8vDtOIO9NWdyJiXx5sZ7eZUCeqf35z/O5\nNM95znMA2DWcTyZfuSKuorTxkY0APPDg/bW2zU9uTu8rLY0W8sxztTtO2lf6/3xx7KQa3ac2x+t7\nuvLVrXoPOxwzMzOzMvNqDGZmZmZWWg52zczMzKy0SlvGoLTMWFth6bGQShtCmsAV2gq7pKV6h0o1\nljpMTuRzdaZ6Y5lARyoXCIV92bLlxJ56Kk4c2717uNZ2/PFr4vlhz6XB4vmxrGDz5nwJsYG0c9rO\nnXGp02wJMoAtW+JGUuOjqUyiLf+na+/OShPSfSbzCWpKYx4ciuUWPYXShR075mtJVTMzM7OFwZld\nMzMzMyut0mZ2q5MxMzs6mWdTQ5rUlU1QG961rdbWno4NpQxotZJnR5ctjUt0Vadin1NTeWZ3shIz\nwXfdFSef9fT21NoOP/wwALZsjVlZCsugbd4cM8Gbnni8duzRx9KmEGkDiFCYhDYxEY9NpUl1HYWN\ntXp64j0nUta3Wlh6bDJlecfTJhnFCXsbNmzAzMzMrMyc2TUzMzOz0nKwa2ZmZmalVdoyhuzP9ePj\n+bq3jz36GJDvbPbkE/mf8cdG485nIyPxtT2tTwswOR5LCI4/7oR4fU++Vu2DD8W1cNfffzcARxx+\nRK0tKy9YsWIFkK+3C/D447F8YWJivHaskkoiJlIJhgrr4ytNrmtrS+vsTuUlEfVlDxOF0o1sUtxU\nWnv3kUceqbXt3LYDMzMzszJzZtfMzMzMSqu0md3qVJyY1dfXWzvW0xOX4XrgFzEb28ZYre3pp+Nk\ntY6O+C0pTg67996YtR3cGXc7G0lZYIDRsbjUWHt7zMLu2pXviJbtmHbEEUfuNb6xsdhHcTmyick0\nCS1lbYtjyDLN2Wtxk7TieVCfEY6/z2zdFt/fkt78+9HZWdp/fjMzMzPAmV0zMzMzK7HSpvZGhmKG\nddmyZbVjR2VLgT25CYDtO/MNILJa2Im0mUSxZnd4924AHk31rpOFmths6bGOjphNbVN+3Wha7mui\nEpcZqxbSsdl9poop2mra9CL7sprX5bYr/lP1pszs2Hi+vNh4qvGtpvcwOpHXKfdX4hJlfem6/p6+\n/LrhPENtZmZmVkbO7JqZmZlZaTnYNbMFQ9IaSUHS15s8/8J0/oUtHMPa1Oe6VvVpZmYHT2nLGLY8\nGZcZe3pLHs9nu4gt7Y9Lgj3+ZD5BbXwylgxUp1IRQWEHtc40aS3rSVOFCWHptGoqQZgIeVlCSBPF\nxsbT8l9TxTKGeEyFrjqm4vnt6U4K+dhViZ+3pWPt5Bd2pslx6XImqhO1tirxfQ3u3BnH1JGXOEyO\n5MuemZmZmZVRaYNdMzskfAe4BXjyYA/EzMwWptIGu09v3QrA4GC+FFiWTW3rjJPI2pRnTrNJZ9my\nX9mENcgnlo2MxKxotZD1Vdq9Ilv9KxQmlbW1xbYsE9zZ3llra+9M1xU2h+hJ7d3dcdOKrq6uWltn\nZ2xrb08T6Sp59nbHZLYZRWWP8QIMDQ0BcP/OuIFEd347VCl8YbYIhRAGgcFZTzQzs0OWa3bNbEGS\ndIqk70raLmm3pJ9IOq/unIY1u5I2po9lkj6fPp8s1uFKOlLS30l6StKopDslXXBg3p2ZmR0opc3s\nVrLspvINFsbStr/t1ZTZbc9j/YGlS4E8w1vM7GYZ1rYsEVrYxKHW1rb37w0d7anWN8vwFsaSbQRR\nmazsdSzL7Bb7zJZCq20TnHdVy+RONehzLGWVR0fj8ml9yv/JO0KhE7OF5QTgp8DdwJeA1cD5wA8k\nvS2E8K0m+ugCfgSsBK4FhoANAJJWATcDJwI/SR+rgSvSuWZmVhKlDXbNbFF7MfDZEMIfZQck/RUx\nAL5C0g9CCEOz9LEauBc4N4Swu67t08RA9wshhA80uEfTJN02TdMp+9KPmZnND5cxmNlCNAh8sngg\nhHArcBWwHPjNJvv5YH2gK6kTeDuwC1g3zT3MzKwkSpvZzUoGenp6asc6siXEUvlCpTA5LGvLFHdJ\nq0klAY3++J/dr7gjWjYxLfuNorj0WFtbLEtoL5QqZKUX2e5qlcreJQ5ZJUSVvK/JdF5t5zXlbVPp\nvWbXFysXpoInqNmCdXsIYVeD49cBFwDPB/5+lj7GgLsaHD8F6ANuTBPcprtHU0IIZzY6njK+ZzTb\nj5mZzQ9nds1sIXpqmuOb0+tAE31sCaFQYJ/Lrp3tHmZmVgKlzeyOjIzsdSzLvlYr1fR13jYxNr7n\nOYUMbZatbbQsWX3fU4Wlx5TuU581jn3F7K3SxDOAyWplj76KE9qye9aOFMYuVerunY+9mq7LJslV\nipnnhjlqswXhyGmOH5Vem1lurFGgW7x2tnuYmVkJOLNrZgvRGZKWNji+Nr3eMYe+7wNGgNMlNcoQ\nr21wzMzMFikHu2a2EA0AHy8ekHQWcWLZIHHntP0SQpgkTkJbSt0EtcI9zMysJEpbxlCc3JXJShOy\nMr72wg5q2WSt2p/2C3/hD2miWXasWAZY32fxD6eqnbP3zmaVNAEu2xkNQFmpQoMyhnyntjjOoPxG\nWXlFdqT4t9up2sS2VGZRuG6y6glqtmDdAPyepBcCN5Gvs9sGvKuJZcdm81HgZcD7U4CbrbN7PvB9\n4Dfm2L+ZmS0QpQ12zWxR2wBcBHwmvXYDtwOfDCH8cK6dhxC2SToH+BTwOuAs4H7g3cBGWhPsrlm/\nfj1nntlwsQYzM5vF+vXrAdbMtR81nqxsZmZzIWkcaAd+frDHYoesbGOT+w7qKOxQ1Yrnbw0wFEI4\nYS4DcWbXzGx+3A3Tr8NrNt+y3f38DNrBsJCeP09QMzMzM7PScrBrZmZmZqXlYNfMzMzMSsvBrpmZ\nmZmVloNdMzMzMystLz1mZmZmZqXlzK6ZmZmZlZaDXTMzMzMrLQe7ZmZmZlZaDnbNzMzMrLQc7JqZ\nmZlZaTnYNTMzM7PScrBrZmZmZqXlYNfMrAmSjpX0VUlPSBqXtFHSFySt2Md+VqbrNqZ+nkj9Hjtf\nY7dyaMUzKOk6SWGGj575fA+2eEl6k6TLJd0oaSg9L/+wn3215Odpszrmo1MzszKRdBJwM3AE8D3g\nPuAFwPuAV0k6J4TwdBP9rEr9PBv4EXA1cArwTuC1kl4UQnh4ft6FLWategYLLp3meGVOA7Uy+xjw\nPGAYeJz4s2ufzcOzPCsHu2Zms/si8Qfze0MIl2cHJX0e+ADwZ8BFTfTzKWKge1kI4ZJCP+8F/iLd\n51UtHLeVR6ueQQBCCOtaPUArvQ8Qg9wHgXOBH+9nPy19lpvh7YLNzGYg6UTgIWAjcFIIYarQthR4\nEhBwRAhh9wz99ANbgSlgdQhhV6GtLd1jTbqHs7tW06pnMJ1/HXBuCEHzNmArPUlricHuVSGEd+zD\ndS17lveFa3bNzGb20vR6bfEHM0AKWG8C+oBfmaWfFwG9wE3FQDf1MwVcm758yZxHbGXTqmewRtL5\nkj4s6RJJr5bU3brhmk2r5c9yMxzsmpnN7OT0+otp2h9Ir88+QP3YoWc+np2rgU8DnwO+Dzwq6U37\nNzyzph2Un4MOds3MZjaQXgenac+OLz9A/dihp5XPzveA1wHHEv/ScAox6F0OfEvSq+cwTrPZHJSf\ng56gZmY2N1nt41wnQLSqHzv0NP3shBAuqzt0P/BRSU8AlxMnUf6gtcMza9q8/Bx0ZtfMbGZZpmFg\nmvZldefNdz926DkQz87fEpcdOz1NFDKbDwfl56CDXTOzmd2fXqerIXtWep2uBq3V/dihZ96fnRDC\nGJBNnOzf337MZnFQfg462DUzm1m2luR5aYmwmpQBOwcYBW6ZpZ9b0nnn1GfOUr/n1d3PLNOqZ3Ba\nkk4GVhAD3m3724/ZLOb9WW7Ewa6Z2QxCCA8RlwVbA/xBXfOlxCzYN4prQko6RdIeuwuFEIaBK9P5\n6+r6uTj1/0OvsWv1WvUMSjpR0jH1/Us6DPha+vLqEIJ3UbM5kdSZnsGTisf351luyXi8qYSZ2cwa\nbG+5HnghcU3cXwBnF7e3lBQA6hfub7Bd8M+AU4HXA1tSPw/N9/uxxacVz6CkC4m1udcTF/bfDhwH\nvIZYQ3kr8IoQws75f0e22Eh6A/CG9OVRwCuBh4Eb07FtIYQ/TOeuATYAj4QQ1tT1s0/PckvG7mDX\nzGx2kp4BfJK4ne8q4k4/3wUuDSFsrzu3YbCb2lYCnyD+T2M18DRx9vvHQwiPz+d7sMVtrs+gpOcC\nHwTOBI4mTgbaBdwDfBv4UghhYv7fiS1GktYRf3ZNpxbYzhTspvamn+VWcLBrZmZmZqXlml0zMzMz\nKy0Hu2ZmZmZWWg52zczMzKy0HOyamZmZWWk52DUzMzOz0nKwa2ZmZmal5WDXzMzMzErLwa6ZmZmZ\nlZaDXTMzMzMrLQe7ZmZmZlZaDnbNzMzMrLQc7JqZmZlZaTnYNTMzM7PScrBrZmZmZqXlYNfMzMzM\nSsvBrpmZmZmVloNdMzMzMyut/wPZbuyuo6CwCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a24395c50>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 349
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
